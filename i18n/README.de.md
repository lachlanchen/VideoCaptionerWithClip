[English](../README.md) ¬∑ [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](README.ar.md) ¬∑ [Espa√±ol](README.es.md) ¬∑ [Fran√ßais](README.fr.md) ¬∑ [Êó•Êú¨Ë™û](README.ja.md) ¬∑ [ÌïúÍµ≠Ïñ¥](README.ko.md) ¬∑ [Ti·∫øng Vi·ªát](README.vi.md) ¬∑ [‰∏≠Êñá (ÁÆÄ‰Ωì)](README.zh-Hans.md) ¬∑ [‰∏≠ÊñáÔºàÁπÅÈ´îÔºâ](README.zh-Hant.md) ¬∑ [Deutsch](README.de.md) ¬∑ [–†—É—Å—Å–∫–∏–π](README.ru.md)


[![LazyingArt banner](https://github.com/lachlanchen/lazyingchen/raw/main/figs/banner.png)](https://github.com/lachlanchen/lazyingchen/blob/main/figs/banner.png)

# Clip-GPT-Captioning

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow)
![Status](https://img.shields.io/badge/README-Expanded-success)
![Repo Layout](https://img.shields.io/badge/Layout-Root%20Scripts-informational)
![Legacy Scripts](https://img.shields.io/badge/Legacy%20Scripts-Present-orange)
![i18n](https://img.shields.io/badge/i18n-Enabled-brightgreen)
![Maintained Path](https://img.shields.io/badge/Video-v2c.py-2ea44f)

Ein Python-Toolkit zur Generierung nat√ºrlicher Bild- und Videobeschriftungen, indem OpenAI CLIP-Vision-Embeddings mit einem GPT-√§hnlichen Sprachmodell kombiniert werden.

## üß≠ Snapshot

| Dimension | Details |
|---|---|
| Aufgabenbereich | Bild- und Videobeschriftung |
| Zentrale Ausgaben | SRT-Untertitel, JSON-Transkripte, beschriftete Bilder |
| Prim√§re Skripte | `i2c.py`, `v2c.py`, `image2caption.py` |
| Legacy-Pfade | `video2caption.py` und versionsspezifische Br√ºder (aus historischen Gr√ºnden erhalten) |
| Datensatzfluss | `data/raw/results.csv` + `data/raw/flickr30k_images/` |

## ‚ú® √úberblick

Dieses Repository bietet:

- Inferenz-Skripte f√ºr Bildbeschriftung und Untertitelung von Videos.
- Eine Trainings-Pipeline, die eine Abbildung von CLIP-Image-Embeddings auf GPT-2-Token-Embeddings lernt.
- Werkzeuge zur Datensatzgenerierung im Stil von Flickr30k.
- Automatischen Checkpoint-Download f√ºr unterst√ºtzte Modellgr√∂√üen, wenn Gewichte fehlen.
- Mehrsprachige README-Varianten in `i18n/` (siehe Sprachleiste oben).

Die aktuelle Implementierung enth√§lt sowohl neuere als auch √§ltere Skripte. Einige Legacy-Dateien werden als Referenz aufbewahrt und sind unten dokumentiert.

## üöÄ Features

- Einzelbild-Captioning √ºber `image2caption.py`.
- Video-Captioning (gleichm√§√üiges Frame-Sampling) √ºber `v2c.py` oder `video2caption.py`.
- Anpassbare Laufzeitoptionen:
  - Anzahl der Frames.
  - Modellgr√∂√üe.
  - Sampling-Temperatur.
  - Checkpoint-Name.
- Multiprocessing-/Threaded-Captioning f√ºr schnellere Video-Inferenz.
- Ausgabeartefakte:
  - SRT-Untertiteldateien (`.srt`).
  - JSON-Transkripte (`.json`) in `v2c.py`.
- Trainings- und Evaluations-Einstiegspunkte f√ºr CLIP+GPT2-Mapping-Experimente.

### Auf einen Blick

| Bereich | Hauptskript(e) | Hinweise |
|---|---|---|
| Bildbeschriftung | `image2caption.py`, `i2c.py`, `predict.py` | CLI + wiederverwendbare Klasse |
| Videobeschriftung | `v2c.py` | Empfohlener stabiler Pfad |
| Legacy-Videofluss | `video2caption.py`, `video2caption_v1.1.py` | Enth√§lt ger√§tespezifische Annahmen |
| Datensatzaufbau | `dataset_generation.py` | Erzeugt `data/processed/dataset.pkl` |
| Training / Evaluation | `training.py`, `evaluate.py` | Nutzt CLIP+GPT2-Mapping |

## üß± Architektur (High Level)

Das Kernmodell in `model/model.py` hat drei Teile:

1. `ImageEncoder`: extrahiert CLIP-Image-Embeddings.
2. `Mapping`: projiziert CLIP-Embeddings in eine GPT-Prefix-Embedding-Sequenz.
3. `TextDecoder`: GPT-2-Sprachmodellkopf, der Captions autoregressiv tokenweise generiert.

Training (`Net.train_forward`) nutzt vorab berechnete CLIP-Image-Embeddings + tokenisierte Captions.
Inferenz (`Net.forward`) verwendet ein PIL-Bild und dekodiert Tokens bis EOS oder `max_len`.

### Datenfluss

1. Datensatz vorbereiten: `dataset_generation.py` liest `data/raw/results.csv` und Bilder in `data/raw/flickr30k_images/`, schreibt `data/processed/dataset.pkl`.
2. Trainieren: `training.py` l√§dt gepickelte Tupel `(image_name, image_embedding, caption)` und trainiert Mapping-/Decoder-Schichten.
3. Evaluieren: `evaluate.py` rendert generierte Captions auf zur√ºckgehaltene Testbilder.
4. Inferenz ausf√ºhren:
   - Bild: `image2caption.py` / `predict.py` / `i2c.py`.
   - Video: `v2c.py` (empfohlen), `video2caption.py` (Legacy).

## üóÇÔ∏è Projektstruktur

```text
VideoCaptionerWithClip/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ image2caption.py               # Einzelbild-Caption-CLI
‚îú‚îÄ‚îÄ predict.py                     # Alternative Einzelbild-Caption-CLI
‚îú‚îÄ‚îÄ i2c.py                         # Wiederverwendbare ImageCaptioner-Klasse + CLI
‚îú‚îÄ‚îÄ v2c.py                         # Video -> SRT + JSON (threaded Frame-Captioning)
‚îú‚îÄ‚îÄ video2caption.py               # Alternative Video -> SRT-Implementierung (Legacy-Einschr√§nkungen)
‚îú‚îÄ‚îÄ video2caption_v1.1.py          # √Ñltere Variante
‚îú‚îÄ‚îÄ video2caption_v1.0_not_work.py # Explizit als nicht funktionierende Legacy-Datei markiert
‚îú‚îÄ‚îÄ training.py                    # Einstiegspunkt f√ºr Modelltraining
‚îú‚îÄ‚îÄ evaluate.py                    # Evaluation auf Test-Split und gerenderte Ausgaben
‚îú‚îÄ‚îÄ dataset_generation.py          # Baut data/processed/dataset.pkl
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ dataset.py                 # Datensatz- + DataLoader-Helfer
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ model.py                   # CLIP-Encoder + Mapping + GPT2-Decoder
‚îÇ   ‚îî‚îÄ‚îÄ trainer.py                 # Hilfsklasse f√ºr Training/Validierung/Test
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # ConfigS / ConfigL Defaults
‚îÇ   ‚îú‚îÄ‚îÄ downloads.py               # Google-Drive-Checkpoint-Downloader
‚îÇ   ‚îî‚îÄ‚îÄ lr_warmup.py               # LR-Warmup-Zeitplan
‚îú‚îÄ‚îÄ i18n/                          # Mehrsprachige README-Varianten
‚îî‚îÄ‚îÄ .auto-readme-work/             # Auto-README-Pipeline-Artefakte
```

## üìã Voraussetzungen

- Python `3.10+` wird empfohlen.
- Eine CUDA-f√§hige GPU ist optional, aber f√ºr Training und Inferenz gro√üer Modelle stark empfohlen.
- `ffmpeg` wird von den aktuellen Skripten nicht direkt ben√∂tigt (OpenCV wird zur Frame-Extraktion verwendet).
- F√ºr den ersten Download von Modellen/Checkpoints aus Hugging Face / Google Drive ist Internetzugang erforderlich.

Aktuell ist keine Lockfile-Datei vorhanden (`requirements.txt` / `pyproject.toml` fehlen), daher werden Abh√§ngigkeiten aus den Imports abgeleitet.

## üõ†Ô∏è Installation

### Standard-Setup aus dem aktuellen Repository-Layout

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

python -m venv .venv
source .venv/bin/activate

pip install --upgrade pip
pip install torch torchvision torchaudio
pip install transformers pillow matplotlib numpy tqdm opencv-python pandas wandb gdown
```

### Urspr√ºngliches README-Installations-Snippet (beibehalten)

Die fr√ºhere README endete in der Mitte eines Blocks. Die urspr√ºnglichen Befehle sind unten exakt als historische Referenz unver√§ndert √ºbernommen:

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip/src
```

Hinweis: In der aktuellen Repository-Struktur liegen die Skripte im Root und nicht unter `src/`.

## ‚ñ∂Ô∏è Schnellstart

| Ziel | Befehl |
|---|---|
| Ein Bild beschriften | `python image2caption.py -I /path/to/image.jpg -S L -C model.pt` |
| Ein Video beschriften | `python v2c.py -V /path/to/video.mp4 -N 10` |
| Datensatz aufbauen | `python dataset_generation.py` |

### Bildbeschriftung (schneller Durchlauf)

```bash
python image2caption.py -I /path/to/image.jpg -S L -C model.pt
```

### Video-Beschriftung (empfohlener Weg)

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

## üéØ Nutzung

### 1. Bildbeschriftung (`image2caption.py`)

```bash
python image2caption.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

Argumente:

- `-I, --img-path`: Pfad des Eingabebildes.
- `-S, --size`: Modellgr√∂√üe (`S` oder `L`).
- `-C, --checkpoint-name`: Checkpoint-Dateiname in `weights/{small|large}`.
- `-R, --res-path`: Ausgabeverzeichnis f√ºr gerendertes Bild mit Caption.
- `-T, --temperature`: Sampling-Temperatur.

### 2. Alternative Bild-CLI (`predict.py`)

```bash
python predict.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

`predict.py` ist funktional √§hnlich zu `image2caption.py`; die Textformatierung der Ausgabe unterscheidet sich leicht.

### 3. Bildbeschriftungs-Klassen-API (`i2c.py`)

```bash
python i2c.py -I /path/to/image.jpg -S L -C model.pt -R ./data/result/prediction -T 1.0
```

Oder importieren in einem eigenen Skript:

```python
from i2c import ImageCaptioner

captioner = ImageCaptioner(model_size="L", checkpoint_name="model.pt")
captioner.set_image_path("/path/to/image.jpg")
caption = captioner.generate_caption(save_image=True)
print(caption)
```

### 4. Video zu Untertiteln + JSON (`v2c.py`)

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

Ausgaben neben dem Eingabevideo:

- `<video_basename>_caption.srt`
- `<video_basename>_caption.json`
- `<video_basename>_captioning_frames/`

### 5. Alternative Videopipeline (`video2caption.py`)

```bash
python video2caption.py -V /path/to/video.mp4 -N 10
```

Wichtig: Dieses Skript enth√§lt derzeit maschinenspezifische hartkodierte Pfade:

- Standard-Python-Pfad: `/home/lachlan/miniconda3/envs/caption/bin/python`
- Caption-Skriptpfad: `/home/lachlan/Projects/image_captioning/clip-gpt-captioning/src/image2caption.py`

Nutze `v2c.py`, es sei denn, du pflegst diese Pfade absichtlich weiter.

### 6. Legacy-Variante (`video2caption_v1.1.py`)

Dieses Skript wird als historische Referenz aufbewahrt. F√ºr aktive Nutzung bitte `v2c.py` bevorzugen.

### 7. Datensatzgenerierung

```bash
python dataset_generation.py
```

Erwartete Rohdaten:

- `data/raw/results.csv` (Pipes-getrennte Caption-Tabelle).
- `data/raw/flickr30k_images/` (Bilddateien, auf die sich die CSV bezieht).

Ausgabe:

- `data/processed/dataset.pkl`

### 8. Training

```bash
python training.py -S L -C model.pt
```

Training nutzt standardm√§√üig Weights & Biases-Logging (`wandb`).

### 9. Evaluation

```bash
python evaluate.py \
  -I ./data/raw/flickr30k_images \
  -R ./data/result/eval \
  -S L \
  -C model.pt \
  -T 1.0
```

Evaluation rendert vorhergesagte Captions auf Testbildern und speichert sie unter:

- `<res-path>/<checkpoint_name_without_ext>_<SIZE>/`

## ‚öôÔ∏è Konfiguration

Modellkonfigurationen sind in `utils/config.py` definiert:

| Config | CLIP-Backbone | GPT-Modell | Gewichtsordner |
|---|---|---|---|
| `ConfigS` | `openai/clip-vit-base-patch32` | `gpt2` | `weights/small` |
| `ConfigL` | `openai/clip-vit-large-patch14` | `gpt2-medium` | `weights/large` |

Wichtige Standardwerte aus den Config-Klassen:

| Feld | `ConfigS` | `ConfigL` |
|---|---:|---:|
| `epochs` | 150 | 120 |
| `lr` | 3e-3 | 5e-3 |
| `batch_size_exp` | 6 | 5 |
| `ep_len` | 4 | 4 |
| `max_len` | 40 | 40 |

Automatische Checkpoint-Download-IDs befinden sich in `utils/downloads.py`:

| Gr√∂√üe | Google Drive-ID |
|---|---|
| `L` | `1Gh32arzhW06C1ZJyzcJSSfdJDi3RgWoG` |
| `S` | `1pSQruQyg8KJq6VmzhMLFbT_VaHJMdlWF` |

## üì¶ Ausgabedateien

### Bild-Inferenz

- Gespeichertes Bild mit √ºberlagerter / generierter √úberschrift unter `--res-path`.
- Dateinamenmuster: `<input_stem>-R<SIZE>.jpg`.

### Video-Inferenz (`v2c.py`)

- SRT: `<video_stem>_caption.srt`
- JSON: `<video_stem>_caption.json`
- Frame-Bilder: `<video_stem>_captioning_frames/`

Beispiel f√ºr ein JSON-Element:

```json
{
  "start": "00:00:03,200",
  "end": "00:00:03,700",
  "lang": "en",
  "text": "A dog running through a field."
}
```

## üß™ Beispiele

### Schnelles Bildbeschriftungsbeispiel

```bash
python image2caption.py -I ./examples/dog.jpg -S S -C model.pt
```

Erwartetes Verhalten:

- Falls `weights/small/model.pt` fehlt, wird sie heruntergeladen.
- Standardm√§√üig wird ein Bild mit Caption in `./data/result/prediction` geschrieben.
- Der Beschriftungstext wird auf stdout ausgegeben.

### Schnelles Video-Beschriftungsbeispiel

```bash
python v2c.py -V ./examples/demo.mp4 -N 8
```

Erwartetes Verhalten:

- 8 gleichm√§√üig gesampelte Frames werden beschriftet.
- `.srt`- und `.json`-Dateien werden neben dem Eingabevideo erzeugt.

### End-to-End-Abfolge f√ºr Training/Evaluation

```bash
python dataset_generation.py
python training.py -S L -C model.pt
python evaluate.py -I ./data/raw/flickr30k_images -R ./data/result/eval -S L -C model.pt -T 1.0
```

## üß≠ Entwicklungshinweise

- Legacy-√úberschneidungen bestehen zwischen `v2c.py`, `video2caption.py` und `video2caption_v1.*`.
- `video2caption_v1.0_not_work.py` wird absichtlich als nicht funktionsf√§higer Legacy-Code beibehalten.
- `training.py` w√§hlt derzeit `ConfigL()` √ºber `config = ConfigL() if args.size.upper() else ConfigS()`; f√ºr nicht-leere `--size`-Werte wird dadurch immer `ConfigL` verwendet.
- `model/trainer.py` nutzt in `test_step` `self.dataset`, obwohl der Initializer `self.test_dataset` setzt; das kann das Sampling in Trainingsl√§ufen brechen, falls nicht angepasst.
- `video2caption_v1.1.py` referenziert `self.config.transform`, aber `ConfigS`/`ConfigL` definieren `transform` nicht.
- In diesem Repository-Snapshot ist derzeit keine CI/Test-Suite definiert.
- i18n-Hinweis: Sprachlinks sind am Anfang dieser README vorhanden; unter `i18n/` k√∂nnen √ºbersetzte Dateien erg√§nzt werden.
- Aktueller Stand: Die Sprachleiste verlinkt auf `i18n/README.ru.md`, doch diese Datei ist in diesem Snapshot nicht vorhanden.

## ü©∫ Fehlerbehebung

- `AssertionError: Image does not exist`
  - Pr√ºfe, ob `-I/--img-path` auf eine g√ºltige Datei verweist.
- `Dataset file not found. Downloading...`
  - `MiniFlickrDataset` wirft diese Meldung, wenn `data/processed/dataset.pkl` fehlt; zuerst `python dataset_generation.py` ausf√ºhren.
- `Path to the test image folder does not exist`
  - Pr√ºfe, ob `evaluate.py -I` auf einen existierenden Ordner zeigt.
- Langsame oder fehlerhafte erste Ausf√ºhrung
  - Der erste Lauf l√§dt Modelle von Hugging Face und ggf. Checkpoints von Google Drive herunter.
- `video2caption.py` gibt leere Beschriftungen aus
  - √úberpr√ºfe den hartkodierten Skriptpfad und den Python-Executable-Pfad oder wechsle auf `v2c.py`.
- `wandb` fragt beim Training nach Anmeldung
  - `wandb login` ausf√ºhren oder Logging in `training.py` bei Bedarf manuell deaktivieren.

## üõ£Ô∏è Roadmap

- Abh√§ngigkeits-Lockfiles (`requirements.txt` oder `pyproject.toml`) f√ºr reproduzierbare Installationen erg√§nzen.
- Doppelte Video-Pipelines in eine gepflegte Implementierung konsolidieren.
- Harte, ger√§tespezifische Pfade aus Legacy-Skripten entfernen.
- Bekannte Trainings-/Evaluations-Edge-Cases in `training.py` und `model/trainer.py` beheben.
- Automatisierte Tests und CI hinzuf√ºgen.
- `i18n/` mit den in der Sprachleiste referenzierten √úbersetzungen f√ºllen.

## ü§ù Mitwirken

Beitr√§ge sind willkommen. Vorgeschlagener Ablauf:

```bash
# 1) Fork und Klonen
git clone git@github.com:<your-user>/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

# 2) Feature-Branch erstellen
git checkout -b feat/your-change

# 3) √Ñnderungen durchf√ºhren und committen
git add .
git commit -m "feat: describe your change"

# 4) Pushen und PR √∂ffnen
git push origin feat/your-change
```

Wenn du das Modellverhalten √§nderst, f√ºge bitte hinzu:

- Reproduzierbare(n) Befehl(e).
- Vorher-/Nachher-Beispielausgaben.
- Hinweise zu Checkpoint- oder Datensatzannahmen.

## ‚ù§Ô∏è Support

| Donate | PayPal | Stripe |
|---|---|---|
| [![Donate](https://img.shields.io/badge/Donate-LazyingArt-0EA5E9?style=for-the-badge&logo=ko-fi&logoColor=white)](https://chat.lazying.art/donate) | [![PayPal](https://img.shields.io/badge/PayPal-RongzhouChen-00457C?style=for-the-badge&logo=paypal&logoColor=white)](https://paypal.me/RongzhouChen) | [![Stripe](https://img.shields.io/badge/Stripe-Donate-635BFF?style=for-the-badge&logo=stripe&logoColor=white)](https://buy.stripe.com/aFadR8gIaflgfQV6T4fw400) |

## üìÑ Lizenz

Keine Lizenzdatei ist in der aktuellen Repository-Version vorhanden.

Annahmepostulat: Bis eine `LICENSE`-Datei hinzugef√ºgt wird, sind Nutzungs-/Verteilungsbedingungen nicht festgelegt.

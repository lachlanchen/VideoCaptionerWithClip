[English](../README.md) Â· [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](README.ar.md) Â· [EspaÃ±ol](README.es.md) Â· [FranÃ§ais](README.fr.md) Â· [æ—¥æœ¬èª](README.ja.md) Â· [í•œêµ­ì–´](README.ko.md) Â· [Tiáº¿ng Viá»‡t](README.vi.md) Â· [ä¸­æ–‡ (ç®€ä½“)](README.zh-Hans.md) Â· [ä¸­æ–‡ï¼ˆç¹é«”ï¼‰](README.zh-Hant.md) Â· [Deutsch](README.de.md) Â· [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README.ru.md)


# Clip-GPT-Captioning

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow)
![Status](https://img.shields.io/badge/README-Expanded-success)
![Repo Layout](https://img.shields.io/badge/Layout-Root%20Scripts-informational)
![Legacy Scripts](https://img.shields.io/badge/Legacy%20Scripts-Present-orange)
![i18n](https://img.shields.io/badge/i18n-Enabled-brightgreen)
![Maintained Path](https://img.shields.io/badge/Video-v2c.py-2ea44f)

é€™æ˜¯ä¸€å€‹ Python å·¥å…·åŒ…ï¼Œé€éçµåˆ OpenAI CLIP è¦–è¦ºåµŒå…¥èˆ‡ GPT é¢¨æ ¼èªè¨€æ¨¡å‹ï¼Œç‚ºåœ–ç‰‡èˆ‡å½±ç‰‡ç”¢ç”Ÿè‡ªç„¶èªè¨€æè¿°ã€‚

## âœ¨ æ¦‚è¦½

æ­¤å„²å­˜åº«æä¾›ï¼š

- ç”¨æ–¼åœ–ç‰‡æè¿°èˆ‡å½±ç‰‡å­—å¹•ç”Ÿæˆçš„æ¨è«–è…³æœ¬ã€‚
- é€é CLIP è¦–è¦ºåµŒå…¥å°æ˜ åˆ° GPT-2 token åµŒå…¥çš„è¨“ç·´æµç¨‹ã€‚
- Flickr30k é¢¨æ ¼è³‡æ–™é›†çš„ç”Ÿæˆå·¥å…·ã€‚
- ç•¶æ¬Šé‡ç¼ºå¤±æ™‚ï¼Œæ”¯æ´æ¨¡å‹å°ºå¯¸çš„è‡ªå‹• checkpoint ä¸‹è¼‰ã€‚
- ä½æ–¼ `i18n/` çš„å¤šèª README ç‰ˆæœ¬ï¼ˆè¦‹ä¸Šæ–¹èªè¨€åˆ—ï¼‰ã€‚

ç›®å‰å¯¦ä½œåŒæ™‚åŒ…å«è¼ƒæ–°çš„è…³æœ¬èˆ‡èˆŠç‰ˆè…³æœ¬ã€‚éƒ¨åˆ†èˆŠç‰ˆæª”æ¡ˆç‚ºåƒè€ƒç”¨é€”è€Œä¿ç•™ï¼Œä¸¦å·²åœ¨ä¸‹æ–‡èªªæ˜ã€‚

## ğŸš€ åŠŸèƒ½

- é€é `image2caption.py` é€²è¡Œå–®å¼µåœ–ç‰‡æè¿°ã€‚
- é€é `v2c.py` æˆ– `video2caption.py` é€²è¡Œå½±ç‰‡æè¿°ï¼ˆå‡å‹»æŠ½å¹€ï¼‰ã€‚
- å¯è‡ªè¨‚åŸ·è¡Œé¸é …ï¼š
  - å½±æ ¼æ•¸é‡ã€‚
  - æ¨¡å‹å°ºå¯¸ã€‚
  - å–æ¨£æº«åº¦ã€‚
  - Checkpoint åç¨±ã€‚
- å¤šç¨‹åº/å¤šåŸ·è¡Œç·’æè¿°æµç¨‹ä»¥åŠ é€Ÿå½±ç‰‡æ¨è«–ã€‚
- è¼¸å‡ºç”¢ç‰©ï¼š
  - SRT å­—å¹•æª”ï¼ˆ`.srt`ï¼‰ã€‚
  - `v2c.py` ç”¢ç”Ÿçš„ JSON é€å­—ç¨¿ï¼ˆ`.json`ï¼‰ã€‚
- ç”¨æ–¼ CLIP+GPT2 å°æ˜ å¯¦é©—çš„è¨“ç·´èˆ‡è©•ä¼°å…¥å£ã€‚

### ä¸€è¦½

| å€åŸŸ | ä¸»è¦è…³æœ¬ | å‚™è¨» |
|---|---|---|
| åœ–ç‰‡æè¿° | `image2caption.py`, `i2c.py`, `predict.py` | CLI + å¯é‡ç”¨é¡åˆ¥ |
| å½±ç‰‡æè¿° | `v2c.py` | å»ºè­°çš„ç¶­è­·è·¯å¾‘ |
| èˆŠç‰ˆå½±ç‰‡æµç¨‹ | `video2caption.py`, `video2caption_v1.1.py` | å«æ©Ÿå™¨ç‰¹å®šå‡è¨­ |
| è³‡æ–™é›†å»ºç½® | `dataset_generation.py` | ç”¢ç”Ÿ `data/processed/dataset.pkl` |
| è¨“ç·´ / è©•ä¼° | `training.py`, `evaluate.py` | ä½¿ç”¨ CLIP+GPT2 å°æ˜  |

## ğŸ§± æ¶æ§‹ï¼ˆé«˜å±¤ï¼‰

`model/model.py` ä¸­çš„æ ¸å¿ƒæ¨¡å‹åŒ…å«ä¸‰å€‹éƒ¨åˆ†ï¼š

1. `ImageEncoder`ï¼šèƒå– CLIP åœ–åƒåµŒå…¥ã€‚
2. `Mapping`ï¼šå°‡ CLIP åµŒå…¥æŠ•å½±ç‚º GPT å‰ç¶´åµŒå…¥åºåˆ—ã€‚
3. `TextDecoder`ï¼šGPT-2 èªè¨€æ¨¡å‹é ­ï¼Œä»¥è‡ªå›æ­¸æ–¹å¼ç”Ÿæˆæè¿° tokenã€‚

è¨“ç·´ï¼ˆ`Net.train_forward`ï¼‰ä½¿ç”¨é å…ˆè¨ˆç®—çš„ CLIP åœ–åƒåµŒå…¥èˆ‡åˆ†è©å¾Œæè¿°ã€‚
æ¨è«–ï¼ˆ`Net.forward`ï¼‰ä½¿ç”¨ PIL åœ–ç‰‡ï¼Œè§£ç¢¼ token ç›´åˆ° EOS æˆ– `max_len`ã€‚

### è³‡æ–™æµç¨‹

1. æº–å‚™è³‡æ–™é›†ï¼š`dataset_generation.py` è®€å– `data/raw/results.csv` èˆ‡ `data/raw/flickr30k_images/` ä¸­åœ–ç‰‡ï¼Œå¯«å…¥ `data/processed/dataset.pkl`ã€‚
2. è¨“ç·´ï¼š`training.py` è¼‰å…¥ pickled tuple `(image_name, image_embedding, caption)` ä¸¦è¨“ç·´ mapper/decoder å±¤ã€‚
3. è©•ä¼°ï¼š`evaluate.py` åœ¨ä¿ç•™æ¸¬è©¦åœ–ç‰‡ä¸Šæ¸²æŸ“ç”Ÿæˆæè¿°ã€‚
4. æä¾›æ¨è«–ï¼š
   - åœ–ç‰‡ï¼š`image2caption.py` / `predict.py` / `i2c.py`ã€‚
   - å½±ç‰‡ï¼š`v2c.py`ï¼ˆå»ºè­°ï¼‰ã€`video2caption.py`ï¼ˆèˆŠç‰ˆï¼‰ã€‚

## ğŸ—‚ï¸ å°ˆæ¡ˆçµæ§‹

```text
VideoCaptionerWithClip/
â”œâ”€â”€ README.md
â”œâ”€â”€ image2caption.py               # Single-image caption CLI
â”œâ”€â”€ predict.py                     # Alternate single-image caption CLI
â”œâ”€â”€ i2c.py                         # Reusable ImageCaptioner class + CLI
â”œâ”€â”€ v2c.py                         # Video -> SRT + JSON (threaded frame captioning)
â”œâ”€â”€ video2caption.py               # Alternate video -> SRT implementation (legacy constraints)
â”œâ”€â”€ video2caption_v1.1.py          # Older variant
â”œâ”€â”€ video2caption_v1.0_not_work.py # Explicitly marked non-working legacy file
â”œâ”€â”€ training.py                    # Model training entrypoint
â”œâ”€â”€ evaluate.py                    # Test-split evaluation and rendered outputs
â”œâ”€â”€ dataset_generation.py          # Builds data/processed/dataset.pkl
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dataset.py                 # Dataset + DataLoader helpers
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py                   # CLIP encoder + mapping + GPT2 decoder
â”‚   â””â”€â”€ trainer.py                 # Training/validation/test utility class
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                  # ConfigS / ConfigL defaults
â”‚   â”œâ”€â”€ downloads.py               # Google Drive checkpoint downloader
â”‚   â””â”€â”€ lr_warmup.py               # LR warmup schedule
â”œâ”€â”€ i18n/                          # Multilingual README variants
â””â”€â”€ .auto-readme-work/             # Auto-README pipeline artifacts
```

## ğŸ“‹ å…ˆæ±ºæ¢ä»¶

- å»ºè­°ä½¿ç”¨ Python `3.10+`ã€‚
- å¯ä½¿ç”¨ CUDA çš„ GPU ç‚ºå¯é¸ï¼Œä½†å¼·çƒˆå»ºè­°ç”¨æ–¼è¨“ç·´èˆ‡å¤§å‹æ¨¡å‹æ¨è«–ã€‚
- ç›®å‰è…³æœ¬ä¸ç›´æ¥éœ€è¦ `ffmpeg`ï¼ˆå½±æ ¼æ“·å–ä½¿ç”¨ OpenCVï¼‰ã€‚
- é¦–æ¬¡åŸ·è¡Œä¸‹è¼‰ Hugging Face / Google Drive çš„æ¨¡å‹èˆ‡ checkpoint æ™‚éœ€è¦ç¶²è·¯é€£ç·šã€‚

ç›®å‰æ²’æœ‰ lockfileï¼ˆç¼ºå°‘ `requirements.txt` / `pyproject.toml`ï¼‰ï¼Œå› æ­¤ä¾è³´å¥—ä»¶ç”± import å…§å®¹æ¨æ–·ã€‚

## ğŸ› ï¸ å®‰è£

### ä¾ç›®å‰å„²å­˜åº«çµæ§‹çš„æ¨™æº–å®‰è£

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

python -m venv .venv
source .venv/bin/activate

pip install --upgrade pip
pip install torch torchvision torchaudio
pip install transformers pillow matplotlib numpy tqdm opencv-python pandas wandb gdown
```

### åŸå§‹ README å®‰è£ç‰‡æ®µï¼ˆä¿ç•™ï¼‰

å…ˆå‰ README åœ¨ç¨‹å¼ç¢¼å€å¡Šä¸­é€”çµæŸã€‚ä¸‹åˆ—åŸå§‹æŒ‡ä»¤å®Œæ•´ä¿ç•™ï¼Œä½œç‚ºå…·æ¬Šå¨æ€§çš„æ­·å²å…§å®¹ï¼š

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip/src
```

æ³¨æ„ï¼šç›®å‰å„²å­˜åº«å¿«ç…§å°‡è…³æœ¬æ”¾åœ¨ repo æ ¹ç›®éŒ„ï¼Œè€Œé `src/`ã€‚

## â–¶ï¸ å¿«é€Ÿé–‹å§‹

### åœ–ç‰‡æè¿°ï¼ˆå¿«é€ŸåŸ·è¡Œï¼‰

```bash
python image2caption.py -I /path/to/image.jpg -S L -C model.pt
```

### å½±ç‰‡æè¿°ï¼ˆå»ºè­°è·¯å¾‘ï¼‰

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

## ğŸ¯ ä½¿ç”¨æ–¹å¼

### 1. åœ–ç‰‡æè¿°ï¼ˆ`image2caption.py`ï¼‰

```bash
python image2caption.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

åƒæ•¸ï¼š

- `-I, --img-path`ï¼šè¼¸å…¥åœ–ç‰‡è·¯å¾‘ã€‚
- `-S, --size`ï¼šæ¨¡å‹å°ºå¯¸ï¼ˆ`S` æˆ– `L`ï¼‰ã€‚
- `-C, --checkpoint-name`ï¼š`weights/{small|large}` ä¸­çš„ checkpoint æª”åã€‚
- `-R, --res-path`ï¼šè¼¸å‡ºåŠ ä¸Šæè¿°æ–‡å­—åœ–ç‰‡çš„ç›®éŒ„ã€‚
- `-T, --temperature`ï¼šå–æ¨£æº«åº¦ã€‚

### 2. æ›¿ä»£åœ–ç‰‡ CLIï¼ˆ`predict.py`ï¼‰

```bash
python predict.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

`predict.py` åŠŸèƒ½èˆ‡ `image2caption.py` é¡ä¼¼ï¼›è¼¸å‡ºæ–‡å­—æ ¼å¼ç•¥æœ‰å·®ç•°ã€‚

### 3. åœ–ç‰‡æè¿°é¡åˆ¥ APIï¼ˆ`i2c.py`ï¼‰

```bash
python i2c.py -I /path/to/image.jpg -S L -C model.pt -R ./data/result/prediction -T 1.0
```

æˆ–åœ¨ä½ çš„è…³æœ¬ä¸­åŒ¯å…¥ï¼š

```python
from i2c import ImageCaptioner

captioner = ImageCaptioner(model_size="L", checkpoint_name="model.pt")
captioner.set_image_path("/path/to/image.jpg")
caption = captioner.generate_caption(save_image=True)
print(caption)
```

### 4. å½±ç‰‡è½‰å­—å¹• + JSONï¼ˆ`v2c.py`ï¼‰

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

è¼¸å‡ºæ–¼è¼¸å…¥å½±ç‰‡æ—ï¼š

- `<video_basename>_caption.srt`
- `<video_basename>_caption.json`
- `<video_basename>_captioning_frames/`

### 5. æ›¿ä»£å½±ç‰‡æµç¨‹ï¼ˆ`video2caption.py`ï¼‰

```bash
python video2caption.py -V /path/to/video.mp4 -N 10
```

é‡è¦ï¼šæ­¤è…³æœ¬ç›®å‰åŒ…å«æ©Ÿå™¨ç‰¹å®šç¡¬ç·¨ç¢¼è·¯å¾‘ï¼š

- Python è·¯å¾‘é è¨­ï¼š`/home/lachlan/miniconda3/envs/caption/bin/python`
- æè¿°è…³æœ¬è·¯å¾‘ï¼š`/home/lachlan/Projects/image_captioning/clip-gpt-captioning/src/image2caption.py`

é™¤éä½ æ‰“ç®—ç¶­è­·é€™äº›è·¯å¾‘ï¼Œå¦å‰‡è«‹ä½¿ç”¨ `v2c.py`ã€‚

### 6. èˆŠç‰ˆè®Šé«”ï¼ˆ`video2caption_v1.1.py`ï¼‰

æ­¤è…³æœ¬ä¿ç•™ä½œç‚ºæ­·å²åƒè€ƒã€‚å¯¦éš›ä½¿ç”¨è«‹å„ªå…ˆé¸æ“‡ `v2c.py`ã€‚

### 7. ç”Ÿæˆè³‡æ–™é›†

```bash
python dataset_generation.py
```

é æœŸåŸå§‹è¼¸å…¥ï¼š

- `data/raw/results.csv`ï¼ˆä»¥ pipe åˆ†éš”çš„æè¿°è¡¨æ ¼ï¼‰ã€‚
- `data/raw/flickr30k_images/`ï¼ˆCSV å¼•ç”¨çš„åœ–ç‰‡æª”ï¼‰ã€‚

è¼¸å‡ºï¼š

- `data/processed/dataset.pkl`

### 8. è¨“ç·´

```bash
python training.py -S L -C model.pt
```

è¨“ç·´é è¨­å•Ÿç”¨ Weights & Biasesï¼ˆ`wandb`ï¼‰è¨˜éŒ„ã€‚

### 9. è©•ä¼°

```bash
python evaluate.py \
  -I ./data/raw/flickr30k_images \
  -R ./data/result/eval \
  -S L \
  -C model.pt \
  -T 1.0
```

è©•ä¼°æœƒå°‡é æ¸¬æè¿°æ¸²æŸ“åˆ°æ¸¬è©¦åœ–ç‰‡ä¸Šï¼Œä¸¦å„²å­˜æ–¼ï¼š

- `<res-path>/<checkpoint_name_without_ext>_<SIZE>/`

## âš™ï¸ è¨­å®š

æ¨¡å‹è¨­å®šå®šç¾©æ–¼ `utils/config.py`ï¼š

| è¨­å®š | CLIP backbone | GPT model | æ¬Šé‡ç›®éŒ„ |
|---|---|---|---|
| `ConfigS` | `openai/clip-vit-base-patch32` | `gpt2` | `weights/small` |
| `ConfigL` | `openai/clip-vit-large-patch14` | `gpt2-medium` | `weights/large` |

è¨­å®šé¡åˆ¥çš„é—œéµé è¨­å€¼ï¼š

| æ¬„ä½ | `ConfigS` | `ConfigL` |
|---|---:|---:|
| `epochs` | 150 | 120 |
| `lr` | 3e-3 | 5e-3 |
| `batch_size_exp` | 6 | 5 |
| `ep_len` | 4 | 4 |
| `max_len` | 40 | 40 |

Checkpoint è‡ªå‹•ä¸‹è¼‰ ID ä½æ–¼ `utils/downloads.py`ï¼š

| å°ºå¯¸ | Google Drive ID |
|---|---|
| `L` | `1Gh32arzhW06C1ZJyzcJSSfdJDi3RgWoG` |
| `S` | `1pSQruQyg8KJq6VmzhMLFbT_VaHJMdlWF` |

## ğŸ“¦ è¼¸å‡ºæª”æ¡ˆ

### åœ–ç‰‡æ¨è«–

- å„²å­˜å¸¶æœ‰è¦†è“‹/ç”Ÿæˆæ¨™é¡Œçš„åœ–ç‰‡è‡³ `--res-path`ã€‚
- æª”åæ ¼å¼ï¼š`<input_stem>-R<SIZE>.jpg`ã€‚

### å½±ç‰‡æ¨è«–ï¼ˆ`v2c.py`ï¼‰

- SRTï¼š`<video_stem>_caption.srt`
- JSONï¼š`<video_stem>_caption.json`
- å½±æ ¼åœ–ç‰‡ï¼š`<video_stem>_captioning_frames/`

JSON å…ƒç´ ç¯„ä¾‹ï¼š

```json
{
  "start": "00:00:03,200",
  "end": "00:00:03,700",
  "lang": "en",
  "text": "A dog running through a field."
}
```

## ğŸ§ª ç¯„ä¾‹

### å¿«é€Ÿåœ–ç‰‡æè¿°ç¯„ä¾‹

```bash
python image2caption.py -I ./examples/dog.jpg -S S -C model.pt
```

é æœŸè¡Œç‚ºï¼š

- è‹¥ç¼ºå°‘ `weights/small/model.pt`ï¼Œæœƒè‡ªå‹•ä¸‹è¼‰ã€‚
- é è¨­æœƒå°‡æè¿°åœ–ç‰‡è¼¸å‡ºåˆ° `./data/result/prediction`ã€‚
- æè¿°æ–‡å­—æœƒè¼¸å‡ºåˆ° stdoutã€‚

### å¿«é€Ÿå½±ç‰‡æè¿°ç¯„ä¾‹

```bash
python v2c.py -V ./examples/demo.mp4 -N 8
```

é æœŸè¡Œç‚ºï¼š

- æœƒå° 8 å€‹å‡å‹»æŠ½æ¨£å½±æ ¼ç”¢ç”Ÿæè¿°ã€‚
- `.srt` èˆ‡ `.json` æª”æœƒåœ¨è¼¸å…¥å½±ç‰‡æ—ç”Ÿæˆã€‚

### ç«¯åˆ°ç«¯è¨“ç·´/è©•ä¼°æµç¨‹

```bash
python dataset_generation.py
python training.py -S L -C model.pt
python evaluate.py -I ./data/raw/flickr30k_images -R ./data/result/eval -S L -C model.pt -T 1.0
```

## ğŸ§­ é–‹ç™¼èªªæ˜

- `v2c.py`ã€`video2caption.py` èˆ‡ `video2caption_v1.*` ä¹‹é–“å­˜åœ¨èˆŠç‰ˆåŠŸèƒ½é‡ç–Šã€‚
- `video2caption_v1.0_not_work.py` åˆ»æ„ä¿ç•™ç‚ºä¸å¯é‹ä½œçš„èˆŠç‰ˆç¨‹å¼ç¢¼ã€‚
- `training.py` ç›®å‰é€é `config = ConfigL() if args.size.upper() else ConfigS()` é¸æ“‡ `ConfigL()`ï¼Œå°éç©º `--size` å€¼éƒ½æœƒè§£æç‚º `ConfigL`ã€‚
- `model/trainer.py` åœ¨ `test_step` ä½¿ç”¨ `self.dataset`ï¼Œä½†åˆå§‹åŒ–æ™‚æŒ‡å®šçš„æ˜¯ `self.test_dataset`ï¼›è‹¥ä¸èª¿æ•´ï¼Œè¨“ç·´æµç¨‹ä¸­çš„æŠ½æ¨£å¯èƒ½å¤±æ•—ã€‚
- `video2caption_v1.1.py` åƒè€ƒäº† `self.config.transform`ï¼Œä½† `ConfigS`/`ConfigL` ä¸¦æœªå®šç¾© `transform`ã€‚
- ç›®å‰å„²å­˜åº«å¿«ç…§æœªå®šç¾© CI/æ¸¬è©¦å¥—ä»¶ã€‚
- i18n èªªæ˜ï¼šæœ¬ README é ‚éƒ¨å·²æœ‰èªè¨€é€£çµï¼›ç¿»è­¯æª”å¯æ–°å¢æ–¼ `i18n/`ã€‚
- ç›®å‰ç‹€æ…‹èªªæ˜ï¼šèªè¨€åˆ—é€£åˆ° `i18n/README.ru.md`ï¼Œä½†æ­¤å¿«ç…§ä¸­è©²æª”æ¡ˆä¸å­˜åœ¨ã€‚

## ğŸ©º ç–‘é›£æ’è§£

- `AssertionError: Image does not exist`
  - ç¢ºèª `-I/--img-path` æŒ‡å‘æœ‰æ•ˆæª”æ¡ˆã€‚
- `Dataset file not found. Downloading...`
  - ç•¶ `data/processed/dataset.pkl` ç¼ºå¤±æ™‚ï¼Œ`MiniFlickrDataset` æœƒæ‹‹å‡ºæ­¤è¨Šæ¯ï¼›è«‹å…ˆåŸ·è¡Œ `python dataset_generation.py`ã€‚
- `Path to the test image folder does not exist`
  - ç¢ºèª `evaluate.py -I` æŒ‡å‘ç¾æœ‰è³‡æ–™å¤¾ã€‚
- é¦–æ¬¡åŸ·è¡Œéæ…¢æˆ–å¤±æ•—
  - åˆæ¬¡åŸ·è¡Œæœƒä¸‹è¼‰ Hugging Face æ¨¡å‹ï¼Œä¹Ÿå¯èƒ½å¾ Google Drive ä¸‹è¼‰ checkpointã€‚
- `video2caption.py` å›å‚³ç©ºç™½æè¿°
  - è«‹æª¢æŸ¥ç¡¬ç·¨ç¢¼è…³æœ¬è·¯å¾‘èˆ‡ Python åŸ·è¡Œæª”è·¯å¾‘ï¼Œæˆ–æ”¹ç”¨ `v2c.py`ã€‚
- è¨“ç·´æ™‚ `wandb` è¦æ±‚ç™»å…¥
  - åŸ·è¡Œ `wandb login`ï¼Œæˆ–è¦–éœ€æ±‚åœ¨ `training.py` æ‰‹å‹•é—œé–‰è¨˜éŒ„ã€‚

## ğŸ›£ï¸ è·¯ç·šåœ–

- æ–°å¢ä¾è³´ lockfileï¼ˆ`requirements.txt` æˆ– `pyproject.toml`ï¼‰ä»¥ä¾¿å¯é‡ç¾å®‰è£ã€‚
- å°‡é‡è¤‡çš„å½±ç‰‡æµç¨‹æ•´åˆç‚ºå–®ä¸€ç¶­è­·å¯¦ä½œã€‚
- ç§»é™¤èˆŠç‰ˆè…³æœ¬ä¸­çš„æ©Ÿå™¨ç¡¬ç·¨ç¢¼è·¯å¾‘ã€‚
- ä¿®æ­£ `training.py` èˆ‡ `model/trainer.py` å·²çŸ¥çš„è¨“ç·´/è©•ä¼°é‚Šç•Œæ¡ˆä¾‹éŒ¯èª¤ã€‚
- æ–°å¢è‡ªå‹•åŒ–æ¸¬è©¦èˆ‡ CIã€‚
- è£œé½Š `i18n/` ä¸­èªè¨€åˆ—æ‰€å¼•ç”¨çš„ README ç¿»è­¯æª”ã€‚

## ğŸ¤ è²¢ç»

æ­¡è¿è²¢ç»ã€‚å»ºè­°æµç¨‹ï¼š

```bash
# 1) Fork and clone
git clone git@github.com:<your-user>/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

# 2) Create a feature branch
git checkout -b feat/your-change

# 3) Make changes and commit
git add .
git commit -m "feat: describe your change"

# 4) Push and open a PR
git push origin feat/your-change
```

è‹¥ä½ è®Šæ›´äº†æ¨¡å‹è¡Œç‚ºï¼Œè«‹é™„ä¸Šï¼š

- å¯é‡ç¾çš„æŒ‡ä»¤ã€‚
- è®Šæ›´å‰/å¾Œçš„ç¯„ä¾‹è¼¸å‡ºã€‚
- é—œæ–¼ checkpoint æˆ–è³‡æ–™é›†å‡è¨­çš„èªªæ˜ã€‚

## ğŸ™Œ æ”¯æ´

ç›®å‰å„²å­˜åº«å¿«ç…§æœªåŒ…å«æ˜ç¢ºçš„æåŠ©/è´ŠåŠ©è¨­å®šã€‚

è‹¥æ—¥å¾Œæ–°å¢è´ŠåŠ©é€£çµï¼Œæ‡‰ä¿ç•™æ–¼æœ¬ç¯€ã€‚

## ğŸ“„ æˆæ¬Š

ç›®å‰å„²å­˜åº«å¿«ç…§æœªåŒ…å«æˆæ¬Šæª”æ¡ˆã€‚

å‡è¨­èªªæ˜ï¼šåœ¨æ–°å¢ `LICENSE` æª”ä¹‹å‰ï¼Œé‡ç”¨/æ•£ä½ˆæ¢æ¬¾å‡æœªå®šç¾©ã€‚

[English](../README.md) Â· [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](README.ar.md) Â· [EspaÃ±ol](README.es.md) Â· [FranÃ§ais](README.fr.md) Â· [æ—¥æœ¬èª](README.ja.md) Â· [í•œêµ­ì–´](README.ko.md) Â· [Tiáº¿ng Viá»‡t](README.vi.md) Â· [ä¸­æ–‡ (ç®€ä½“)](README.zh-Hans.md) Â· [ä¸­æ–‡ï¼ˆç¹é«”ï¼‰](README.zh-Hant.md) Â· [Deutsch](README.de.md) Â· [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README.ru.md)


[![LazyingArt banner](https://github.com/lachlanchen/lachlanchen/raw/main/figs/banner.png)](https://github.com/lachlanchen/lachlanchen/blob/main/figs/banner.png)

# Clip-GPT-Captioning

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow)
![Status](https://img.shields.io/badge/README-Expanded-success)
![Repo Layout](https://img.shields.io/badge/Layout-Root%20Scripts-informational)
![Legacy Scripts](https://img.shields.io/badge/Legacy%20Scripts-Present-orange)
![i18n](https://img.shields.io/badge/i18n-Enabled-brightgreen)
![Maintained Path](https://img.shields.io/badge/Video-v2c.py-2ea44f)

é€™æ˜¯ä¸€å€‹ Python å·¥å…·åŒ…ï¼Œé€éçµåˆ OpenAI CLIP è¦–è¦ºåµŒå…¥èˆ‡ GPT é¢¨æ ¼èªè¨€æ¨¡å‹ï¼Œç‚ºåœ–ç‰‡èˆ‡å½±ç‰‡ç”¢ç”Ÿè‡ªç„¶èªè¨€å­—å¹•ã€‚

## ğŸ§­ Snapshot

| ç¶­åº¦ | è©³ç´° |
|---|---|
| ä»»å‹™è¦†è“‹ | åœ–ç‰‡èˆ‡å½±ç‰‡å­—å¹•ç”Ÿæˆ |
| æ ¸å¿ƒè¼¸å‡º | SRT å­—å¹•ã€JSON é€å­—ç¨¿ã€æ¨™è¨»å¾Œçš„åœ–ç‰‡ |
| ä¸»è¦è…³æœ¬ | `i2c.py`ã€`v2c.py`ã€`image2caption.py` |
| èˆŠç‰ˆè·¯å¾‘ | `video2caption.py` èˆ‡å…¶ç‰ˆæœ¬åˆ†æ”¯ï¼ˆä¿ç•™ä¾›åƒè€ƒï¼‰ |
| è³‡æ–™æµç¨‹ | `data/raw/results.csv` + `data/raw/flickr30k_images/` |

## âœ¨ æ¦‚è¦½

æ­¤å„²å­˜åº«æä¾›ï¼š

- åœ–ç‰‡æè¿°èˆ‡å½±ç‰‡å­—å¹•ç”Ÿæˆçš„æ¨è«–è…³æœ¬ã€‚
- ä¸€å¥—è¨“ç·´æµç¨‹ï¼Œå­¸ç¿’å°‡ CLIP è¦–è¦ºåµŒå…¥æ˜ å°„åˆ° GPT-2 token åµŒå…¥ã€‚
- é©ç”¨æ–¼ Flickr30k é¢¨æ ¼è³‡æ–™çš„è³‡æ–™é›†ç”Ÿæˆå·¥å…·ã€‚
- ç•¶ç¼ºå°‘æ¬Šé‡æ™‚ï¼Œè‡ªå‹•ä¸‹è¼‰æ”¯æ´çš„æ¨¡å‹å°ºå¯¸ã€‚
- `i18n/` ä¸‹çš„å¤šèª README ç‰ˆæœ¬ï¼ˆè¦‹èªè¨€åˆ—ï¼‰ã€‚

ç›®å‰å¯¦ä½œåŒ…å«æ–°èˆŠå…©å¥—è…³æœ¬ã€‚éƒ¨åˆ†èˆŠç‰ˆæª”æ¡ˆä»ä¿ç•™ä»¥ä¾›åƒè€ƒï¼Œä¸¦åœ¨ä¸‹æ–¹èªªæ˜ã€‚

## ğŸš€ åŠŸèƒ½

- é€é `image2caption.py` é€²è¡Œå–®å¼µåœ–ç‰‡å­—å¹•è¼¸å‡ºã€‚
- é€é `v2c.py` æˆ– `video2caption.py` é€²è¡Œå½±ç‰‡å­—å¹•ï¼ˆå‡å‹»å–æ¨£å½±æ ¼ï¼‰ã€‚
- å¯èª¿æ•´çš„åŸ·è¡Œåƒæ•¸ï¼š
  - å½±æ ¼æ•¸ã€‚
  - æ¨¡å‹å°ºå¯¸ã€‚
  - æ¡æ¨£æº«åº¦ã€‚
  - Checkpoint åç¨±ã€‚
- å¤šæ ¸å¿ƒ/å¤šåŸ·è¡Œç·’çš„å½±ç‰‡æ¨è«–ï¼Œæå‡è™•ç†é€Ÿåº¦ã€‚
- è¼¸å‡ºæˆæœï¼š
  - SRT å­—å¹•æª”ï¼ˆ`.srt`ï¼‰ã€‚
  - `v2c.py` ç”¢ç”Ÿçš„ JSON é€å­—ç¨¿ï¼ˆ`.json`ï¼‰ã€‚
- CLIP+GPT2 æ˜ å°„å¯¦é©—çš„è¨“ç·´èˆ‡è©•ä¼°å…¥å£ã€‚

### ä¸€çœ¼çœ‹æ‡‚

| å€åŸŸ | ä¸»è¦è…³æœ¬ | èªªæ˜ |
|---|---|---|
| åœ–ç‰‡æè¿° | `image2caption.py`ã€`i2c.py`ã€`predict.py` | CLI èˆ‡å¯é‡ç”¨é¡åˆ¥ |
| å½±ç‰‡æè¿° | `v2c.py` | å»ºè­°ä½¿ç”¨çš„ç¶­è­·è·¯å¾‘ |
| èˆŠç‰ˆå½±ç‰‡æµç¨‹ | `video2caption.py`ã€`video2caption_v1.1.py` | å«æ©Ÿå™¨ç‰¹å®šå‡è¨­ |
| è³‡æ–™é›†å»ºç½® | `dataset_generation.py` | ç”¢ç”Ÿ `data/processed/dataset.pkl` |
| è¨“ç·´ / è©•ä¼° | `training.py`ã€`evaluate.py` | ä½¿ç”¨ CLIP+GPT2 æ˜ å°„ |

## ğŸ§± æ¶æ§‹ï¼ˆé«˜éšï¼‰

`model/model.py` çš„æ ¸å¿ƒæ¨¡å‹åŒ…å«ä¸‰å€‹éƒ¨ä»½ï¼š

1. `ImageEncoder`ï¼šèƒå– CLIP åœ–åƒåµŒå…¥ã€‚
2. `Mapping`ï¼šå°‡ CLIP åµŒå…¥æŠ•å½±åˆ° GPT å‰ç¶´åµŒå…¥åºåˆ—ã€‚
3. `TextDecoder`ï¼šGPT-2 èªè¨€æ¨¡å‹é ­ï¼Œé€éè‡ªå›æ­¸æ–¹å¼é€å­—ç”Ÿæˆå­—å¹• tokenã€‚

è¨“ç·´æµç¨‹ï¼ˆ`Net.train_forward`ï¼‰ä½¿ç”¨é å…ˆè¨ˆç®—å¥½çš„ CLIP åœ–åƒåµŒå…¥èˆ‡åˆ†è©å¾Œå­—å¹•ã€‚
æ¨è«–æµç¨‹ï¼ˆ`Net.forward`ï¼‰ä½¿ç”¨ PIL åœ–ç‰‡ï¼Œä¸¦æŒçºŒè§£ç¢¼ token è‡³ EOS æˆ– `max_len`ã€‚

### è³‡æ–™æµç¨‹

1. æº–å‚™è³‡æ–™é›†ï¼š`dataset_generation.py` è®€å– `data/raw/results.csv` èˆ‡ `data/raw/flickr30k_images/`ï¼Œä¸¦å¯«å…¥ `data/processed/dataset.pkl`ã€‚
2. è¨“ç·´ï¼š`training.py` è¼‰å…¥ pickled tuple `(image_name, image_embedding, caption)`ï¼Œä¸¦è¨“ç·´æ˜ å°„ï¼è§£ç¢¼å±¤ã€‚
3. è©•ä¼°ï¼š`evaluate.py` åœ¨ä¿ç•™æ¸¬è©¦å½±åƒä¸Šè¼¸å‡ºé æ¸¬å­—å¹•ã€‚
4. æ¨è«–å…¥å£ï¼š
   - åœ–ç‰‡ï¼š`image2caption.py` / `predict.py` / `i2c.py`ã€‚
   - å½±ç‰‡ï¼š`v2c.py`ï¼ˆå»ºè­°ï¼‰ã€`video2caption.py`ï¼ˆèˆŠç‰ˆï¼‰ã€‚

## ğŸ—‚ï¸ å°ˆæ¡ˆçµæ§‹

```text
VideoCaptionerWithClip/
â”œâ”€â”€ README.md
â”œâ”€â”€ image2caption.py               # Single-image caption CLI
â”œâ”€â”€ predict.py                     # Alternate single-image caption CLI
â”œâ”€â”€ i2c.py                         # Reusable ImageCaptioner class + CLI
â”œâ”€â”€ v2c.py                         # Video -> SRT + JSON (threaded frame captioning)
â”œâ”€â”€ video2caption.py               # Alternate video -> SRT implementation (legacy constraints)
â”œâ”€â”€ video2caption_v1.1.py          # Older variant
â”œâ”€â”€ video2caption_v1.0_not_work.py # Explicitly marked non-working legacy file
â”œâ”€â”€ training.py                    # Model training entrypoint
â”œâ”€â”€ evaluate.py                    # Test-split evaluation and rendered outputs
â”œâ”€â”€ dataset_generation.py          # Builds data/processed/dataset.pkl
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dataset.py                 # Dataset + DataLoader helpers
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py                   # CLIP encoder + mapping + GPT2 decoder
â”‚   â””â”€â”€ trainer.py                 # Training/validation/test utility class
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                  # ConfigS / ConfigL defaults
â”‚   â”œâ”€â”€ downloads.py               # Google Drive checkpoint downloader
â”‚   â””â”€â”€ lr_warmup.py               # LR warmup schedule
â”œâ”€â”€ i18n/                          # Multilingual README variants
â””â”€â”€ .auto-readme-work/             # Auto-README pipeline artifacts
```

## ğŸ“‹ å…ˆæ±ºæ¢ä»¶

- å»ºè­°ä½¿ç”¨ Python `3.10+`ã€‚
- å¯ç”¨ CUDA çš„ GPU éå¿…è¦ä½†å¼·çƒˆå»ºè­°ç”¨æ–¼è¨“ç·´èˆ‡å¤§å‹æ¨¡å‹æ¨è«–ã€‚
- ç›®å‰è…³æœ¬ä¸ç›´æ¥ä¾è³´ `ffmpeg`ï¼ˆå½±æ ¼æ“·å–ä½¿ç”¨ OpenCVï¼‰ã€‚
- é¦–æ¬¡å¾ Hugging Face / Google Drive ä¸‹è¼‰æ¨¡å‹èˆ‡ checkpoint æ™‚éœ€è¦ç¶²è·¯é€£ç·šã€‚

ç›®å‰æœªæä¾›é–æª”ï¼ˆç¼ºå°‘ `requirements.txt` / `pyproject.toml`ï¼‰ï¼Œæ•…å¯æ¨æ¸¬ä¾è³´è‡ªåŒ¯å…¥æ¨¡çµ„ã€‚

## ğŸ› ï¸ å®‰è£

### ä¾ç›®å‰å„²å­˜åº«ä½ˆå±€çš„æ¨™æº–å®‰è£æ–¹å¼

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

python -m venv .venv
source .venv/bin/activate

pip install --upgrade pip
pip install torch torchvision torchaudio
pip install transformers pillow matplotlib numpy tqdm opencv-python pandas wandb gdown
```

### ä¿ç•™åŸå§‹ README çš„å®‰è£ç‰‡æ®µ

èˆŠç‰ˆ README åœ¨ç¨‹å¼ç¢¼å€æ®µä¸­é€”ä¸­æ–·ï¼Œä¿ç•™åŸå§‹æŒ‡ä»¤å¦‚ä¸‹ï¼Œä½œç‚ºæ­·å²çœŸå€¼å…§å®¹ï¼š

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip/src
```

æ³¨æ„ï¼šç›®å‰å„²å­˜åº«å¿«ç…§å°‡è…³æœ¬æ”¾åœ¨ repo æ ¹ç›®éŒ„ï¼Œè€Œä¸æ˜¯ `src/` ä¸‹ã€‚

## â–¶ï¸ å¿«é€Ÿé–‹å§‹

| ç›®æ¨™ | æŒ‡ä»¤ |
|---|---|
| å°åœ–ç‰‡é€²è¡Œå­—å¹• | `python image2caption.py -I /path/to/image.jpg -S L -C model.pt` |
| å°å½±ç‰‡é€²è¡Œå­—å¹• | `python v2c.py -V /path/to/video.mp4 -N 10` |
| å»ºç«‹è³‡æ–™é›† | `python dataset_generation.py` |

### åœ–ç‰‡å­—å¹•ï¼ˆå¿«é€ŸåŸ·è¡Œï¼‰

```bash
python image2caption.py -I /path/to/image.jpg -S L -C model.pt
```

### å½±ç‰‡å­—å¹•ï¼ˆå»ºè­°è·¯å¾‘ï¼‰

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

## ğŸ¯ ä½¿ç”¨æ–¹å¼

### 1. åœ–ç‰‡æè¿°ï¼ˆ`image2caption.py`ï¼‰

```bash
python image2caption.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

åƒæ•¸ï¼š

- `-I, --img-path`ï¼šè¼¸å…¥åœ–ç‰‡è·¯å¾‘ã€‚
- `-S, --size`ï¼šæ¨¡å‹å°ºå¯¸ï¼ˆ`S` æˆ– `L`ï¼‰ã€‚
- `-C, --checkpoint-name`ï¼š`weights/{small|large}` ä¸‹çš„ checkpoint æª”åã€‚
- `-R, --res-path`ï¼šè¼¸å‡ºåŠ è¨»å­—å¹•åœ–ç‰‡çš„è³‡æ–™å¤¾ã€‚
- `-T, --temperature`ï¼šæ¡æ¨£æº«åº¦ã€‚

### 2. æ›¿ä»£åœ–ç‰‡ CLIï¼ˆ`predict.py`ï¼‰

```bash
python predict.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

`predict.py` åŠŸèƒ½ä¸Šèˆ‡ `image2caption.py` é¡ä¼¼ï¼Œè¼¸å‡ºæ–‡å­—æ ¼å¼ç•¥æœ‰å·®ç•°ã€‚

### 3. åœ–ç‰‡æè¿°é¡åˆ¥ APIï¼ˆ`i2c.py`ï¼‰

```bash
python i2c.py -I /path/to/image.jpg -S L -C model.pt -R ./data/result/prediction -T 1.0
```

æˆ–åœ¨è‡ªè¨‚è…³æœ¬ä¸­å¼•å…¥ï¼š

```python
from i2c import ImageCaptioner

captioner = ImageCaptioner(model_size="L", checkpoint_name="model.pt")
captioner.set_image_path("/path/to/image.jpg")
caption = captioner.generate_caption(save_image=True)
print(caption)
```

### 4. å½±ç‰‡è½‰å­—å¹•èˆ‡ JSONï¼ˆ`v2c.py`ï¼‰

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

è¼¸å‡ºèˆ‡è¼¸å…¥å½±ç‰‡åŒç›®éŒ„ï¼š

- `<video_basename>_caption.srt`
- `<video_basename>_caption.json`
- `<video_basename>_captioning_frames/`

### 5. æ›¿ä»£å½±ç‰‡æµç¨‹ï¼ˆ`video2caption.py`ï¼‰

```bash
python video2caption.py -V /path/to/video.mp4 -N 10
```

é‡è¦ï¼šæ­¤è…³æœ¬ç›®å‰ä»ä¿ç•™æ©Ÿå™¨ç‰¹å®šçš„ç¡¬ç·¨ç¢¼è·¯å¾‘ï¼š

- Python é è¨­è·¯å¾‘ï¼š`/home/lachlan/miniconda3/envs/caption/bin/python`
- å­—å¹•è…³æœ¬è·¯å¾‘ï¼š`/home/lachlan/Projects/image_captioning/clip-gpt-captioning/src/image2caption.py`

é™¤éä½ åˆ»æ„ç¶­è­·é€™äº›è·¯å¾‘ï¼Œå¦å‰‡è«‹æ”¹ç”¨ `v2c.py`ã€‚

### 6. èˆŠç‰ˆè®Šé«”ï¼ˆ`video2caption_v1.1.py`ï¼‰

æ­¤è…³æœ¬ä¿ç•™ç‚ºæ­·å²åƒè€ƒï¼›å¯¦éš›ä½¿ç”¨è«‹å„ªå…ˆæ¡ç”¨ `v2c.py`ã€‚

### 7. ç”Ÿæˆè³‡æ–™é›†

```bash
python dataset_generation.py
```

é æœŸè¼¸å…¥ï¼š

- `data/raw/results.csv`ï¼ˆä»¥ç®¡ç·šç¬¦è™Ÿ `|` åˆ†éš”çš„æè¿°è¡¨ï¼‰ã€‚
- `data/raw/flickr30k_images/`ï¼ˆCSV ä¸­åƒç…§çš„åœ–ç‰‡æª”ï¼‰ã€‚

è¼¸å‡ºï¼š

- `data/processed/dataset.pkl`

### 8. è¨“ç·´

```bash
python training.py -S L -C model.pt
```

è¨“ç·´é è¨­ä½¿ç”¨ Weights & Biasesï¼ˆ`wandb`ï¼‰è¨˜éŒ„ã€‚

### 9. è©•ä¼°

```bash
python evaluate.py \
  -I ./data/raw/flickr30k_images \
  -R ./data/result/eval \
  -S L \
  -C model.pt \
  -T 1.0
```

è©•ä¼°æœƒå°‡é æ¸¬çµæœç–ŠåŠ åˆ°æ¸¬è©¦å½±åƒä¸¦å„²å­˜è‡³ï¼š

- `<res-path>/<checkpoint_name_without_ext>_<SIZE>/`

## âš™ï¸ è¨­å®š

æ¨¡å‹è¨­å®šå®šç¾©æ–¼ `utils/config.py`ï¼š

| è¨­å®š | CLIP backbone | GPT model | æ¬Šé‡ç›®éŒ„ |
|---|---|---|---|
| `ConfigS` | `openai/clip-vit-base-patch32` | `gpt2` | `weights/small` |
| `ConfigL` | `openai/clip-vit-large-patch14` | `gpt2-medium` | `weights/large` |

é—œéµé è¨­å€¼ï¼š

| æ¬„ä½ | `ConfigS` | `ConfigL` |
|---|---:|---:|
| `epochs` | 150 | 120 |
| `lr` | 3e-3 | 5e-3 |
| `batch_size_exp` | 6 | 5 |
| `ep_len` | 4 | 4 |
| `max_len` | 40 | 40 |

Checkpoint è‡ªå‹•ä¸‹è¼‰ ID è¨˜éŒ„æ–¼ `utils/downloads.py`ï¼š

| å°ºå¯¸ | Google Drive ID |
|---|---|
| `L` | `1Gh32arzhW06C1ZJyzcJSSfdJDi3RgWoG` |
| `S` | `1pSQruQyg8KJq6VmzhMLFbT_VaHJMdlWF` |

## ğŸ“¦ è¼¸å‡ºæª”æ¡ˆ

### åœ–ç‰‡æ¨è«–

- å„²å­˜ç–ŠåŠ å­—å¹•çš„åœ–ç‰‡åˆ° `--res-path`ã€‚
- æª”åæ ¼å¼ï¼š`<input_stem>-R<SIZE>.jpg`ã€‚

### å½±ç‰‡æ¨è«–ï¼ˆ`v2c.py`ï¼‰

- SRTï¼š`<video_stem>_caption.srt`
- JSONï¼š`<video_stem>_caption.json`
- å½±æ ¼åœ–ç‰‡ï¼š`<video_stem>_captioning_frames/`

JSON å…ƒç´ ç¯„ä¾‹ï¼š

```json
{
  "start": "00:00:03,200",
  "end": "00:00:03,700",
  "lang": "en",
  "text": "A dog running through a field."
}
```

## ğŸ§ª ç¯„ä¾‹

### å¿«é€Ÿåœ–ç‰‡å­—å¹•ç¯„ä¾‹

```bash
python image2caption.py -I ./examples/dog.jpg -S S -C model.pt
```

é æœŸè¡Œç‚ºï¼š

- å¦‚æœ `weights/small/model.pt` ä¸å­˜åœ¨ï¼Œå°‡æœƒä¸‹è¼‰ã€‚
- é è¨­æœƒè¼¸å‡ºå­—å¹•åœ–ç‰‡è‡³ `./data/result/prediction`ã€‚
- å­—å¹•æ–‡å­—å°‡è¼¸å‡ºè‡³æ¨™æº–è¼¸å‡ºï¼ˆstdoutï¼‰ã€‚

### å¿«é€Ÿå½±ç‰‡å­—å¹•ç¯„ä¾‹

```bash
python v2c.py -V ./examples/demo.mp4 -N 8
```

é æœŸè¡Œç‚ºï¼š

- æœƒå° 8 å€‹å‡å‹»æŠ½æ¨£çš„å½±æ ¼ç”¢ç”Ÿå­—å¹•ã€‚
- `.srt` èˆ‡ `.json` æœƒèˆ‡è¼¸å…¥å½±ç‰‡ä¸€ä½µç”Ÿæˆã€‚

### ç«¯åˆ°ç«¯è¨“ç·´èˆ‡è©•ä¼°æµç¨‹

```bash
python dataset_generation.py
python training.py -S L -C model.pt
python evaluate.py -I ./data/raw/flickr30k_images -R ./data/result/eval -S L -C model.pt -T 1.0
```

## ğŸ§­ é–‹ç™¼èªªæ˜

- `v2c.py`ã€`video2caption.py` èˆ‡ `video2caption_v1.*` é–“æœ‰èˆŠç‰ˆæµç¨‹é‡ç–Šã€‚
- `video2caption_v1.0_not_work.py` æœ‰æ„ä¿ç•™ä½œç‚ºä¸èƒ½é‹ä½œçš„èˆŠç‰ˆç¨‹å¼ç¢¼ã€‚
- `training.py` ç›®å‰é€é `config = ConfigL() if args.size.upper() else ConfigS()` é¸å– `ConfigL()`ï¼Œå°éç©º `--size` éƒ½æœƒè§£æç‚º `ConfigL`ã€‚
- `model/trainer.py` åœ¨ `test_step` ä½¿ç”¨ `self.dataset`ï¼Œä½†åˆå§‹åŒ–æ™‚æ˜¯ `self.test_dataset`ï¼Œå¯èƒ½å°è‡´è¨“ç·´å–æ¨£å¤±æ•—ï¼Œéœ€è¦–æƒ…æ³ä¿®æ­£ã€‚
- `video2caption_v1.1.py` ä½¿ç”¨ `self.config.transform`ï¼Œä½† `ConfigS`ï¼`ConfigL` ä¸¦æœªå®šç¾© `transform`ã€‚
- ç›®å‰æœ¬å„²å­˜åº«å¿«ç…§æœªå»ºç«‹ CI / æ¸¬è©¦å¥—ä»¶ã€‚
- i18n å‚™è¨»ï¼šREADME é ‚éƒ¨å·²æœ‰èªè¨€é€£çµï¼Œå¯åœ¨ `i18n/` ä¸­æ–°å¢ç¿»è­¯ã€‚
- ç¾æ³å‚™è¨»ï¼šèªè¨€åˆ—éˆæ¥ `i18n/README.ru.md`ï¼Œä½†æœ¬å¿«ç…§ä¸­è©²æª”æ¡ˆä¸å­˜åœ¨ã€‚

## ğŸ©º ç–‘é›£æ’è§£

- `AssertionError: Image does not exist`
  - ç¢ºèª `-I/--img-path` æŒ‡å‘æœ‰æ•ˆæª”æ¡ˆã€‚
- `Dataset file not found. Downloading...`
  - ç•¶ç¼ºå°‘ `data/processed/dataset.pkl` æ™‚ï¼Œ`MiniFlickrDataset` æœƒæ‹‹å‡ºæ­¤è¨Šæ¯ï¼›è«‹å…ˆåŸ·è¡Œ `python dataset_generation.py`ã€‚
- `Path to the test image folder does not exist`
  - ç¢ºèª `evaluate.py -I` æŒ‡å‘ç¾æœ‰è³‡æ–™å¤¾ã€‚
- é¦–æ¬¡åŸ·è¡Œç·©æ…¢æˆ–å¤±æ•—
  - é¦–æ¬¡åŸ·è¡Œæœƒä¸‹è¼‰ Hugging Face æ¨¡å‹ï¼Œä¸¦å¯èƒ½å¾ Google Drive ä¸‹è¼‰ checkpointã€‚
- `video2caption.py` å›å‚³ç©ºç™½å­—å¹•
  - è«‹æª¢æŸ¥ç¡¬ç·¨ç¢¼è…³æœ¬è·¯å¾‘èˆ‡ Python å¯åŸ·è¡Œæª”ï¼Œæˆ–æ”¹ç”¨ `v2c.py`ã€‚
- `wandb` æç¤ºè¨“ç·´æ™‚ç™»å…¥
  - åŸ·è¡Œ `wandb login`ï¼Œæˆ–è¦–éœ€æ±‚åœ¨ `training.py` ä¸­æ‰‹å‹•åœç”¨ç´€éŒ„ã€‚

## ğŸ›£ï¸ é‡Œç¨‹ç¢‘

- æ–°å¢ä¾è³´é–å®šæª”ï¼ˆ`requirements.txt` æˆ– `pyproject.toml`ï¼‰ä»¥æå‡å¯é‡ç¾æ€§ã€‚
- å°‡é‡è¤‡çš„å½±ç‰‡æµç¨‹æ•´ä½µç‚ºå–®ä¸€è·¯å¾‘ç¶­è­·ã€‚
- ç§»é™¤èˆŠç‰ˆè…³æœ¬ä¸­çš„æ©Ÿå™¨ç¡¬ç·¨ç¢¼è·¯å¾‘ã€‚
- ä¿®æ­£ `training.py` èˆ‡ `model/trainer.py` å·²çŸ¥çš„è¨“ç·´ï¼è©•ä¼°é‚Šç•Œæ¡ˆä¾‹éŒ¯èª¤ã€‚
- æ–°å¢è‡ªå‹•åŒ–æ¸¬è©¦èˆ‡ CIã€‚
- è£œé½Šèªè¨€åˆ—ä¸­å¼•ç”¨åˆ°çš„ `i18n/` README ç¿»è­¯æª”ã€‚

## ğŸ¤ è²¢ç»

æ­¡è¿æäº¤è²¢ç»ã€‚å»ºè­°æµç¨‹å¦‚ä¸‹ï¼š

```bash
# 1) Fork and clone
git clone git@github.com:<your-user>/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

# 2) Create a feature branch
git checkout -b feat/your-change

# 3) Make changes and commit
git add .
git commit -m "feat: describe your change"

# 4) Push and open a PR
git push origin feat/your-change
```

å¦‚æœä½ è®Šæ›´æ¨¡å‹è¡Œç‚ºï¼Œè«‹ä¸€ä½µæä¾›ï¼š

- å¯é‡ç¾çš„æŒ‡ä»¤ã€‚
- è®Šæ›´å‰å¾Œçš„è¼¸å‡ºç¯„ä¾‹ã€‚
- Checkpoint èˆ‡è³‡æ–™é›†å‡è¨­å‚™è¨»ã€‚

## â¤ï¸ Support

| Donate | PayPal | Stripe |
|---|---|---|
| [![Donate](https://img.shields.io/badge/Donate-LazyingArt-0EA5E9?style=for-the-badge&logo=ko-fi&logoColor=white)](https://chat.lazying.art/donate) | [![PayPal](https://img.shields.io/badge/PayPal-RongzhouChen-00457C?style=for-the-badge&logo=paypal&logoColor=white)](https://paypal.me/RongzhouChen) | [![Stripe](https://img.shields.io/badge/Stripe-Donate-635BFF?style=for-the-badge&logo=stripe&logoColor=white)](https://buy.stripe.com/aFadR8gIaflgfQV6T4fw400) |

## ğŸ“„ æˆæ¬Š

æœ¬å„²å­˜åº«å¿«ç…§ç›®å‰æœªæä¾›æˆæ¬Šæª”ã€‚

å‡è¨­èªªæ˜ï¼šåœ¨åŠ å…¥ `LICENSE` æª”å‰ï¼Œé‡è£½èˆ‡æ•£ä½ˆæ¢æ¬¾ä»æœªå®šç¾©ã€‚

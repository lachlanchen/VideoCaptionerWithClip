[English](../README.md) ¬∑ [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](README.ar.md) ¬∑ [Espa√±ol](README.es.md) ¬∑ [Fran√ßais](README.fr.md) ¬∑ [Êó•Êú¨Ë™û](README.ja.md) ¬∑ [ÌïúÍµ≠Ïñ¥](README.ko.md) ¬∑ [Ti·∫øng Vi·ªát](README.vi.md) ¬∑ [‰∏≠Êñá (ÁÆÄ‰Ωì)](README.zh-Hans.md) ¬∑ [‰∏≠ÊñáÔºàÁπÅÈ´îÔºâ](README.zh-Hant.md) ¬∑ [Deutsch](README.de.md) ¬∑ [–†—É—Å—Å–∫–∏–π](README.ru.md)


[![LazyingArt banner](https://github.com/lachlanchen/lachlanchen/raw/main/figs/banner.png)](https://github.com/lachlanchen/lachlanchen/blob/main/figs/banner.png)

# Clip-GPT-Captioning

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow)
![Status](https://img.shields.io/badge/README-Expanded-success)
![Repo Layout](https://img.shields.io/badge/Layout-Root%20Scripts-informational)
![Legacy Scripts](https://img.shields.io/badge/Legacy%20Scripts-Present-orange)
![i18n](https://img.shields.io/badge/i18n-Enabled-brightgreen)
![Maintained Path](https://img.shields.io/badge/Video-v2c.py-2ea44f)
![Contributions](https://img.shields.io/badge/Contributions-Welcome-2ea44f?style=flat-square)
![Issues](https://img.shields.io/github/issues-raw/lachlanchen/VideoCaptionerWithClip?style=flat-square)
![Last Commit](https://img.shields.io/github/last-commit/lachlanchen/VideoCaptionerWithClip?style=flat-square)

---

## üß≠ Navegaci√≥n r√°pida

| Secci√≥n | Para qu√© sirve |
|---|---|
| Snapshot | Ver el alcance del repositorio y el inventario actual de scripts |
| Overview | Ver objetivos y capacidades |
| Uso | Seguir los flujos CLI/API exactos |
| Soluci√≥n de problemas | Resolver incidencias comunes r√°pidamente |
| Hoja de ruta | Seguir objetivos de limpieza y mejora conocidos |

---

Un toolkit de Python para generar subt√≠tulos y textos en lenguaje natural sobre im√°genes y v√≠deos combinando embeddings visuales de OpenAI CLIP con un modelo de lenguaje tipo GPT.

## üß≠ Resumen

| Dimensi√≥n | Detalles |
|---|---|
| Cobertura de tareas | Captionado de imagen y v√≠deo |
| Salidas principales | Subt√≠tulos SRT, transcripciones JSON, im√°genes con pie de imagen |
| Scripts principales | `i2c.py`, `v2c.py`, `image2caption.py` |
| Rutas heredadas | `video2caption.py` y sus variantes versionadas (conservadas por historial) |
| Flujo de datos | `data/raw/results.csv` + `data/raw/flickr30k_images/` |

## ‚ú® Visi√≥n general

Este repositorio ofrece:

- Scripts de inferencia para captionado de im√°genes y generaci√≥n de subt√≠tulos de v√≠deo.
- Un pipeline de entrenamiento que aprende un mapeo entre embeddings visuales de CLIP y embeddings de tokens de GPT-2.
- Utilidades para generar datasets con estilo Flickr30k.
- Descarga autom√°tica de checkpoints para tama√±os de modelo compatibles cuando faltan los pesos.
- Variantes de README multiling√ºes en `i18n/` (ver la barra de idiomas arriba).

La implementaci√≥n actual incluye scripts nuevos y heredados. Algunos ficheros legacy se conservan para referencia y est√°n documentados abajo.

## üöÄ Funcionalidades

- Captionado de una sola imagen mediante `image2caption.py`.
- Captionado de v√≠deo (muestreo uniforme de fotogramas) con `v2c.py` o `video2caption.py`.
- Opciones de ejecuci√≥n personalizables:
  - N√∫mero de fotogramas.
  - Tama√±o del modelo.
  - Temperatura de muestreo.
  - Nombre del checkpoint.
- Captionado en multiproceso para acelerar inferencia de v√≠deo.
- Artefactos de salida:
  - Archivos de subt√≠tulos SRT (`.srt`).
  - Transcripciones JSON (`.json`) en `v2c.py`.
- Entradas de entrenamiento y evaluaci√≥n para experimentos de mapeo CLIP+GPT2.

### A simple vista

| √Årea | Script principal | Notas |
|---|---|---|
| Captionado de imagen | `image2caption.py`, `i2c.py`, `predict.py` | CLI + clase reutilizable |
| Captionado de v√≠deo | `v2c.py` | Ruta mantenida recomendada |
| Flujo legacy de v√≠deo | `video2caption.py`, `video2caption_v1.1.py` | Incluye suposiciones espec√≠ficas de m√°quina |
| Construcci√≥n de dataset | `dataset_generation.py` | Genera `data/processed/dataset.pkl` |
| Entrenamiento / evaluaci√≥n | `training.py`, `evaluate.py` | Usa mapeo CLIP+GPT2 |

## üß± Arquitectura (vista general)

El modelo principal en `model/model.py` tiene tres partes:

1. `ImageEncoder`: extrae embeddings de imagen de CLIP.
2. `Mapping`: proyecta el embedding de CLIP en una secuencia de embeddings de prefijo GPT.
3. `TextDecoder`: cabecera de lenguaje basada en GPT-2 que genera tokens de forma autoregresiva.

El entrenamiento (`Net.train_forward`) usa embeddings de imagen CLIP precalculados + captions tokenizados.
La inferencia (`Net.forward`) usa una imagen PIL y decodifica tokens hasta EOS o `max_len`.

### Flujo de datos

1. Preparar dataset: `dataset_generation.py` lee `data/raw/results.csv` y las im√°genes en `data/raw/flickr30k_images/`, y escribe `data/processed/dataset.pkl`.
2. Entrenar: `training.py` carga tuplas serializadas `(image_name, image_embedding, caption)` y entrena capas mapper/decoder.
3. Evaluar: `evaluate.py` genera captions para im√°genes de prueba del split retenido.
4. Ejecutar inferencia:
   - imagen: `image2caption.py` / `predict.py` / `i2c.py`.
   - v√≠deo: `v2c.py` (recomendado), `video2caption.py` (legacy).

## üóÇÔ∏è Estructura del proyecto

```text
VideoCaptionerWithClip/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ image2caption.py               # CLI de captionado para imagen √∫nica
‚îú‚îÄ‚îÄ predict.py                     # CLI alternativa para imagen √∫nica
‚îú‚îÄ‚îÄ i2c.py                         # Clase reutilizable ImageCaptioner + CLI
‚îú‚îÄ‚îÄ v2c.py                         # V√≠deo -> SRT + JSON (captionado de fotogramas con hilos)
‚îú‚îÄ‚îÄ video2caption.py               # Implementaci√≥n alternativa v√≠deo -> SRT (legacy, con limitaciones)
‚îú‚îÄ‚îÄ video2caption_v1.1.py          # Variante anterior
‚îú‚îÄ‚îÄ video2caption_v1.0_not_work.py # Archivo legacy expl√≠citamente marcado como no funcional
‚îú‚îÄ‚îÄ training.py                    # Punto de entrada de entrenamiento del modelo
‚îú‚îÄ‚îÄ evaluate.py                    # Evaluaci√≥n en split de prueba y salidas renderizadas
‚îú‚îÄ‚îÄ dataset_generation.py          # Construye data/processed/dataset.pkl
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ dataset.py                 # Helpers de Dataset + DataLoader
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ model.py                   # Encodificador CLIP + mapping + decodificador GPT2
‚îÇ   ‚îî‚îÄ‚îÄ trainer.py                 # Clase utilitaria train/val/test
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Defaults ConfigS / ConfigL
‚îÇ   ‚îú‚îÄ‚îÄ downloads.py               # Descargador de checkpoints desde Google Drive
‚îÇ   ‚îî‚îÄ‚îÄ lr_warmup.py               # Planificador de warmup de LR
‚îú‚îÄ‚îÄ i18n/                          # Variantes del README en varios idiomas
‚îî‚îÄ‚îÄ .auto-readme-work/             # Artefactos del pipeline auto-README
```

## üìã Requisitos previos

- Python `3.10+` recomendado.
- Se recomienda GPU con CUDA, especialmente para entrenamiento y inferencia con modelos grandes, aunque es opcional.
- `ffmpeg` no es requisito directo de los scripts actuales (OpenCV se usa para la extracci√≥n de fotogramas).
- Se requiere acceso a internet para la primera descarga de modelos/checkpoints desde Hugging Face / Google Drive.

Actualmente no hay lockfile presente (`requirements.txt` / `pyproject.toml` faltantes), por lo que las dependencias se infieren desde los imports.

## üõ†Ô∏è Instalaci√≥n

### Configuraci√≥n can√≥nica para el layout actual del repositorio

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

python -m venv .venv
source .venv/bin/activate

pip install --upgrade pip
pip install torch torchvision torchaudio
pip install transformers pillow matplotlib numpy tqdm opencv-python pandas wandb gdown
```

### Fragmento de instalaci√≥n del README original (conservado)

El README anterior terminaba a mitad de bloque. Los comandos originales se conservan abajo exactamente como fuente hist√≥rica:

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip/src
```

Nota: la instant√°nea actual del repositorio coloca los scripts en la ra√≠z, no bajo `src/`.

## ‚ñ∂Ô∏è Inicio r√°pido

| Objetivo | Comando |
|---|---|
| Captionar una imagen | `python image2caption.py -I /path/to/image.jpg -S L -C model.pt` |
| Captionar un v√≠deo | `python v2c.py -V /path/to/video.mp4 -N 10` |
| Construir dataset | `python dataset_generation.py` |

### Captionado de imagen (ejecuci√≥n r√°pida)

```bash
python image2caption.py -I /path/to/image.jpg -S L -C model.pt
```

### Captionado de v√≠deo (ruta recomendada)

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

## üéØ Uso

### 1. Captionado de imagen (`image2caption.py`)

```bash
python image2caption.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

Argumentos:

- `-I, --img-path`: ruta de imagen de entrada.
- `-S, --size`: tama√±o del modelo (`S` o `L`).
- `-C, --checkpoint-name`: nombre del checkpoint dentro de `weights/{small|large}`.
- `-R, --res-path`: directorio de salida para la imagen con el caption renderizado.
- `-T, --temperature`: temperatura de muestreo.

### 2. CLI alternativa para imagen (`predict.py`)

```bash
python predict.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

`predict.py` es funcionalmente similar a `image2caption.py`; el formato del texto de salida difiere levemente.

### 3. API de clase para captionado de imagen (`i2c.py`)

```bash
python i2c.py -I /path/to/image.jpg -S L -C model.pt -R ./data/result/prediction -T 1.0
```

O imp√≥rtalo en tu propio script:

```python
from i2c import ImageCaptioner

captioner = ImageCaptioner(model_size="L", checkpoint_name="model.pt")
captioner.set_image_path("/path/to/image.jpg")
caption = captioner.generate_caption(save_image=True)
print(caption)
```

### 4. De v√≠deo a subt√≠tulos + JSON (`v2c.py`)

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

Salida junto al v√≠deo de entrada:

- `<video_basename>_caption.srt`
- `<video_basename>_caption.json`
- `<video_basename>_captioning_frames/`

### 5. Pipeline alternativo de v√≠deo (`video2caption.py`)

```bash
python video2caption.py -V /path/to/video.mp4 -N 10
```

Importante: este script contiene rutas duras espec√≠ficas de m√°quina:

- Python path predeterminado: `/home/lachlan/miniconda3/envs/caption/bin/python`
- Ruta del script de caption: `/home/lachlan/Projects/image_captioning/clip-gpt-captioning/src/image2caption.py`

Usa `v2c.py` a menos que mantengas estas rutas de forma intencionada.

### 6. Variante heredada (`video2caption_v1.1.py`)

Este script se conserva como referencia hist√≥rica. Para uso activo, prefiere `v2c.py`.

### 7. Generaci√≥n de dataset

```bash
python dataset_generation.py
```

Entradas esperadas:

- `data/raw/results.csv` (tabla de captions separada por `|`).
- `data/raw/flickr30k_images/` (ficheros de imagen referenciados por el CSV).

Salida:

- `data/processed/dataset.pkl`

### 8. Entrenamiento

```bash
python training.py -S L -C model.pt
```

El entrenamiento usa logging de Weights & Biases (`wandb`) por defecto.

### 9. Evaluaci√≥n

```bash
python evaluate.py \
  -I ./data/raw/flickr30k_images \
  -R ./data/result/eval \
  -S L \
  -C model.pt \
  -T 1.0
```

La evaluaci√≥n renderiza los captions predichos sobre im√°genes de prueba y los guarda en:

- `<res-path>/<checkpoint_name_without_ext>_<SIZE>/`

## ‚öôÔ∏è Configuraci√≥n

Las configuraciones del modelo se definen en `utils/config.py`:

| Config | Backbone de CLIP | Modelo GPT | Carpeta de pesos |
|---|---|---|---|
| `ConfigS` | `openai/clip-vit-base-patch32` | `gpt2` | `weights/small` |
| `ConfigL` | `openai/clip-vit-large-patch14` | `gpt2-medium` | `weights/large` |

Valores predeterminados de las clases de configuraci√≥n:

| Campo | `ConfigS` | `ConfigL` |
|---|---:|---:|
| `epochs` | 150 | 120 |
| `lr` | 3e-3 | 5e-3 |
| `batch_size_exp` | 6 | 5 |
| `ep_len` | 4 | 4 |
| `max_len` | 40 | 40 |

Los IDs de auto-descarga de checkpoints est√°n en `utils/downloads.py`:

| Tama√±o | ID de Google Drive |
|---|---|
| `L` | `1Gh32arzhW06C1ZJyzcJSSfdJDi3RgWoG` |
| `S` | `1pSQruQyg8KJq6VmzhMLFbT_VaHJMdlWF` |

## üì¶ Archivos de salida

### Inferencia de imagen

- Imagen guardada con t√≠tulo o texto overlay en `--res-path`.
- Patr√≥n de nombre de archivo: `<input_stem>-R<SIZE>.jpg`.

### Inferencia de v√≠deo (`v2c.py`)

- SRT: `<video_stem>_caption.srt`
- JSON: `<video_stem>_caption.json`
- Im√°genes de fotogramas: `<video_stem>_captioning_frames/`

Ejemplo de elemento JSON:

```json
{
  "start": "00:00:03,200",
  "end": "00:00:03,700",
  "lang": "en",
  "text": "A dog running through a field."
}
```

## üß™ Ejemplos

### Ejemplo r√°pido de imagen

```bash
python image2caption.py -I ./examples/dog.jpg -S S -C model.pt
```

Comportamiento esperado:

- Si falta `weights/small/model.pt`, se descarga autom√°ticamente.
- Por defecto, se guarda una imagen con caption en `./data/result/prediction`.
- El texto del caption se imprime en stdout.

### Ejemplo r√°pido de caption de v√≠deo

```bash
python v2c.py -V ./examples/demo.mp4 -N 8
```

Comportamiento esperado:

- Se generan captions para 8 fotogramas muestreados uniformemente.
- Los archivos `.srt` y `.json` se generan junto al v√≠deo de entrada.

### Secuencia completa entrenamiento/evaluaci√≥n

```bash
python dataset_generation.py
python training.py -S L -C model.pt
python evaluate.py -I ./data/raw/flickr30k_images -R ./data/result/eval -S L -C model.pt -T 1.0
```

## üß≠ Notas de desarrollo

- Existe solapamiento legacy entre `v2c.py`, `video2caption.py` y `video2caption_v1.*`.
- `video2caption_v1.0_not_work.py` se conserva intencionadamente como c√≥digo legado no funcional.
- `training.py` actualmente selecciona `ConfigL()` en `config = ConfigL() if args.size.upper() else ConfigS()`, lo que siempre resuelve a `ConfigL` para valores no vac√≠os de `--size`.
- `model/trainer.py` usa `self.dataset` en `test_step`, mientras el inicializador asigna `self.test_dataset`; esto puede romper el muestreo en ejecuciones de entrenamiento si no se ajusta.
- `video2caption_v1.1.py` referencia `self.config.transform`, pero `ConfigS`/`ConfigL` no definen `transform`.
- Actualmente no existe suite de CI/pruebas en este snapshot del repositorio.
- Nota de i18n: en este README hay enlaces de idiomas arriba; se pueden agregar traducciones en `i18n/`.
- Nota de estado actual: los enlaces de idioma apuntan a `i18n/README.ru.md`, pero ese archivo no est√° presente en este snapshot.

## ü©∫ Soluci√≥n de problemas

- `AssertionError: Image does not exist`
  - Verifica que `-I/--img-path` apunte a un archivo v√°lido.
- `Dataset file not found. Downloading...`
  - `MiniFlickrDataset` lanza esto cuando falta `data/processed/dataset.pkl`; ejecuta primero `python dataset_generation.py`.
- `Path to the test image folder does not exist`
  - Confirma que `evaluate.py -I` apunte a una carpeta existente.
- Primera corrida lenta o fallida
  - La primera ejecuci√≥n descarga modelos de Hugging Face y puede descargar checkpoints desde Google Drive.
- `video2caption.py` devuelve captions vac√≠os
  - Verifica la ruta del script hardcodeada y la ruta del ejecutable Python, o cambia a `v2c.py`.
- `wandb` solicita login durante entrenamiento
  - Ejecuta `wandb login` o desactiva el logging manualmente en `training.py` si hace falta.

## üõ£Ô∏è Hoja de ruta

- A√±adir lockfiles de dependencias (`requirements.txt` o `pyproject.toml`) para instalaciones reproducibles.
- Unificar pipelines de v√≠deo duplicados en una implementaci√≥n mantenida.
- Eliminar rutas hardcodeadas de m√°quina de los scripts legacy.
- Corregir bugs conocidos en casos l√≠mite de entrenamiento/evaluaci√≥n en `training.py` y `model/trainer.py`.
- A√±adir pruebas y CI automatizados.
- Poblar `i18n/` con los README traducidos referenciados en la barra de idiomas.

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Flujo sugerido:

```bash
# 1) Fork and clone
git clone git@github.com:<your-user>/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

# 2) Crear rama de funcionalidades
git checkout -b feat/your-change

# 3) Hacer cambios y confirmar

git add .
git commit -m "feat: describe your change"

# 4) Enviar y abrir PR
git push origin feat/your-change
```

Si cambias el comportamiento del modelo, incluye:

- Comando(s) reproducibles.
- Ejemplos de salida antes/despu√©s.
- Notas sobre supuestos de checkpoint o dataset.

---

## üìÑ Licencia

No existe un archivo de licencia en el snapshot actual del repositorio.

Nota de supuesto: hasta que se a√±ada un archivo `LICENSE`, los t√©rminos de reutilizaci√≥n/distribuci√≥n permanecen indefinidos.


## ‚ù§Ô∏è Support

| Donate | PayPal | Stripe |
| --- | --- | --- |
| [![Donate](https://camo.githubusercontent.com/24a4914f0b42c6f435f9e101621f1e52535b02c225764b2f6cc99416926004b7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d4c617a79696e674172742d3045413545393f7374796c653d666f722d7468652d6261646765266c6f676f3d6b6f2d6669266c6f676f436f6c6f723d7768697465)](https://chat.lazying.art/donate) | [![PayPal](https://camo.githubusercontent.com/d0f57e8b016517a4b06961b24d0ca87d62fdba16e18bbdb6aba28e978dc0ea21/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617950616c2d526f6e677a686f754368656e2d3030343537433f7374796c653d666f722d7468652d6261646765266c6f676f3d70617970616c266c6f676f436f6c6f723d7768697465)](https://paypal.me/RongzhouChen) | [![Stripe](https://camo.githubusercontent.com/1152dfe04b6943afe3a8d2953676749603fb9f95e24088c92c97a01a897b4942/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5374726970652d446f6e6174652d3633354246463f7374796c653d666f722d7468652d6261646765266c6f676f3d737472697065266c6f676f436f6c6f723d7768697465)](https://buy.stripe.com/aFadR8gIaflgfQV6T4fw400) |

[English](../README.md) ¬∑ [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](README.ar.md) ¬∑ [Espa√±ol](README.es.md) ¬∑ [Fran√ßais](README.fr.md) ¬∑ [Êó•Êú¨Ë™û](README.ja.md) ¬∑ [ÌïúÍµ≠Ïñ¥](README.ko.md) ¬∑ [Ti·∫øng Vi·ªát](README.vi.md) ¬∑ [‰∏≠Êñá (ÁÆÄ‰Ωì)](README.zh-Hans.md) ¬∑ [‰∏≠ÊñáÔºàÁπÅÈ´îÔºâ](README.zh-Hant.md) ¬∑ [Deutsch](README.de.md) ¬∑ [–†—É—Å—Å–∫–∏–π](README.ru.md)


[![LazyingArt banner](https://github.com/lachlanchen/lachlanchen/raw/main/figs/banner.png)](https://github.com/lachlanchen/lachlanchen/blob/main/figs/banner.png)

# Clip-GPT-Captioning

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow)
![Status](https://img.shields.io/badge/README-Expanded-success)
![Repo Layout](https://img.shields.io/badge/Layout-Root%20Scripts-informational)
![Legacy Scripts](https://img.shields.io/badge/Legacy%20Scripts-Present-orange)
![i18n](https://img.shields.io/badge/i18n-Enabled-brightgreen)
![Maintained Path](https://img.shields.io/badge/Video-v2c.py-2ea44f)

Une bo√Æte √† outils Python pour g√©n√©rer des l√©gendes en langage naturel sur des images et des vid√©os en combinant les embeddings visuels OpenAI CLIP avec un mod√®le de langage de type GPT.

## üß≠ Snapshot

| Dimension | D√©tails |
|---|---|
| Couverture de t√¢che | L√©gende d'images et de vid√©os |
| Sorties principales | Sous-titres SRT, transcriptions JSON, images l√©gend√©es |
| Scripts principaux | `i2c.py`, `v2c.py`, `image2caption.py` |
| Chemins h√©rit√©s | `video2caption.py` et variantes versionn√©es (conserv√©es pour l'historique) |
| Flux de donn√©es | `data/raw/results.csv` + `data/raw/flickr30k_images/` |

## ‚ú® Vue d'ensemble

Ce d√©p√¥t fournit :

- Des scripts d'inf√©rence pour le sous-titrage d'images et la g√©n√©ration de sous-titres vid√©o.
- Un pipeline d'entra√Ænement qui apprend une projection des embeddings visuels CLIP vers les embeddings de tokens GPT-2.
- Des utilitaires de g√©n√©ration de jeu de donn√©es pour des donn√©es de style Flickr30k.
- Le t√©l√©chargement automatique de checkpoints pour les tailles de mod√®les prises en charge quand les poids sont manquants.
- Des variantes du README multilingues sous `i18n/` (voir la barre de langues ci-dessus).

L'impl√©mentation actuelle inclut √† la fois des scripts r√©cents et des scripts h√©rit√©s. Certains fichiers h√©rit√©s sont conserv√©s √† des fins de r√©f√©rence et sont document√©s ci-dessous.

## üöÄ Fonctionnalit√©s

- G√©n√©ration de l√©gende d'une image via `image2caption.py`.
- G√©n√©ration de l√©gende vid√©o (√©chantillonnage uniforme des frames) via `v2c.py` ou `video2caption.py`.
- Options d'ex√©cution personnalisables :
  - Nombre de frames.
  - Taille du mod√®le.
  - Temp√©rature d'√©chantillonnage.
  - Nom du checkpoint.
- L√©gendage multiprocessus/thread√© pour une inf√©rence vid√©o plus rapide.
- Artefacts de sortie :
  - Fichiers de sous-titres SRT (`.srt`).
  - Transcriptions JSON (`.json`) dans `v2c.py`.
- Points d'entr√©e entra√Ænement et √©valuation pour les exp√©riences de mapping CLIP+GPT2.

### √Ä vue d'ensemble

| Domaine | Script(s) principal(aux) | Remarques |
|---|---|---|
| L√©gende d'images | `image2caption.py`, `i2c.py`, `predict.py` | CLI + classe r√©utilisable |
| L√©gende de vid√©os | `v2c.py` | Chemin maintenu recommand√© |
| Flux vid√©o h√©rit√© | `video2caption.py`, `video2caption_v1.1.py` | Contient des hypoth√®ses sp√©cifiques √† la machine |
| Construction du dataset | `dataset_generation.py` | Produit `data/processed/dataset.pkl` |
| Entra√Ænement / √©valuation | `training.py`, `evaluate.py` | Utilise le mapping CLIP+GPT2 |

## üß± Architecture (vue d'ensemble)

Le mod√®le central dans `model/model.py` comporte trois parties :

1. `ImageEncoder` : extrait l'embedding d'image CLIP.
2. `Mapping` : projette l'embedding CLIP vers une s√©quence d'embeddings de pr√©fixe GPT.
3. `TextDecoder` : t√™te GPT-2 qui g√©n√®re de mani√®re autor√©gressive les tokens de l√©gende.

L'entra√Ænement (`Net.train_forward`) utilise des embeddings d'image CLIP pr√©-calcul√©s + des l√©gendes tokenis√©es.
L'inf√©rence (`Net.forward`) utilise une image PIL et d√©code les tokens jusqu'√† EOS ou `max_len`.

### Flux de donn√©es

1. Pr√©parer le dataset : `dataset_generation.py` lit `data/raw/results.csv` et les images dans `data/raw/flickr30k_images/`, puis √©crit `data/processed/dataset.pkl`.
2. Entra√Æner : `training.py` charge des tuples pickl√©s `(image_name, image_embedding, caption)` et entra√Æne les couches mapper/d√©codeur.
3. √âvaluer : `evaluate.py` applique les l√©gendes g√©n√©r√©es aux images du jeu de test.
4. Servir l'inf√©rence :
   - image : `image2caption.py` / `predict.py` / `i2c.py`.
   - vid√©o : `v2c.py` (recommand√©), `video2caption.py` (h√©rit√©).

## üóÇÔ∏è Structure du projet

```text
VideoCaptionerWithClip/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ image2caption.py               # CLI de l√©gende d'image unique
‚îú‚îÄ‚îÄ predict.py                     # CLI alternatif de l√©gende d'image
‚îú‚îÄ‚îÄ i2c.py                         # Classe ImageCaptioner r√©utilisable + CLI
‚îú‚îÄ‚îÄ v2c.py                         # Vid√©o -> SRT + JSON (l√©gende de frames en threads)
‚îú‚îÄ‚îÄ video2caption.py               # Impl√©mentation alternative vid√©o -> SRT (contraintes h√©rit√©es)
‚îú‚îÄ‚îÄ video2caption_v1.1.py          # Variante plus ancienne
‚îú‚îÄ‚îÄ video2caption_v1.0_not_work.py # Fichier explicitement marqu√© non fonctionnel
‚îú‚îÄ‚îÄ training.py                    # Point d'entr√©e de l'entra√Ænement
‚îú‚îÄ‚îÄ evaluate.py                    # √âvaluation sur split test et rendu des sorties
‚îú‚îÄ‚îÄ dataset_generation.py          # G√©n√®re data/processed/dataset.pkl
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ dataset.py                 # Dataset + utilitaires DataLoader
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ model.py                   # encodeur CLIP + mapping + d√©codeur GPT2
‚îÇ   ‚îî‚îÄ‚îÄ trainer.py                 # classe utilitaire entra√Ænement/validation/test
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # valeurs par d√©faut ConfigS / ConfigL
‚îÇ   ‚îú‚îÄ‚îÄ downloads.py               # t√©l√©chargement checkpoint Google Drive
‚îÇ   ‚îî‚îÄ‚îÄ lr_warmup.py               # planification de warmup de LR
‚îú‚îÄ‚îÄ i18n/                          # variantes du README multilingues
‚îî‚îÄ‚îÄ .auto-readme-work/             # artefacts pipeline auto-README
```

## üìã Pr√©requis

- Python `3.10+` recommand√©.
- Un GPU compatible CUDA est optionnel, mais fortement recommand√© pour l'entra√Ænement et l'inf√©rence de grands mod√®les.
- `ffmpeg` n'est pas requis directement par les scripts actuels (OpenCV est utilis√© pour l'extraction des frames).
- Un acc√®s Internet est n√©cessaire au premier t√©l√©chargement des mod√®les/checkpoints depuis Hugging Face / Google Drive.

Aucun lockfile n'est pr√©sent actuellement (`requirements.txt` / `pyproject.toml` absent), donc les d√©pendances sont d√©duites depuis les imports.

## üõ†Ô∏è Installation

### Configuration canonique √† partir de la structure du d√©p√¥t actuelle

```bash

git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

python -m venv .venv
source .venv/bin/activate

pip install --upgrade pip
pip install torch torchvision torchaudio
pip install transformers pillow matplotlib numpy tqdm opencv-python pandas wandb gdown
```

### Extrait d'installation du README original (pr√©serv√©)

Le README pr√©c√©dent se terminait au milieu d'un bloc. Les commandes d'origine sont conserv√©es ci-dessous exactement comme contenu historique source :

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip/src
```

Note : l'instantan√© actuel du d√©p√¥t place les scripts √† la racine du d√©p√¥t, pas sous `src/`.

## ‚ñ∂Ô∏è D√©marrage rapide

| Objectif | Commande |
|---|---|
| L√©gender une image | `python image2caption.py -I /path/to/image.jpg -S L -C model.pt` |
| L√©gender une vid√©o | `python v2c.py -V /path/to/video.mp4 -N 10` |
| G√©n√©rer le dataset | `python dataset_generation.py` |

### L√©gende d'une image (ex√©cution rapide)

```bash
python image2caption.py -I /path/to/image.jpg -S L -C model.pt
```

### L√©gende vid√©o (chemin recommand√©)

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

## üéØ Utilisation

### 1. L√©gende d'image (`image2caption.py`)

```bash
python image2caption.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

Arguments :

- `-I, --img-path` : chemin de l'image d'entr√©e.
- `-S, --size` : taille du mod√®le (`S` ou `L`).
- `-C, --checkpoint-name` : nom du checkpoint dans `weights/{small|large}`.
- `-R, --res-path` : r√©pertoire de sortie pour l'image l√©gend√©e rendue.
- `-T, --temperature` : temp√©rature d'√©chantillonnage.

### 2. CLI image alternative (`predict.py`)

```bash
python predict.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

`predict.py` est fonctionnellement proche de `image2caption.py` ; le format texte de sortie diff√®re l√©g√®rement.

### 3. API de classe pour la l√©gende d'image (`i2c.py`)

```bash
python i2c.py -I /path/to/image.jpg -S L -C model.pt -R ./data/result/prediction -T 1.0
```

Ou importez-la dans votre propre script :

```python
from i2c import ImageCaptioner

captioner = ImageCaptioner(model_size="L", checkpoint_name="model.pt")
captioner.set_image_path("/path/to/image.jpg")
caption = captioner.generate_caption(save_image=True)
print(caption)
```

### 4. Vid√©o vers sous-titres + JSON (`v2c.py`)

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

Sorties √† c√¥t√© de la vid√©o d'entr√©e :

- `<video_basename>_caption.srt`
- `<video_basename>_caption.json`
- `<video_basename>_captioning_frames/`

### 5. Pipeline vid√©o alternatif (`video2caption.py`)

```bash
python video2caption.py -V /path/to/video.mp4 -N 10
```

Important : ce script contient actuellement des chemins cod√©s en dur sp√©cifiques √† la machine :

- Python path par d√©faut : `/home/lachlan/miniconda3/envs/caption/bin/python`
- Chemin du script de l√©gende : `/home/lachlan/Projects/image_captioning/clip-gpt-captioning/src/image2caption.py`

Utilisez `v2c.py` sauf si vous maintenez volontairement ces chemins.

### 6. Variante h√©rit√©e (`video2caption_v1.1.py`)

Ce script est conserv√© √† titre de r√©f√©rence historique. Pr√©f√©rez `v2c.py` pour une utilisation active.

### 7. G√©n√©ration du dataset

```bash
python dataset_generation.py
```

Entr√©es attendues :

- `data/raw/results.csv` (table de l√©gendes s√©par√©es par `|`).
- `data/raw/flickr30k_images/` (fichiers image r√©f√©renc√©s par le CSV).

Sortie :

- `data/processed/dataset.pkl`

### 8. Entra√Ænement

```bash
python training.py -S L -C model.pt
```

L'entra√Ænement utilise la journalisation Weights & Biases (`wandb`) par d√©faut.

### 9. √âvaluation

```bash
python evaluate.py \
  -I ./data/raw/flickr30k_images \
  -R ./data/result/eval \
  -S L \
  -C model.pt \
  -T 1.0
```

L'√©valuation rend les l√©gendes pr√©dites sur les images de test et les enregistre dans :

- `<res-path>/<checkpoint_name_without_ext>_<SIZE>/`

## ‚öôÔ∏è Configuration

Les configurations de mod√®le sont d√©finies dans `utils/config.py` :

| Config | Backbone CLIP | Mod√®le GPT | Dossier des poids |
|---|---|---|---|
| `ConfigS` | `openai/clip-vit-base-patch32` | `gpt2` | `weights/small` |
| `ConfigL` | `openai/clip-vit-large-patch14` | `gpt2-medium` | `weights/large` |

Principaux param√®tres par d√©faut des classes de configuration :

| Champ | `ConfigS` | `ConfigL` |
|---|---:|---:|
| `epochs` | 150 | 120 |
| `lr` | 3e-3 | 5e-3 |
| `batch_size_exp` | 6 | 5 |
| `ep_len` | 4 | 4 |
| `max_len` | 40 | 40 |

Les IDs de t√©l√©chargement automatique des checkpoints sont dans `utils/downloads.py` :

| Taille | ID Google Drive |
|---|---|
| `L` | `1Gh32arzhW06C1ZJyzcJSSfdJDi3RgWoG` |
| `S` | `1pSQruQyg8KJq6VmzhMLFbT_VaHJMdlWF` |

## üì¶ Fichiers de sortie

### Inf√©rence image

- Image enregistr√©e avec titre superpos√©/g√©n√©r√© dans `--res-path`.
- Motif de nom de fichier : `<input_stem>-R<SIZE>.jpg`.

### Inf√©rence vid√©o (`v2c.py`)

- SRT : `<video_stem>_caption.srt`
- JSON : `<video_stem>_caption.json`
- Images des frames : `<video_stem>_captioning_frames/`

Exemple d'√©l√©ment JSON :

```json
{
  "start": "00:00:03,200",
  "end": "00:00:03,700",
  "lang": "en",
  "text": "A dog running through a field."
}
```

## üß™ Exemples

### Exemple rapide de l√©gende d'image

```bash
python image2caption.py -I ./examples/dog.jpg -S S -C model.pt
```

Comportement attendu :

- Si `weights/small/model.pt` est manquant, il est t√©l√©charg√©.
- Une image l√©gend√©e est √©crite par d√©faut dans `./data/result/prediction`.
- Le texte de la l√©gende est affich√© sur stdout.

### Exemple rapide de l√©gende vid√©o

```bash
python v2c.py -V ./examples/demo.mp4 -N 8
```

Comportement attendu :

- 8 frames sont l√©gend√©es par √©chantillonnage uniforme.
- Des fichiers `.srt` et `.json` sont g√©n√©r√©s √† c√¥t√© de la vid√©o d'entr√©e.

### Cha√Æne entra√Ænement/√©valuation de bout en bout

```bash
python dataset_generation.py
python training.py -S L -C model.pt
python evaluate.py -I ./data/raw/flickr30k_images -R ./data/result/eval -S L -C model.pt -T 1.0
```

## üß≠ Notes de d√©veloppement

- Un chevauchement h√©rit√© existe entre `v2c.py`, `video2caption.py` et `video2caption_v1.*`.
- `video2caption_v1.0_not_work.py` est conserv√© intentionnellement comme code h√©rit√© non fonctionnel.
- `training.py` s√©lectionne actuellement `ConfigL()` via `config = ConfigL() if args.size.upper() else ConfigS()`, ce qui se r√©sout toujours vers `ConfigL` pour des valeurs `--size` non vides.
- `model/trainer.py` utilise `self.dataset` dans `test_step`, tandis que l'initialiseur assigne `self.test_dataset` ; cela peut casser l'√©chantillonnage pendant les runs d'entra√Ænement si ce n'est pas ajust√©.
- `video2caption_v1.1.py` r√©f√©rence `self.config.transform`, alors que `ConfigS`/`ConfigL` ne d√©finissent pas `transform`.
- Aucun suite de CI/tests n'est actuellement d√©finie dans cet instantan√© de d√©p√¥t.
- Note i18n : des liens de langue sont pr√©sents en haut de ce README ; des fichiers traduits peuvent √™tre ajout√©s sous `i18n/`.
- Note d'√©tat actuelle : la barre de langue r√©f√©rence `i18n/README.ru.md`, mais ce fichier n'est pas pr√©sent dans cet instantan√©.

## ü©∫ D√©pannage

- `AssertionError: Image does not exist`
  - V√©rifiez que `-I/--img-path` pointe vers un fichier valide.
- `Dataset file not found. Downloading...`
  - `MiniFlickrDataset` l√®ve ce message quand `data/processed/dataset.pkl` est absent ; ex√©cutez d'abord `python dataset_generation.py`.
- `Path to the test image folder does not exist`
  - V√©rifiez que `evaluate.py -I` pointe vers un dossier existant.
- Ex√©cution initiale lente ou en √©chec
  - La premi√®re ex√©cution t√©l√©charge les mod√®les Hugging Face et peut t√©l√©charger des checkpoints Google Drive.
- `video2caption.py` renvoie des l√©gendes vides
  - V√©rifiez le chemin du script cod√© en dur et le chemin de l'ex√©cutable Python, ou passez √† `v2c.py`.
- `wandb` demande une connexion pendant l'entra√Ænement
  - Ex√©cutez `wandb login` ou d√©sactivez la journalisation dans `training.py` si n√©cessaire.

## üõ£Ô∏è Feuille de route

- Ajouter des fichiers lock de d√©pendances (`requirements.txt` ou `pyproject.toml`) pour des installations reproductibles.
- Unifier les pipelines vid√©o dupliqu√©s en une impl√©mentation maintenue.
- Supprimer les chemins machines cod√©s en dur des scripts h√©rit√©s.
- Corriger les bugs connus de cas limites d'entra√Ænement/√©valuation dans `training.py` et `model/trainer.py`.
- Ajouter des tests automatis√©s et une CI.
- Compl√©ter `i18n/` avec les README traduits r√©f√©renc√©s dans la barre de langues.

## ü§ù Contribution

Les contributions sont les bienvenues. Workflow sugg√©r√© :

```bash
# 1) Fork et clone
 git clone git@github.com:<your-user>/VideoCaptionerWithClip.git
 cd VideoCaptionerWithClip

# 2) Cr√©er une branche de fonctionnalit√©
 git checkout -b feat/your-change

# 3) Faire les changements et valider
 git add .
 git commit -m "feat: describe your change"

# 4) Pousser et ouvrir une PR
 git push origin feat/your-change
```

Si vous modifiez le comportement du mod√®le, incluez :

- Une(ou plusieurs) commande(s) reproductibles.
- Des exemples de sortie avant/apr√®s.
- Des notes sur les hypoth√®ses li√©es aux checkpoints ou au dataset.

## ‚ù§Ô∏è Support

| Donate | PayPal | Stripe |
|---|---|---|
| [![Donate](https://img.shields.io/badge/Donate-LazyingArt-0EA5E9?style=for-the-badge&logo=ko-fi&logoColor=white)](https://chat.lazying.art/donate) | [![PayPal](https://img.shields.io/badge/PayPal-RongzhouChen-00457C?style=for-the-badge&logo=paypal&logoColor=white)](https://paypal.me/RongzhouChen) | [![Stripe](https://img.shields.io/badge/Stripe-Donate-635BFF?style=for-the-badge&logo=stripe&logoColor=white)](https://buy.stripe.com/aFadR8gIaflgfQV6T4fw400) |

## üìÑ License

Aucun fichier de licence n'est pr√©sent dans l'instantan√© actuel du d√©p√¥t.

Note d'hypoth√®se : tant qu'un fichier `LICENSE` n'est pas ajout√©, les conditions de r√©utilisation/distribution restent ind√©finies.

[English](../README.md) ¬∑ [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](README.ar.md) ¬∑ [Espa√±ol](README.es.md) ¬∑ [Fran√ßais](README.fr.md) ¬∑ [Êó•Êú¨Ë™û](README.ja.md) ¬∑ [ÌïúÍµ≠Ïñ¥](README.ko.md) ¬∑ [Ti·∫øng Vi·ªát](README.vi.md) ¬∑ [‰∏≠Êñá (ÁÆÄ‰Ωì)](README.zh-Hans.md) ¬∑ [‰∏≠ÊñáÔºàÁπÅÈ´îÔºâ](README.zh-Hant.md) ¬∑ [Deutsch](README.de.md) ¬∑ [–†—É—Å—Å–∫–∏–π](README.ru.md)


# Clip-GPT-Captioning

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow)
![Status](https://img.shields.io/badge/README-Expanded-success)
![Repo Layout](https://img.shields.io/badge/Layout-Root%20Scripts-informational)
![Legacy Scripts](https://img.shields.io/badge/Legacy%20Scripts-Present-orange)
![i18n](https://img.shields.io/badge/i18n-Enabled-brightgreen)
![Maintained Path](https://img.shields.io/badge/Video-v2c.py-2ea44f)

Une bo√Æte √† outils Python pour g√©n√©rer des l√©gendes en langage naturel sur des images et des vid√©os en combinant les embeddings visuels OpenAI CLIP avec un mod√®le de langage de type GPT.

## ‚ú® Vue d'ensemble

Ce d√©p√¥t fournit :

- Des scripts d'inf√©rence pour le sous-titrage d'images et la g√©n√©ration de sous-titres vid√©o.
- Un pipeline d'entra√Ænement qui apprend une projection entre les embeddings visuels CLIP et les embeddings de tokens GPT-2.
- Des utilitaires de g√©n√©ration de jeu de donn√©es pour des donn√©es de type Flickr30k.
- Le t√©l√©chargement automatique de checkpoints pour les tailles de mod√®le prises en charge lorsque les poids sont absents.
- Des variantes multilingues du README sous `i18n/` (voir la barre des langues ci-dessus).

L'impl√©mentation actuelle inclut des scripts r√©cents et h√©rit√©s. Certains fichiers h√©rit√©s sont conserv√©s √† titre de r√©f√©rence et document√©s ci-dessous.

## üöÄ Fonctionnalit√©s

- L√©gendage d'image unique via `image2caption.py`.
- L√©gendage vid√©o (√©chantillonnage uniforme des frames) via `v2c.py` ou `video2caption.py`.
- Options d'ex√©cution personnalisables :
  - Nombre de frames.
  - Taille du mod√®le.
  - Temp√©rature d'√©chantillonnage.
  - Nom du checkpoint.
- L√©gendage multiprocessus/thread√© pour acc√©l√©rer l'inf√©rence vid√©o.
- Art√©facts de sortie :
  - Fichiers de sous-titres SRT (`.srt`).
  - Transcriptions JSON (`.json`) dans `v2c.py`.
- Points d'entr√©e d'entra√Ænement et d'√©valuation pour les exp√©riences de projection CLIP+GPT2.

### En un coup d'oeil

| Domaine | Script(s) principal(aux) | Remarques |
|---|---|---|
| L√©gendage d'image | `image2caption.py`, `i2c.py`, `predict.py` | CLI + classe r√©utilisable |
| L√©gendage vid√©o | `v2c.py` | Voie recommand√©e et maintenue |
| Flux vid√©o h√©rit√© | `video2caption.py`, `video2caption_v1.1.py` | Contient des hypoth√®ses sp√©cifiques √† la machine |
| Construction du dataset | `dataset_generation.py` | Produit `data/processed/dataset.pkl` |
| Entra√Ænement / √©val | `training.py`, `evaluate.py` | Utilise la projection CLIP+GPT2 |

## üß± Architecture (haut niveau)

Le mod√®le central dans `model/model.py` comporte trois parties :

1. `ImageEncoder` : extrait l'embedding d'image CLIP.
2. `Mapping` : projette l'embedding CLIP vers une s√©quence d'embeddings de pr√©fixe GPT.
3. `TextDecoder` : t√™te de mod√®le de langage GPT-2 qui g√©n√®re de fa√ßon autor√©gressive les tokens de l√©gende.

L'entra√Ænement (`Net.train_forward`) utilise des embeddings d'image CLIP pr√©-calcul√©s + des l√©gendes tokenis√©es.
L'inf√©rence (`Net.forward`) utilise une image PIL et d√©code les tokens jusqu'√† EOS ou `max_len`.

### Flux de donn√©es

1. Pr√©parer le dataset : `dataset_generation.py` lit `data/raw/results.csv` et les images dans `data/raw/flickr30k_images/`, puis √©crit `data/processed/dataset.pkl`.
2. Entra√Æner : `training.py` charge les tuples s√©rialis√©s `(image_name, image_embedding, caption)` et entra√Æne les couches mapper/decoder.
3. √âvaluer : `evaluate.py` rend les l√©gendes g√©n√©r√©es sur des images de test.
4. Servir l'inf√©rence :
   - image : `image2caption.py` / `predict.py` / `i2c.py`.
   - vid√©o : `v2c.py` (recommand√©), `video2caption.py` (h√©rit√©).

## üóÇÔ∏è Structure du projet

```text
VideoCaptionerWithClip/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ image2caption.py               # Single-image caption CLI
‚îú‚îÄ‚îÄ predict.py                     # Alternate single-image caption CLI
‚îú‚îÄ‚îÄ i2c.py                         # Reusable ImageCaptioner class + CLI
‚îú‚îÄ‚îÄ v2c.py                         # Video -> SRT + JSON (threaded frame captioning)
‚îú‚îÄ‚îÄ video2caption.py               # Alternate video -> SRT implementation (legacy constraints)
‚îú‚îÄ‚îÄ video2caption_v1.1.py          # Older variant
‚îú‚îÄ‚îÄ video2caption_v1.0_not_work.py # Explicitly marked non-working legacy file
‚îú‚îÄ‚îÄ training.py                    # Model training entrypoint
‚îú‚îÄ‚îÄ evaluate.py                    # Test-split evaluation and rendered outputs
‚îú‚îÄ‚îÄ dataset_generation.py          # Builds data/processed/dataset.pkl
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ dataset.py                 # Dataset + DataLoader helpers
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ model.py                   # CLIP encoder + mapping + GPT2 decoder
‚îÇ   ‚îî‚îÄ‚îÄ trainer.py                 # Training/validation/test utility class
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # ConfigS / ConfigL defaults
‚îÇ   ‚îú‚îÄ‚îÄ downloads.py               # Google Drive checkpoint downloader
‚îÇ   ‚îî‚îÄ‚îÄ lr_warmup.py               # LR warmup schedule
‚îú‚îÄ‚îÄ i18n/                          # Multilingual README variants
‚îî‚îÄ‚îÄ .auto-readme-work/             # Auto-README pipeline artifacts
```

## üìã Pr√©requis

- Python `3.10+` recommand√©.
- Un GPU compatible CUDA est optionnel mais fortement recommand√© pour l'entra√Ænement et l'inf√©rence avec les grands mod√®les.
- `ffmpeg` n'est pas requis directement par les scripts actuels (OpenCV est utilis√© pour l'extraction de frames).
- Un acc√®s internet est n√©cessaire au premier lancement pour t√©l√©charger les mod√®les/checkpoints depuis Hugging Face / Google Drive.

Aucun lockfile n'est actuellement pr√©sent (`requirements.txt` / `pyproject.toml` absents), les d√©pendances sont donc d√©duites des imports.

## üõ†Ô∏è Installation

### Configuration canonique depuis la structure actuelle du d√©p√¥t

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

python -m venv .venv
source .venv/bin/activate

pip install --upgrade pip
pip install torch torchvision torchaudio
pip install transformers pillow matplotlib numpy tqdm opencv-python pandas wandb gdown
```

### Extrait d'installation du README d'origine (pr√©serv√©)

Le README pr√©c√©dent se terminait au milieu d'un bloc. Les commandes d'origine sont conserv√©es ci-dessous exactement comme contenu historique de r√©f√©rence :

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip/src
```

Remarque : dans l'instantan√© actuel du d√©p√¥t, les scripts se trouvent √† la racine du d√©p√¥t, pas sous `src/`.

## ‚ñ∂Ô∏è D√©marrage rapide

### L√©gendage d'image (ex√©cution rapide)

```bash
python image2caption.py -I /path/to/image.jpg -S L -C model.pt
```

### L√©gendage vid√©o (voie recommand√©e)

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

## üéØ Utilisation

### 1. L√©gendage d'image (`image2caption.py`)

```bash
python image2caption.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

Arguments :

- `-I, --img-path` : chemin de l'image d'entr√©e.
- `-S, --size` : taille du mod√®le (`S` ou `L`).
- `-C, --checkpoint-name` : nom de fichier du checkpoint dans `weights/{small|large}`.
- `-R, --res-path` : r√©pertoire de sortie pour l'image l√©gend√©e rendue.
- `-T, --temperature` : temp√©rature d'√©chantillonnage.

### 2. CLI image alternative (`predict.py`)

```bash
python predict.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

`predict.py` est fonctionnellement similaire √† `image2caption.py` ; le format du texte de sortie diff√®re l√©g√®rement.

### 3. API de classe pour l√©gendage d'image (`i2c.py`)

```bash
python i2c.py -I /path/to/image.jpg -S L -C model.pt -R ./data/result/prediction -T 1.0
```

Ou importez-la dans votre propre script :

```python
from i2c import ImageCaptioner

captioner = ImageCaptioner(model_size="L", checkpoint_name="model.pt")
captioner.set_image_path("/path/to/image.jpg")
caption = captioner.generate_caption(save_image=True)
print(caption)
```

### 4. Vid√©o vers sous-titres + JSON (`v2c.py`)

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

Sorties √† c√¥t√© de la vid√©o d'entr√©e :

- `<video_basename>_caption.srt`
- `<video_basename>_caption.json`
- `<video_basename>_captioning_frames/`

### 5. Pipeline vid√©o alternatif (`video2caption.py`)

```bash
python video2caption.py -V /path/to/video.mp4 -N 10
```

Important : ce script contient actuellement des chemins cod√©s en dur, sp√©cifiques √† une machine :

- Python path default: `/home/lachlan/miniconda3/envs/caption/bin/python`
- Caption script path: `/home/lachlan/Projects/image_captioning/clip-gpt-captioning/src/image2caption.py`

Utilisez `v2c.py` sauf si vous maintenez volontairement ces chemins.

### 6. Variante h√©rit√©e (`video2caption_v1.1.py`)

Ce script est conserv√© √† des fins de r√©f√©rence historique. Pr√©f√©rez `v2c.py` pour un usage actif.

### 7. G√©n√©ration du dataset

```bash
python dataset_generation.py
```

Entr√©es brutes attendues :

- `data/raw/results.csv` (table de l√©gendes s√©par√©e par des pipes).
- `data/raw/flickr30k_images/` (fichiers image r√©f√©renc√©s par le CSV).

Sortie :

- `data/processed/dataset.pkl`

### 8. Entra√Ænement

```bash
python training.py -S L -C model.pt
```

L'entra√Ænement utilise par d√©faut la journalisation Weights & Biases (`wandb`).

### 9. √âvaluation

```bash
python evaluate.py \
  -I ./data/raw/flickr30k_images \
  -R ./data/result/eval \
  -S L \
  -C model.pt \
  -T 1.0
```

L'√©valuation rend les l√©gendes pr√©dites sur les images de test et les enregistre sous :

- `<res-path>/<checkpoint_name_without_ext>_<SIZE>/`

## ‚öôÔ∏è Configuration

Les configurations de mod√®le sont d√©finies dans `utils/config.py` :

| Config | CLIP backbone | GPT model | Weights dir |
|---|---|---|---|
| `ConfigS` | `openai/clip-vit-base-patch32` | `gpt2` | `weights/small` |
| `ConfigL` | `openai/clip-vit-large-patch14` | `gpt2-medium` | `weights/large` |

Principales valeurs par d√©faut des classes de configuration :

| Field | `ConfigS` | `ConfigL` |
|---|---:|---:|
| `epochs` | 150 | 120 |
| `lr` | 3e-3 | 5e-3 |
| `batch_size_exp` | 6 | 5 |
| `ep_len` | 4 | 4 |
| `max_len` | 40 | 40 |

Les IDs de t√©l√©chargement automatique des checkpoints sont dans `utils/downloads.py` :

| Size | Google Drive ID |
|---|---|
| `L` | `1Gh32arzhW06C1ZJyzcJSSfdJDi3RgWoG` |
| `S` | `1pSQruQyg8KJq6VmzhMLFbT_VaHJMdlWF` |

## üì¶ Fichiers de sortie

### Inf√©rence image

- Image enregistr√©e avec titre superpos√©/g√©n√©r√© dans `--res-path`.
- Sch√©ma de nom de fichier : `<input_stem>-R<SIZE>.jpg`.

### Inf√©rence vid√©o (`v2c.py`)

- SRT: `<video_stem>_caption.srt`
- JSON: `<video_stem>_caption.json`
- Images de frames : `<video_stem>_captioning_frames/`

Exemple d'√©l√©ment JSON :

```json
{
  "start": "00:00:03,200",
  "end": "00:00:03,700",
  "lang": "en",
  "text": "A dog running through a field."
}
```

## üß™ Exemples

### Exemple rapide de l√©gendage d'image

```bash
python image2caption.py -I ./examples/dog.jpg -S S -C model.pt
```

Comportement attendu :

- Si `weights/small/model.pt` est absent, il est t√©l√©charg√©.
- Une image l√©gend√©e est √©crite par d√©faut dans `./data/result/prediction`.
- Le texte de la l√©gende est affich√© sur stdout.

### Exemple rapide de l√©gendage vid√©o

```bash
python v2c.py -V ./examples/demo.mp4 -N 8
```

Comportement attendu :

- 8 frames √©chantillonn√©es uniform√©ment sont l√©gend√©es.
- Des fichiers `.srt` et `.json` sont g√©n√©r√©s √† c√¥t√© de la vid√©o d'entr√©e.

### S√©quence entra√Ænement/√©valuation de bout en bout

```bash
python dataset_generation.py
python training.py -S L -C model.pt
python evaluate.py -I ./data/raw/flickr30k_images -R ./data/result/eval -S L -C model.pt -T 1.0
```

## üß≠ Notes de d√©veloppement

- Un chevauchement h√©rit√© existe entre `v2c.py`, `video2caption.py` et `video2caption_v1.*`.
- `video2caption_v1.0_not_work.py` est intentionnellement conserv√© comme code h√©rit√© non fonctionnel.
- `training.py` s√©lectionne actuellement `ConfigL()` via `config = ConfigL() if args.size.upper() else ConfigS()`, ce qui r√©sout toujours vers `ConfigL` pour les valeurs non vides de `--size`.
- `model/trainer.py` utilise `self.dataset` dans `test_step`, tandis que l'initialiseur assigne `self.test_dataset` ; cela peut casser l'√©chantillonnage pendant l'entra√Ænement sans ajustement.
- `video2caption_v1.1.py` r√©f√©rence `self.config.transform`, mais `ConfigS`/`ConfigL` ne d√©finissent pas `transform`.
- Aucune suite de tests/CI n'est actuellement d√©finie dans cet instantan√© du d√©p√¥t.
- Note i18n : des liens de langue sont pr√©sents en haut de ce README ; des fichiers traduits peuvent √™tre ajout√©s sous `i18n/`.
- Note sur l'√©tat actuel : la barre de langue r√©f√©rence `i18n/README.ru.md`, mais ce fichier n'est pas pr√©sent dans cet instantan√©.

## ü©∫ D√©pannage

- `AssertionError: Image does not exist`
  - V√©rifiez que `-I/--img-path` pointe vers un fichier valide.
- `Dataset file not found. Downloading...`
  - `MiniFlickrDataset` d√©clenche ceci quand `data/processed/dataset.pkl` est manquant ; ex√©cutez d'abord `python dataset_generation.py`.
- `Path to the test image folder does not exist`
  - V√©rifiez que `evaluate.py -I` pointe vers un dossier existant.
- Premier lancement lent ou en √©chec
  - Le premier lancement t√©l√©charge les mod√®les Hugging Face et peut t√©l√©charger des checkpoints depuis Google Drive.
- `video2caption.py` renvoie des l√©gendes vides
  - V√©rifiez le chemin du script cod√© en dur et le chemin de l'ex√©cutable Python, ou passez √† `v2c.py`.
- `wandb` demande une connexion pendant l'entra√Ænement
  - Ex√©cutez `wandb login` ou d√©sactivez manuellement la journalisation dans `training.py` si n√©cessaire.

## üõ£Ô∏è Feuille de route

- Ajouter des lockfiles de d√©pendances (`requirements.txt` ou `pyproject.toml`) pour des installations reproductibles.
- Unifier les pipelines vid√©o dupliqu√©s en une impl√©mentation maintenue.
- Supprimer les chemins machine cod√©s en dur des scripts h√©rit√©s.
- Corriger les bugs connus de cas limites d'entra√Ænement/√©valuation dans `training.py` et `model/trainer.py`.
- Ajouter des tests automatis√©s et de la CI.
- Compl√©ter `i18n/` avec les README traduits r√©f√©renc√©s dans la barre des langues.

## ü§ù Contribution

Les contributions sont les bienvenues. Workflow sugg√©r√© :

```bash
# 1) Fork and clone
git clone git@github.com:<your-user>/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

# 2) Create a feature branch
git checkout -b feat/your-change

# 3) Make changes and commit
git add .
git commit -m "feat: describe your change"

# 4) Push and open a PR
git push origin feat/your-change
```

Si vous modifiez le comportement du mod√®le, incluez :

- Des commandes reproductibles.
- Des exemples de sorties avant/apr√®s.
- Des notes sur les hypoth√®ses li√©es aux checkpoints ou au dataset.

## üôå Support

Aucune configuration explicite de donation/sponsoring n'est pr√©sente dans l'instantan√© actuel du d√©p√¥t.

Si des liens de sponsoring sont ajout√©s plus tard, ils doivent √™tre pr√©serv√©s dans cette section.

## üìÑ Licence

Aucun fichier de licence n'est pr√©sent dans l'instantan√© actuel du d√©p√¥t.

Note d'hypoth√®se : tant qu'un fichier `LICENSE` n'est pas ajout√©, les conditions de r√©utilisation/distribution sont ind√©finies.

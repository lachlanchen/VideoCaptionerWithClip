[English](../README.md) Â· [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](README.ar.md) Â· [EspaÃ±ol](README.es.md) Â· [FranÃ§ais](README.fr.md) Â· [æ—¥æœ¬èª](README.ja.md) Â· [í•œêµ­ì–´](README.ko.md) Â· [Tiáº¿ng Viá»‡t](README.vi.md) Â· [ä¸­æ–‡ (ç®€ä½“)](README.zh-Hans.md) Â· [ä¸­æ–‡ï¼ˆç¹é«”ï¼‰](README.zh-Hant.md) Â· [Deutsch](README.de.md) Â· [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README.ru.md)


# Clip-GPT-Captioning

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow)
![Status](https://img.shields.io/badge/README-Expanded-success)
![Repo Layout](https://img.shields.io/badge/Layout-Root%20Scripts-informational)
![Legacy Scripts](https://img.shields.io/badge/Legacy%20Scripts-Present-orange)
![i18n](https://img.shields.io/badge/i18n-Enabled-brightgreen)
![Maintained Path](https://img.shields.io/badge/Video-v2c.py-2ea44f)

Bá»™ cÃ´ng cá»¥ Python Ä‘á»ƒ táº¡o chÃº thÃ­ch ngÃ´n ngá»¯ tá»± nhiÃªn cho hÃ¬nh áº£nh vÃ  video báº±ng cÃ¡ch káº¿t há»£p embedding thá»‹ giÃ¡c OpenAI CLIP vá»›i mÃ´ hÃ¬nh ngÃ´n ngá»¯ kiá»ƒu GPT.

## âœ¨ Tá»•ng quan

Kho lÆ°u trá»¯ nÃ y cung cáº¥p:

- Script suy luáº­n cho chÃº thÃ­ch áº£nh vÃ  táº¡o phá»¥ Ä‘á» video.
- Pipeline huáº¥n luyá»‡n há»c Ã¡nh xáº¡ tá»« CLIP visual embeddings sang GPT-2 token embeddings.
- Tiá»‡n Ã­ch táº¡o dataset theo kiá»ƒu Flickr30k.
- Tá»± Ä‘á»™ng táº£i checkpoint cho cÃ¡c kÃ­ch thÆ°á»›c model Ä‘Æ°á»£c há»— trá»£ khi thiáº¿u weights.
- CÃ¡c biáº¿n thá»ƒ README Ä‘a ngÃ´n ngá»¯ trong `i18n/` (xem thanh ngÃ´n ngá»¯ á»Ÿ trÃªn).

Báº£n triá»ƒn khai hiá»‡n táº¡i bao gá»“m cáº£ script má»›i vÃ  script legacy. Má»™t sá»‘ file legacy Ä‘Æ°á»£c giá»¯ láº¡i Ä‘á»ƒ tham kháº£o vÃ  Ä‘Æ°á»£c mÃ´ táº£ bÃªn dÆ°á»›i.

## ğŸš€ TÃ­nh nÄƒng

- Táº¡o caption cho má»™t áº£nh qua `image2caption.py`.
- Táº¡o caption cho video (láº¥y máº«u frame Ä‘á»“ng Ä‘á»u) qua `v2c.py` hoáº·c `video2caption.py`.
- TÃ¹y chá»‰nh cÃ¡c tÃ¹y chá»n runtime:
  - Sá»‘ lÆ°á»£ng frame.
  - KÃ­ch thÆ°á»›c model.
  - Nhiá»‡t Ä‘á»™ láº¥y máº«u.
  - TÃªn checkpoint.
- Caption Ä‘a tiáº¿n trÃ¬nh/Ä‘a luá»“ng Ä‘á»ƒ suy luáº­n video nhanh hÆ¡n.
- Tá»‡p Ä‘áº§u ra:
  - Tá»‡p phá»¥ Ä‘á» SRT (`.srt`).
  - Transcript JSON (`.json`) trong `v2c.py`.
- Äiá»ƒm vÃ o huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ cho thÃ­ nghiá»‡m Ã¡nh xáº¡ CLIP+GPT2.

### TÃ³m táº¯t nhanh

| Khu vá»±c | Script chÃ­nh | Ghi chÃº |
|---|---|---|
| ChÃº thÃ­ch áº£nh | `image2caption.py`, `i2c.py`, `predict.py` | CLI + lá»›p tÃ¡i sá»­ dá»¥ng |
| ChÃº thÃ­ch video | `v2c.py` | ÄÆ°á»ng dáº«n Ä‘Æ°á»£c duy trÃ¬, khuyáº¿n nghá»‹ dÃ¹ng |
| Luá»“ng video legacy | `video2caption.py`, `video2caption_v1.1.py` | Chá»©a cÃ¡c giáº£ Ä‘á»‹nh phá»¥ thuá»™c mÃ¡y cá»¥ thá»ƒ |
| Táº¡o dataset | `dataset_generation.py` | Táº¡o `data/processed/dataset.pkl` |
| Huáº¥n luyá»‡n / Ä‘Ã¡nh giÃ¡ | `training.py`, `evaluate.py` | DÃ¹ng Ã¡nh xáº¡ CLIP+GPT2 |

## ğŸ§± Kiáº¿n trÃºc (Má»©c cao)

MÃ´ hÃ¬nh cá»‘t lÃµi trong `model/model.py` cÃ³ ba pháº§n:

1. `ImageEncoder`: trÃ­ch xuáº¥t CLIP image embedding.
2. `Mapping`: chiáº¿u CLIP embedding thÃ nh chuá»—i GPT prefix embedding.
3. `TextDecoder`: Ä‘áº§u ra mÃ´ hÃ¬nh ngÃ´n ngá»¯ GPT-2 Ä‘á»ƒ tá»± há»“i quy sinh token caption.

Huáº¥n luyá»‡n (`Net.train_forward`) dÃ¹ng CLIP image embeddings Ä‘Ã£ tÃ­nh trÆ°á»›c + caption Ä‘Ã£ token hÃ³a.
Suy luáº­n (`Net.forward`) dÃ¹ng áº£nh PIL vÃ  giáº£i mÃ£ token Ä‘áº¿n EOS hoáº·c `max_len`.

### Luá»“ng dá»¯ liá»‡u

1. Chuáº©n bá»‹ dataset: `dataset_generation.py` Ä‘á»c `data/raw/results.csv` vÃ  áº£nh trong `data/raw/flickr30k_images/`, ghi `data/processed/dataset.pkl`.
2. Huáº¥n luyá»‡n: `training.py` náº¡p tuple pickle `(image_name, image_embedding, caption)` vÃ  huáº¥n luyá»‡n cÃ¡c lá»›p mapper/decoder.
3. ÄÃ¡nh giÃ¡: `evaluate.py` render caption Ä‘Æ°á»£c táº¡o lÃªn cÃ¡c áº£nh test hold-out.
4. Phá»¥c vá»¥ suy luáº­n:
   - áº£nh: `image2caption.py` / `predict.py` / `i2c.py`.
   - video: `v2c.py` (khuyáº¿n nghá»‹), `video2caption.py` (legacy).

## ğŸ—‚ï¸ Cáº¥u trÃºc dá»± Ã¡n

```text
VideoCaptionerWithClip/
â”œâ”€â”€ README.md
â”œâ”€â”€ image2caption.py               # CLI caption áº£nh Ä‘Æ¡n
â”œâ”€â”€ predict.py                     # CLI caption áº£nh Ä‘Æ¡n thay tháº¿
â”œâ”€â”€ i2c.py                         # Lá»›p ImageCaptioner tÃ¡i sá»­ dá»¥ng + CLI
â”œâ”€â”€ v2c.py                         # Video -> SRT + JSON (caption frame Ä‘a luá»“ng)
â”œâ”€â”€ video2caption.py               # CÃ i Ä‘áº·t Video -> SRT thay tháº¿ (rÃ ng buá»™c legacy)
â”œâ”€â”€ video2caption_v1.1.py          # Biáº¿n thá»ƒ cÅ© hÆ¡n
â”œâ”€â”€ video2caption_v1.0_not_work.py # File legacy Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u rÃµ lÃ  khÃ´ng hoáº¡t Ä‘á»™ng
â”œâ”€â”€ training.py                    # Äiá»ƒm vÃ o huáº¥n luyá»‡n model
â”œâ”€â”€ evaluate.py                    # ÄÃ¡nh giÃ¡ táº­p test vÃ  Ä‘áº§u ra Ä‘Ã£ render
â”œâ”€â”€ dataset_generation.py          # Táº¡o data/processed/dataset.pkl
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dataset.py                 # Bá»™ trá»£ giÃºp Dataset + DataLoader
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py                   # CLIP encoder + mapping + GPT2 decoder
â”‚   â””â”€â”€ trainer.py                 # Lá»›p tiá»‡n Ã­ch huáº¥n luyá»‡n/xÃ¡c thá»±c/kiá»ƒm thá»­
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                  # Máº·c Ä‘á»‹nh ConfigS / ConfigL
â”‚   â”œâ”€â”€ downloads.py               # TrÃ¬nh táº£i checkpoint tá»« Google Drive
â”‚   â””â”€â”€ lr_warmup.py               # Lá»‹ch LR warmup
â”œâ”€â”€ i18n/                          # CÃ¡c biáº¿n thá»ƒ README Ä‘a ngÃ´n ngá»¯
â””â”€â”€ .auto-readme-work/             # Táº¡o tÃ¡c pipeline Auto-README
```

## ğŸ“‹ Äiá»u kiá»‡n tiÃªn quyáº¿t

- Khuyáº¿n nghá»‹ Python `3.10+`.
- GPU há»— trá»£ CUDA lÃ  tÃ¹y chá»n nhÆ°ng ráº¥t nÃªn cÃ³ cho huáº¥n luyá»‡n vÃ  suy luáº­n model lá»›n.
- `ffmpeg` khÃ´ng báº¯t buá»™c trá»±c tiáº¿p vá»›i script hiá»‡n táº¡i (dÃ¹ng OpenCV Ä‘á»ƒ trÃ­ch frame).
- Cáº§n internet á»Ÿ láº§n cháº¡y Ä‘áº§u Ä‘á»ƒ táº£i model/checkpoint tá»« Hugging Face / Google Drive.

Hiá»‡n chÆ°a cÃ³ lockfile (`requirements.txt` / `pyproject.toml` thiáº¿u), nÃªn dependency Ä‘Æ°á»£c suy ra tá»« cÃ¡c import.

## ğŸ› ï¸ CÃ i Ä‘áº·t

### Thiáº¿t láº­p chuáº©n tá»« cáº¥u trÃºc repo hiá»‡n táº¡i

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

python -m venv .venv
source .venv/bin/activate

pip install --upgrade pip
pip install torch torchvision torchaudio
pip install transformers pillow matplotlib numpy tqdm opencv-python pandas wandb gdown
```

### Äoáº¡n cÃ i Ä‘áº·t tá»« README gá»‘c (Ä‘Æ°á»£c giá»¯ nguyÃªn)

README trÆ°á»›c Ä‘Ã³ káº¿t thÃºc giá»¯a chá»«ng trong má»™t block. CÃ¡c lá»‡nh gá»‘c Ä‘Æ°á»£c giá»¯ nguyÃªn bÃªn dÆ°á»›i nhÆ° ná»™i dung lá»‹ch sá»­ nguá»“n chuáº©n:

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip/src
```

LÆ°u Ã½: báº£n snapshot repo hiá»‡n táº¡i Ä‘áº·t script á»Ÿ root repo, khÃ´ng náº±m trong `src/`.

## â–¶ï¸ Báº¯t Ä‘áº§u nhanh

### ChÃº thÃ­ch áº£nh (cháº¡y nhanh)

```bash
python image2caption.py -I /path/to/image.jpg -S L -C model.pt
```

### ChÃº thÃ­ch video (Ä‘Æ°á»ng dáº«n khuyáº¿n nghá»‹)

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

## ğŸ¯ CÃ¡ch dÃ¹ng

### 1. ChÃº thÃ­ch áº£nh (`image2caption.py`)

```bash
python image2caption.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

Tham sá»‘:

- `-I, --img-path`: Ä‘Æ°á»ng dáº«n áº£nh Ä‘áº§u vÃ o.
- `-S, --size`: kÃ­ch thÆ°á»›c model (`S` hoáº·c `L`).
- `-C, --checkpoint-name`: tÃªn file checkpoint trong `weights/{small|large}`.
- `-R, --res-path`: thÆ° má»¥c Ä‘áº§u ra cho áº£nh Ä‘Ã£ render caption.
- `-T, --temperature`: nhiá»‡t Ä‘á»™ láº¥y máº«u.

### 2. CLI áº£nh thay tháº¿ (`predict.py`)

```bash
python predict.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

`predict.py` tÆ°Æ¡ng tá»± vá» chá»©c nÄƒng vá»›i `image2caption.py`; Ä‘á»‹nh dáº¡ng vÄƒn báº£n Ä‘áº§u ra khÃ¡c nháº¹.

### 3. API lá»›p chÃº thÃ­ch áº£nh (`i2c.py`)

```bash
python i2c.py -I /path/to/image.jpg -S L -C model.pt -R ./data/result/prediction -T 1.0
```

Hoáº·c import trong script cá»§a báº¡n:

```python
from i2c import ImageCaptioner

captioner = ImageCaptioner(model_size="L", checkpoint_name="model.pt")
captioner.set_image_path("/path/to/image.jpg")
caption = captioner.generate_caption(save_image=True)
print(caption)
```

### 4. Video thÃ nh phá»¥ Ä‘á» + JSON (`v2c.py`)

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

Äáº§u ra náº±m cáº¡nh video Ä‘áº§u vÃ o:

- `<video_basename>_caption.srt`
- `<video_basename>_caption.json`
- `<video_basename>_captioning_frames/`

### 5. Pipeline video thay tháº¿ (`video2caption.py`)

```bash
python video2caption.py -V /path/to/video.mp4 -N 10
```

Quan trá»ng: script nÃ y hiá»‡n chá»©a cÃ¡c Ä‘Æ°á»ng dáº«n hardcode phá»¥ thuá»™c mÃ¡y:

- Python path máº·c Ä‘á»‹nh: `/home/lachlan/miniconda3/envs/caption/bin/python`
- ÄÆ°á»ng dáº«n script caption: `/home/lachlan/Projects/image_captioning/clip-gpt-captioning/src/image2caption.py`

DÃ¹ng `v2c.py` trá»« khi báº¡n chá»§ Ä‘Ã­ch duy trÃ¬ cÃ¡c Ä‘Æ°á»ng dáº«n nÃ y.

### 6. Biáº¿n thá»ƒ legacy (`video2caption_v1.1.py`)

Script nÃ y Ä‘Æ°á»£c giá»¯ láº¡i Ä‘á»ƒ tham chiáº¿u lá»‹ch sá»­. Vá»›i nhu cáº§u dÃ¹ng thá»±c táº¿, hÃ£y Æ°u tiÃªn `v2c.py`.

### 7. Táº¡o dataset

```bash
python dataset_generation.py
```

Äáº§u vÃ o raw dá»± kiáº¿n:

- `data/raw/results.csv` (báº£ng caption phÃ¢n tÃ¡ch báº±ng pipe).
- `data/raw/flickr30k_images/` (cÃ¡c tá»‡p áº£nh Ä‘Æ°á»£c CSV tham chiáº¿u).

Äáº§u ra:

- `data/processed/dataset.pkl`

### 8. Huáº¥n luyá»‡n

```bash
python training.py -S L -C model.pt
```

Huáº¥n luyá»‡n máº·c Ä‘á»‹nh dÃ¹ng logging Weights & Biases (`wandb`).

### 9. ÄÃ¡nh giÃ¡

```bash
python evaluate.py \
  -I ./data/raw/flickr30k_images \
  -R ./data/result/eval \
  -S L \
  -C model.pt \
  -T 1.0
```

ÄÃ¡nh giÃ¡ sáº½ render caption dá»± Ä‘oÃ¡n lÃªn áº£nh test vÃ  lÆ°u táº¡i:

- `<res-path>/<checkpoint_name_without_ext>_<SIZE>/`

## âš™ï¸ Cáº¥u hÃ¬nh

Cáº¥u hÃ¬nh model Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong `utils/config.py`:

| Config | CLIP backbone | GPT model | Weights dir |
|---|---|---|---|
| `ConfigS` | `openai/clip-vit-base-patch32` | `gpt2` | `weights/small` |
| `ConfigL` | `openai/clip-vit-large-patch14` | `gpt2-medium` | `weights/large` |

CÃ¡c giÃ¡ trá»‹ máº·c Ä‘á»‹nh chÃ­nh tá»« lá»›p config:

| Field | `ConfigS` | `ConfigL` |
|---|---:|---:|
| `epochs` | 150 | 120 |
| `lr` | 3e-3 | 5e-3 |
| `batch_size_exp` | 6 | 5 |
| `ep_len` | 4 | 4 |
| `max_len` | 40 | 40 |

ID tá»± Ä‘á»™ng táº£i checkpoint náº±m trong `utils/downloads.py`:

| Size | Google Drive ID |
|---|---|
| `L` | `1Gh32arzhW06C1ZJyzcJSSfdJDi3RgWoG` |
| `S` | `1pSQruQyg8KJq6VmzhMLFbT_VaHJMdlWF` |

## ğŸ“¦ Tá»‡p Ä‘áº§u ra

### Suy luáº­n áº£nh

- áº¢nh Ä‘Æ°á»£c lÆ°u vá»›i tiÃªu Ä‘á» Ä‘Ã£ chÃ¨n/táº¡o táº¡i `--res-path`.
- Máº«u tÃªn tá»‡p: `<input_stem>-R<SIZE>.jpg`.

### Suy luáº­n video (`v2c.py`)

- SRT: `<video_stem>_caption.srt`
- JSON: `<video_stem>_caption.json`
- áº¢nh frame: `<video_stem>_captioning_frames/`

VÃ­ dá»¥ má»™t pháº§n tá»­ JSON:

```json
{
  "start": "00:00:03,200",
  "end": "00:00:03,700",
  "lang": "en",
  "text": "A dog running through a field."
}
```

## ğŸ§ª VÃ­ dá»¥

### VÃ­ dá»¥ nhanh cho chÃº thÃ­ch áº£nh

```bash
python image2caption.py -I ./examples/dog.jpg -S S -C model.pt
```

HÃ nh vi dá»± kiáº¿n:

- Náº¿u thiáº¿u `weights/small/model.pt`, tá»‡p sáº½ Ä‘Æ°á»£c táº£i vá».
- áº¢nh cÃ³ caption máº·c Ä‘á»‹nh Ä‘Æ°á»£c ghi vÃ o `./data/result/prediction`.
- VÄƒn báº£n caption Ä‘Æ°á»£c in ra stdout.

### VÃ­ dá»¥ nhanh cho chÃº thÃ­ch video

```bash
python v2c.py -V ./examples/demo.mp4 -N 8
```

HÃ nh vi dá»± kiáº¿n:

- 8 frame láº¥y máº«u Ä‘á»“ng Ä‘á»u sáº½ Ä‘Æ°á»£c táº¡o caption.
- Tá»‡p `.srt` vÃ  `.json` Ä‘Æ°á»£c táº¡o cáº¡nh video Ä‘áº§u vÃ o.

### Chuá»—i huáº¥n luyá»‡n/Ä‘Ã¡nh giÃ¡ end-to-end

```bash
python dataset_generation.py
python training.py -S L -C model.pt
python evaluate.py -I ./data/raw/flickr30k_images -R ./data/result/eval -S L -C model.pt -T 1.0
```

## ğŸ§­ Ghi chÃº phÃ¡t triá»ƒn

- CÃ³ sá»± chá»“ng láº·p legacy giá»¯a `v2c.py`, `video2caption.py` vÃ  `video2caption_v1.*`.
- `video2caption_v1.0_not_work.py` Ä‘Æ°á»£c giá»¯ láº¡i cÃ³ chá»§ Ä‘Ã­ch nhÆ° mÃ£ legacy khÃ´ng hoáº¡t Ä‘á»™ng.
- `training.py` hiá»‡n chá»n `ConfigL()` qua `config = ConfigL() if args.size.upper() else ConfigS()`, luÃ´n tráº£ vá» `ConfigL` vá»›i má»i giÃ¡ trá»‹ `--size` khÃ´ng rá»—ng.
- `model/trainer.py` dÃ¹ng `self.dataset` trong `test_step`, trong khi hÃ m khá»Ÿi táº¡o gÃ¡n `self.test_dataset`; Ä‘iá»u nÃ y cÃ³ thá»ƒ lÃ m há»ng láº¥y máº«u trong cÃ¡c láº§n cháº¡y huáº¥n luyá»‡n náº¿u chÆ°a chá»‰nh sá»­a.
- `video2caption_v1.1.py` tham chiáº¿u `self.config.transform`, nhÆ°ng `ConfigS`/`ConfigL` khÃ´ng Ä‘á»‹nh nghÄ©a `transform`.
- KhÃ´ng cÃ³ CI/test suite Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong snapshot repo hiá»‡n táº¡i.
- Ghi chÃº i18n: liÃªn káº¿t ngÃ´n ngá»¯ cÃ³ á»Ÿ Ä‘áº§u README nÃ y; cÃ¡c tá»‡p dá»‹ch cÃ³ thá»ƒ Ä‘Æ°á»£c thÃªm dÆ°á»›i `i18n/`.
- Ghi chÃº tráº¡ng thÃ¡i hiá»‡n táº¡i: thanh ngÃ´n ngá»¯ liÃªn káº¿t tá»›i `i18n/README.ru.md`, nhÆ°ng tá»‡p Ä‘Ã³ khÃ´ng cÃ³ trong snapshot nÃ y.

## ğŸ©º Kháº¯c phá»¥c sá»± cá»‘

- `AssertionError: Image does not exist`
  - XÃ¡c nháº­n `-I/--img-path` trá» tá»›i tá»‡p há»£p lá»‡.
- `Dataset file not found. Downloading...`
  - `MiniFlickrDataset` sáº½ bÃ¡o lá»—i nÃ y khi thiáº¿u `data/processed/dataset.pkl`; hÃ£y cháº¡y `python dataset_generation.py` trÆ°á»›c.
- `Path to the test image folder does not exist`
  - XÃ¡c nháº­n `evaluate.py -I` trá» tá»›i thÆ° má»¥c tá»“n táº¡i.
- Láº§n cháº¡y Ä‘áº§u cháº­m hoáº·c tháº¥t báº¡i
  - Láº§n cháº¡y Ä‘áº§u sáº½ táº£i model tá»« Hugging Face vÃ  cÃ³ thá»ƒ táº£i checkpoint tá»« Google Drive.
- `video2caption.py` tráº£ vá» caption rá»—ng
  - Kiá»ƒm tra Ä‘Æ°á»ng dáº«n script hardcode vÃ  Ä‘Æ°á»ng dáº«n trÃ¬nh thÃ´ng dá»‹ch Python, hoáº·c chuyá»ƒn sang `v2c.py`.
- `wandb` yÃªu cáº§u Ä‘Äƒng nháº­p trong lÃºc huáº¥n luyá»‡n
  - Cháº¡y `wandb login` hoáº·c táº¯t logging thá»§ cÃ´ng trong `training.py` náº¿u cáº§n.

## ğŸ›£ï¸ Lá»™ trÃ¬nh

- ThÃªm lockfile dependency (`requirements.txt` hoáº·c `pyproject.toml`) Ä‘á»ƒ cÃ i Ä‘áº·t cÃ³ thá»ƒ tÃ¡i láº­p.
- Há»£p nháº¥t cÃ¡c pipeline video trÃ¹ng láº·p thÃ nh má»™t báº£n triá»ƒn khai Ä‘Æ°á»£c duy trÃ¬.
- Loáº¡i bá» Ä‘Æ°á»ng dáº«n mÃ¡y hardcode khá»i script legacy.
- Sá»­a cÃ¡c lá»—i biÃªn Ä‘Ã£ biáº¿t cá»§a huáº¥n luyá»‡n/Ä‘Ã¡nh giÃ¡ trong `training.py` vÃ  `model/trainer.py`.
- ThÃªm kiá»ƒm thá»­ tá»± Ä‘á»™ng vÃ  CI.
- HoÃ n thiá»‡n `i18n/` vá»›i cÃ¡c README dá»‹ch Ä‘Æ°á»£c tham chiáº¿u trong thanh ngÃ´n ngá»¯.

## ğŸ¤ ÄÃ³ng gÃ³p

Hoan nghÃªnh Ä‘Ã³ng gÃ³p. Quy trÃ¬nh gá»£i Ã½:

```bash
# 1) Fork and clone
git clone git@github.com:<your-user>/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

# 2) Create a feature branch
git checkout -b feat/your-change

# 3) Make changes and commit
git add .
git commit -m "feat: describe your change"

# 4) Push and open a PR
git push origin feat/your-change
```

Náº¿u báº¡n thay Ä‘á»•i hÃ nh vi model, hÃ£y kÃ¨m theo:

- (CÃ¡c) lá»‡nh cÃ³ thá»ƒ tÃ¡i láº­p.
- Máº«u Ä‘áº§u ra trÆ°á»›c/sau.
- Ghi chÃº vá» cÃ¡c giáº£ Ä‘á»‹nh checkpoint hoáº·c dataset.

## ğŸ™Œ Há»— trá»£

KhÃ´ng cÃ³ cáº¥u hÃ¬nh donation/sponsorship rÃµ rÃ ng trong snapshot repo hiá»‡n táº¡i.

Náº¿u liÃªn káº¿t tÃ i trá»£ Ä‘Æ°á»£c thÃªm sau nÃ y, chÃºng nÃªn Ä‘Æ°á»£c giá»¯ trong pháº§n nÃ y.

## ğŸ“„ Giáº¥y phÃ©p

KhÃ´ng cÃ³ tá»‡p license trong snapshot repo hiá»‡n táº¡i.

Ghi chÃº giáº£ Ä‘á»‹nh: cho Ä‘áº¿n khi cÃ³ tá»‡p `LICENSE`, Ä‘iá»u khoáº£n tÃ¡i sá»­ dá»¥ng/phÃ¢n phá»‘i váº«n chÆ°a Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh.

[English](../README.md) Â· [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](README.ar.md) Â· [EspaÃ±ol](README.es.md) Â· [FranÃ§ais](README.fr.md) Â· [æ—¥æœ¬èª](README.ja.md) Â· [í•œêµ­ì–´](README.ko.md) Â· [Tiáº¿ng Viá»‡t](README.vi.md) Â· [ä¸­æ–‡ (ç®€ä½“)](README.zh-Hans.md) Â· [ä¸­æ–‡ï¼ˆç¹é«”ï¼‰](README.zh-Hant.md) Â· [Deutsch](README.de.md) Â· [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README.ru.md)


[![LazyingArt banner](https://github.com/lachlanchen/lachlanchen/raw/main/figs/banner.png)](https://github.com/lachlanchen/lachlanchen/blob/main/figs/banner.png)


# Clip-GPT-Captioning

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow)
![Status](https://img.shields.io/badge/README-Expanded-success)
![Repo Layout](https://img.shields.io/badge/Layout-Root%20Scripts-informational)
![Legacy Scripts](https://img.shields.io/badge/Legacy%20Scripts-Present-orange)
![i18n](https://img.shields.io/badge/i18n-Enabled-brightgreen)
![Maintained Path](https://img.shields.io/badge/Video-v2c.py-2ea44f)

Bá»™ cÃ´ng cá»¥ Python Ä‘á»ƒ táº¡o chÃº thÃ­ch ngÃ´n ngá»¯ tá»± nhiÃªn cho hÃ¬nh áº£nh vÃ  video báº±ng cÃ¡ch káº¿t há»£p embeddings thá»‹ giÃ¡c cá»§a OpenAI CLIP vá»›i mÃ´ hÃ¬nh ngÃ´n ngá»¯ kiá»ƒu GPT.

## ğŸ§­ Snapshot

| KÃ­ch thÆ°á»›c | Chi tiáº¿t |
|---|---|
| Pháº¡m vi nhiá»‡m vá»¥ | ChÃº thÃ­ch áº£nh vÃ  video |
| Káº¿t quáº£ chÃ­nh | Phá»¥ Ä‘á» SRT, transcript JSON, áº£nh cÃ³ chÃº thÃ­ch |
| Script chÃ­nh | `i2c.py`, `v2c.py`, `image2caption.py` |
| ÄÆ°á»ng dáº«n legacy | `video2caption.py` vÃ  cÃ¡c phiÃªn báº£n phiÃªn báº£n hoÃ¡ (giá»¯ láº¡i cho má»¥c Ä‘Ã­ch tham kháº£o) |
| Luá»“ng dá»¯ liá»‡u dataset | `data/raw/results.csv` + `data/raw/flickr30k_images/` |

## âœ¨ Tá»•ng quan

Repository nÃ y cung cáº¥p:

- Script suy luáº­n cho viá»‡c táº¡o caption áº£nh vÃ  phá»¥ Ä‘á» video.
- Pipeline huáº¥n luyá»‡n há»c Ã¡nh xáº¡ tá»« embeddings thá»‹ giÃ¡c CLIP sang token embeddings cá»§a GPT-2.
- Tiá»‡n Ã­ch táº¡o dataset theo kiá»ƒu Flickr30k.
- Tá»± Ä‘á»™ng táº£i checkpoint cá»§a cÃ¡c kÃ­ch thÆ°á»›c model Ä‘Æ°á»£c há»— trá»£ khi thiáº¿u trá»ng sá»‘.
- CÃ¡c phiÃªn báº£n README Ä‘a ngÃ´n ngá»¯ trong `i18n/` (xem thanh ngÃ´n ngá»¯ á»Ÿ trÃªn).

Triá»ƒn khai hiá»‡n táº¡i bao gá»“m cáº£ script má»›i vÃ  script legacy. Má»™t sá»‘ file legacy Ä‘Æ°á»£c giá»¯ láº¡i Ä‘á»ƒ tham kháº£o vÃ  Ä‘Æ°á»£c mÃ´ táº£ bÃªn dÆ°á»›i.

## ğŸš€ TÃ­nh nÄƒng

- ChÃº thÃ­ch áº£nh Ä‘Æ¡n qua `image2caption.py`.
- ChÃº thÃ­ch video (láº¥y máº«u frame Ä‘á»“ng Ä‘á»u) qua `v2c.py` hoáº·c `video2caption.py`.
- TÃ¹y chá»‰nh tuá»³ chá»n cháº¡y:
  - Sá»‘ lÆ°á»£ng frame.
  - KÃ­ch thÆ°á»›c model.
  - Nhiá»‡t Ä‘á»™ sampling.
  - TÃªn checkpoint.
- Suy luáº­n video song song/multi-thread Ä‘á»ƒ nhanh hÆ¡n.
- Tá»‡p Ä‘áº§u ra:
  - Tá»‡p phá»¥ Ä‘á» SRT (`.srt`).
  - Transcript JSON (`.json`) trong `v2c.py`.
- Äiá»ƒm vÃ o huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ cho thÃ­ nghiá»‡m Ã¡nh xáº¡ CLIP+GPT2.

### TÃ³m táº¯t nhanh

| Má»¥c | Script chÃ­nh | Ghi chÃº |
|---|---|---|
| ChÃº thÃ­ch áº£nh | `image2caption.py`, `i2c.py`, `predict.py` | CLI + lá»›p dÃ¹ng láº¡i Ä‘Æ°á»£c |
| ChÃº thÃ­ch video | `v2c.py` | ÄÆ°á»ng dáº«n Ä‘Æ°á»£c duy trÃ¬, khuyáº¿n nghá»‹ |
| DÃ²ng video legacy | `video2caption.py`, `video2caption_v1.1.py` | Chá»©a giáº£ Ä‘á»‹nh phá»¥ thuá»™c mÃ¡y |
| Táº¡o dataset | `dataset_generation.py` | Táº¡o ra `data/processed/dataset.pkl` |
| Huáº¥n luyá»‡n / Ä‘Ã¡nh giÃ¡ | `training.py`, `evaluate.py` | DÃ¹ng Ã¡nh xáº¡ CLIP+GPT2 |

## ğŸ§± Kiáº¿n trÃºc (Má»©c tá»•ng quan)

MÃ´ hÃ¬nh lÃµi trong `model/model.py` gá»“m ba pháº§n:

1. `ImageEncoder`: trÃ­ch xuáº¥t embedding hÃ¬nh áº£nh CLIP.
2. `Mapping`: biáº¿n Ä‘á»•i embedding CLIP thÃ nh má»™t chuá»—i embedding tiá»n tá»‘ cá»§a GPT.
3. `TextDecoder`: pháº§n GPT-2 tá»± há»“i quy sinh token caption.

Huáº¥n luyá»‡n (`Net.train_forward`) dÃ¹ng embeddings hÃ¬nh áº£nh CLIP Ä‘Ã£ Ä‘Æ°á»£c tÃ­nh trÆ°á»›c cá»™ng vá»›i caption Ä‘Ã£ tokenize.
Suy luáº­n (`Net.forward`) dÃ¹ng áº£nh PIL vÃ  giáº£i mÃ£ token cho Ä‘áº¿n khi gáº·p EOS hoáº·c `max_len`.

### Luá»“ng dá»¯ liá»‡u

1. Chuáº©n bá»‹ dataset: `dataset_generation.py` Ä‘á»c `data/raw/results.csv` vÃ  áº£nh trong `data/raw/flickr30k_images/`, ghi `data/processed/dataset.pkl`.
2. Huáº¥n luyá»‡n: `training.py` náº¡p tuple pickle `(image_name, image_embedding, caption)` rá»“i huáº¥n luyá»‡n cÃ¡c lá»›p mapper/decoder.
3. ÄÃ¡nh giÃ¡: `evaluate.py` render caption sinh ra trÃªn áº£nh test Ä‘Ã£ tÃ¡ch ra.
4. Cung cáº¥p suy luáº­n:
   - áº£nh: `image2caption.py` / `predict.py` / `i2c.py`.
   - video: `v2c.py` (khuyáº¿n nghá»‹), `video2caption.py` (legacy).

## ğŸ—‚ï¸ Cáº¥u trÃºc dá»± Ã¡n

```text
VideoCaptionerWithClip/
â”œâ”€â”€ README.md
â”œâ”€â”€ image2caption.py               # CLI caption áº£nh Ä‘Æ¡n
â”œâ”€â”€ predict.py                     # CLI caption áº£nh Ä‘Æ¡n thay tháº¿
â”œâ”€â”€ i2c.py                         # Lá»›p ImageCaptioner dÃ¹ng láº¡i Ä‘Æ°á»£c + CLI
â”œâ”€â”€ v2c.py                         # Video -> SRT + JSON (caption frame theo Ä‘a luá»“ng)
â”œâ”€â”€ video2caption.py               # Triá»ƒn khai thay tháº¿ video -> SRT (rÃ ng buá»™c legacy)
â”œâ”€â”€ video2caption_v1.1.py          # Biáº¿n thá»ƒ cÅ© hÆ¡n
â”œâ”€â”€ video2caption_v1.0_not_work.py # File legacy Ä‘Æ°á»£c ghi rÃµ lÃ  khÃ´ng hoáº¡t Ä‘á»™ng
â”œâ”€â”€ training.py                    # Äiá»ƒm vÃ o huáº¥n luyá»‡n mÃ´ hÃ¬nh
â”œâ”€â”€ evaluate.py                    # ÄÃ¡nh giÃ¡ táº­p test vÃ  Ä‘áº§u ra Ä‘Ã£ render
â”œâ”€â”€ dataset_generation.py          # Táº¡o data/processed/dataset.pkl
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dataset.py                 # Dataset + DataLoader helpers
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py                   # CLIP encoder + mapping + GPT2 decoder
â”‚   â””â”€â”€ trainer.py                 # Lá»›p tiá»‡n Ã­ch train/validation/test
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                  # Máº·c Ä‘á»‹nh ConfigS / ConfigL
â”‚   â”œâ”€â”€ downloads.py               # TrÃ¬nh táº£i checkpoint Google Drive
â”‚   â””â”€â”€ lr_warmup.py               # Lá»‹ch trÃ¬nh LR warmup
â”œâ”€â”€ i18n/                          # CÃ¡c phiÃªn báº£n README Ä‘a ngÃ´n ngá»¯
â””â”€â”€ .auto-readme-work/             # TÃ i sáº£n/artefact cá»§a pipeline auto-readme
```

## ğŸ“‹ YÃªu cáº§u tiÃªn quyáº¿t

- Python `3.10+` Ä‘Æ°á»£c khuyáº¿n nghá»‹.
- GPU há»— trá»£ CUDA khÃ´ng báº¯t buá»™c nhÆ°ng Ä‘Æ°á»£c khuyáº¿n nghá»‹ máº¡nh cho huáº¥n luyá»‡n vÃ  suy luáº­n model lá»›n.
- `ffmpeg` khÃ´ng báº¯t buá»™c trá»±c tiáº¿p bá»Ÿi cÃ¡c script hiá»‡n táº¡i (OpenCV Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ trÃ­ch frame).
- Cáº§n káº¿t ná»‘i internet láº§n Ä‘áº§u Ä‘á»ƒ táº£i model/checkpoint tá»« Hugging Face / Google Drive.

Hiá»‡n chÆ°a cÃ³ lockfile (`requirements.txt` / `pyproject.toml` váº¯ng máº·t), nÃªn dependency Ä‘Æ°á»£c suy ra tá»« cÃ¡c `import`.

## ğŸ› ï¸ CÃ i Ä‘áº·t

### Thiáº¿t láº­p chuáº©n theo cáº¥u trÃºc repository hiá»‡n táº¡i

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

python -m venv .venv
source .venv/bin/activate

pip install --upgrade pip
pip install torch torchvision torchaudio
pip install transformers pillow matplotlib numpy tqdm opencv-python pandas wandb gdown
```

### Äoáº¡n cÃ i Ä‘áº·t tá»« README gá»‘c (Ä‘Æ°á»£c giá»¯ nguyÃªn)

README trÆ°á»›c Ä‘Ã³ káº¿t thÃºc giá»¯a chá»«ng trong má»™t block. CÃ¡c lá»‡nh gá»‘c giá»¯ nguyÃªn nhÆ° ná»™i dung lá»‹ch sá»­ nguá»“n sau Ä‘Ã¢y:

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip/src
```

LÆ°u Ã½: snapshot repository hiá»‡n táº¡i Ä‘áº·t script á»Ÿ root repo, khÃ´ng náº±m trong `src/`.

## â–¶ï¸ Báº¯t Ä‘áº§u nhanh

| Má»¥c tiÃªu | Lá»‡nh |
|---|---|
| ChÃº thÃ­ch áº£nh | `python image2caption.py -I /path/to/image.jpg -S L -C model.pt` |
| ChÃº thÃ­ch video | `python v2c.py -V /path/to/video.mp4 -N 10` |
| Táº¡o dataset | `python dataset_generation.py` |

### ChÃº thÃ­ch áº£nh (cháº¡y nhanh)

```bash
python image2caption.py -I /path/to/image.jpg -S L -C model.pt
```

### ChÃº thÃ­ch video (Ä‘Æ°á»ng dáº«n khuyáº¿n nghá»‹)

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

## ğŸ¯ CÃ¡ch sá»­ dá»¥ng

### 1. ChÃº thÃ­ch áº£nh (`image2caption.py`)

```bash
python image2caption.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

Tham sá»‘:

- `-I, --img-path`: Ä‘Æ°á»ng dáº«n áº£nh Ä‘áº§u vÃ o.
- `-S, --size`: kÃ­ch thÆ°á»›c model (`S` hoáº·c `L`).
- `-C, --checkpoint-name`: tÃªn checkpoint trong `weights/{small|large}`.
- `-R, --res-path`: thÆ° má»¥c Ä‘áº§u ra cho áº£nh Ä‘Ã£ render caption.
- `-T, --temperature`: nhiá»‡t Ä‘á»™ sampling.

### 2. CLI áº£nh thay tháº¿ (`predict.py`)

```bash
python predict.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

`predict.py` vá» chá»©c nÄƒng tÆ°Æ¡ng tá»± `image2caption.py`; Ä‘á»‹nh dáº¡ng vÄƒn báº£n Ä‘áº§u ra cÃ³ chÃªnh lá»‡ch nháº¹.

### 3. API lá»›p chÃº thÃ­ch áº£nh (`i2c.py`)

```bash
python i2c.py -I /path/to/image.jpg -S L -C model.pt -R ./data/result/prediction -T 1.0
```

Hoáº·c import trong script riÃªng cá»§a báº¡n:

```python
from i2c import ImageCaptioner

captioner = ImageCaptioner(model_size="L", checkpoint_name="model.pt")
captioner.set_image_path("/path/to/image.jpg")
caption = captioner.generate_caption(save_image=True)
print(caption)
```

### 4. Video thÃ nh phá»¥ Ä‘á» + JSON (`v2c.py`)

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

Äáº§u ra náº±m cáº¡nh video Ä‘áº§u vÃ o:

- `<video_basename>_caption.srt`
- `<video_basename>_caption.json`
- `<video_basename>_captioning_frames/`

### 5. Pipeline video thay tháº¿ (`video2caption.py`)

```bash
python video2caption.py -V /path/to/video.mp4 -N 10
```

Quan trá»ng: script hiá»‡n táº¡i váº«n chá»©a cÃ¡c Ä‘Æ°á»ng dáº«n hardcoded phá»¥ thuá»™c mÃ¡y:

- Python path máº·c Ä‘á»‹nh: `/home/lachlan/miniconda3/envs/caption/bin/python`
- ÄÆ°á»ng dáº«n script caption: `/home/lachlan/Projects/image_captioning/clip-gpt-captioning/src/image2caption.py`

HÃ£y dÃ¹ng `v2c.py` trá»« khi báº¡n cá»‘ tÃ¬nh duy trÃ¬ cÃ¡c Ä‘Æ°á»ng dáº«n nÃ y.

### 6. Biáº¿n thá»ƒ legacy (`video2caption_v1.1.py`)

Script nÃ y Ä‘Æ°á»£c giá»¯ láº¡i Ä‘á»ƒ tham chiáº¿u lá»‹ch sá»­. Vá»›i dÃ¹ng thá»±c táº¿, Æ°u tiÃªn `v2c.py`.

### 7. Táº¡o dataset

```bash
python dataset_generation.py
```

Äáº§u vÃ o thÃ´ dá»± kiáº¿n:

- `data/raw/results.csv` (báº£ng caption phÃ¢n tÃ¡ch báº±ng dáº¥u `|`).
- `data/raw/flickr30k_images/` (cÃ¡c file áº£nh Ä‘Æ°á»£c CSV tham chiáº¿u).

Äáº§u ra:

- `data/processed/dataset.pkl`

### 8. Huáº¥n luyá»‡n

```bash
python training.py -S L -C model.pt
```

Huáº¥n luyá»‡n máº·c Ä‘á»‹nh dÃ¹ng logging cá»§a Weights & Biases (`wandb`).

### 9. ÄÃ¡nh giÃ¡

```bash
python evaluate.py \
  -I ./data/raw/flickr30k_images \
  -R ./data/result/eval \
  -S L \
  -C model.pt \
  -T 1.0
```

ÄÃ¡nh giÃ¡ render caption dá»± Ä‘oÃ¡n lÃªn áº£nh test vÃ  lÆ°u táº¡i:

- `<res-path>/<checkpoint_name_without_ext>_<SIZE>/`

## âš™ï¸ Cáº¥u hÃ¬nh

Cáº¥u hÃ¬nh mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong `utils/config.py`:

| Config | CLIP backbone | GPT model | Weights dir |
|---|---|---|---|
| `ConfigS` | `openai/clip-vit-base-patch32` | `gpt2` | `weights/small` |
| `ConfigL` | `openai/clip-vit-large-patch14` | `gpt2-medium` | `weights/large` |

GiÃ¡ trá»‹ máº·c Ä‘á»‹nh tá»« cÃ¡c lá»›p cáº¥u hÃ¬nh:

| TrÆ°á»ng | `ConfigS` | `ConfigL` |
|---|---:|---:|
| `epochs` | 150 | 120 |
| `lr` | 3e-3 | 5e-3 |
| `batch_size_exp` | 6 | 5 |
| `ep_len` | 4 | 4 |
| `max_len` | 40 | 40 |

ID tá»± Ä‘á»™ng táº£i checkpoint náº±m trong `utils/downloads.py`:

| KÃ­ch thÆ°á»›c | Google Drive ID |
|---|---|
| `L` | `1Gh32arzhW06C1ZJyzcJSSfdJDi3RgWoG` |
| `S` | `1pSQruQyg8KJq6VmzhMLFbT_VaHJMdlWF` |

## ğŸ“¦ Tá»‡p Ä‘áº§u ra

### Suy luáº­n áº£nh

- áº¢nh cÃ³ caption chá»“ng/ná»™i dung Ä‘Ã¨ lÃªn Ä‘Æ°á»£c lÆ°u táº¡i `--res-path`.
- Máº«u tÃªn file: `<input_stem>-R<SIZE>.jpg`.

### Suy luáº­n video (`v2c.py`)

- SRT: `<video_stem>_caption.srt`
- JSON: `<video_stem>_caption.json`
- Frame áº£nh: `<video_stem>_captioning_frames/`

VÃ­ dá»¥ má»™t pháº§n tá»­ JSON:

```json
{
  "start": "00:00:03,200",
  "end": "00:00:03,700",
  "lang": "en",
  "text": "A dog running through a field."
}
```

## ğŸ§ª VÃ­ dá»¥

### VÃ­ dá»¥ nhanh cho chÃº thÃ­ch áº£nh

```bash
python image2caption.py -I ./examples/dog.jpg -S S -C model.pt
```

HÃ nh vi dá»± kiáº¿n:

- Náº¿u thiáº¿u `weights/small/model.pt`, file sáº½ Ä‘Æ°á»£c táº£i vá».
- Máº·c Ä‘á»‹nh má»™t áº£nh cÃ³ caption Ä‘Æ°á»£c ghi vÃ o `./data/result/prediction`.
- VÄƒn báº£n caption Ä‘Æ°á»£c in ra stdout.

### VÃ­ dá»¥ nhanh cho chÃº thÃ­ch video

```bash
python v2c.py -V ./examples/demo.mp4 -N 8
```

HÃ nh vi dá»± kiáº¿n:

- 8 frame Ä‘Æ°á»£c láº¥y máº«u Ä‘á»“ng Ä‘á»u Ä‘á»ƒ táº¡o caption.
- Tá»‡p `.srt` vÃ  `.json` Ä‘Æ°á»£c táº¡o bÃªn cáº¡nh video Ä‘áº§u vÃ o.

### Chuá»—i huáº¥n luyá»‡n/Ä‘Ã¡nh giÃ¡ end-to-end

```bash
python dataset_generation.py
python training.py -S L -C model.pt
python evaluate.py -I ./data/raw/flickr30k_images -R ./data/result/eval -S L -C model.pt -T 1.0
```

## ğŸ§­ Ghi chÃº phÃ¡t triá»ƒn

- CÃ³ pháº§n chá»“ng láº·p legacy giá»¯a `v2c.py`, `video2caption.py`, vÃ  `video2caption_v1.*`.
- `video2caption_v1.0_not_work.py` Ä‘Æ°á»£c giá»¯ láº¡i cÃ³ chá»§ Ä‘Ã­ch nhÆ° mÃ£ legacy khÃ´ng hoáº¡t Ä‘á»™ng.
- `training.py` hiá»‡n chá»n `ConfigL()` qua `config = ConfigL() if args.size.upper() else ConfigS()`, luÃ´n giáº£i quyáº¿t vá» `ConfigL` cho má»i `--size` khÃ´ng rá»—ng.
- `model/trainer.py` dÃ¹ng `self.dataset` trong `test_step`, trong khi hÃ m khá»Ÿi táº¡o gÃ¡n `self.test_dataset`; Ä‘iá»u nÃ y cÃ³ thá»ƒ lÃ m há»ng sampling trong cÃ¡c láº§n cháº¡y huáº¥n luyá»‡n náº¿u chÆ°a chá»‰nh sá»­a.
- `video2caption_v1.1.py` tham chiáº¿u `self.config.transform`, nhÆ°ng `ConfigS`/`ConfigL` khÃ´ng Ä‘á»‹nh nghÄ©a `transform`.
- Táº¡m thá»i chÆ°a cÃ³ CI/test suite trong snapshot repository hiá»‡n táº¡i.
- Ghi chÃº i18n: cÃ¡c liÃªn káº¿t ngÃ´n ngá»¯ Ä‘Ã£ cÃ³ á»Ÿ Ä‘áº§u README; cÃ³ thá»ƒ bá»• sung thÃªm báº£n dá»‹ch khÃ¡c trong `i18n/`.
- Ghi chÃº tráº¡ng thÃ¡i hiá»‡n táº¡i: thanh ngÃ´n ngá»¯ liÃªn káº¿t Ä‘áº¿n `i18n/README.ru.md`, nhÆ°ng file nÃ y chÆ°a cÃ³ trong snapshot.

## ğŸ©º Xá»­ lÃ½ sá»± cá»‘

- `AssertionError: Image does not exist`
  - Kiá»ƒm tra `-I/--img-path` trá» Ä‘áº¿n má»™t file há»£p lá»‡.
- `Dataset file not found. Downloading...`
  - `MiniFlickrDataset` nÃªu lá»—i nÃ y khi `data/processed/dataset.pkl` chÆ°a cÃ³; cháº¡y `python dataset_generation.py` trÆ°á»›c.
- `Path to the test image folder does not exist`
  - Kiá»ƒm tra `evaluate.py -I` trá» Ä‘áº¿n folder hiá»‡n cÃ³.
- Cháº¡y láº§n Ä‘áº§u cháº­m hoáº·c lá»—i
  - Láº§n cháº¡y Ä‘áº§u sáº½ táº£i model tá»« Hugging Face vÃ  cÃ³ thá»ƒ táº£i checkpoint tá»« Google Drive.
- `video2caption.py` tráº£ vá» caption rá»—ng
  - Kiá»ƒm tra Ä‘Æ°á»ng dáº«n script hardcode vÃ  Python executable, hoáº·c chuyá»ƒn sang `v2c.py`.
- `wandb` yÃªu cáº§u Ä‘Äƒng nháº­p khi huáº¥n luyá»‡n
  - Cháº¡y `wandb login` hoáº·c táº¯t logging thá»§ cÃ´ng trong `training.py` náº¿u cáº§n.

## ğŸ›£ï¸ Lá»™ trÃ¬nh

- ThÃªm lockfile dependency (`requirements.txt` hoáº·c `pyproject.toml`) Ä‘á»ƒ cÃ i Ä‘áº·t tÃ¡i láº­p.
- Há»£p nháº¥t cÃ¡c pipeline video trÃ¹ng láº·p thÃ nh má»™t triá»ƒn khai duy nháº¥t Ä‘Æ°á»£c duy trÃ¬.
- Loáº¡i bá» hardcoded machine paths khá»i cÃ¡c script legacy.
- Sá»­a cÃ¡c bug biÃªn ná»•i tiáº¿ng trong `training.py` vÃ  `model/trainer.py`.
- ThÃªm tests tá»± Ä‘á»™ng vÃ  CI.
- Bá»• sung Ä‘áº§y Ä‘á»§ `i18n/` vá»›i cÃ¡c README Ä‘Ã£ dá»‹ch Ä‘Æ°á»£c tham chiáº¿u trong thanh ngÃ´n ngá»¯.

## ğŸ¤ ÄÃ³ng gÃ³p

ÄÃ³ng gÃ³p ráº¥t Ä‘Æ°á»£c hoan nghÃªnh. Quy trÃ¬nh gá»£i Ã½:

```bash
# 1) Fork vÃ  clone
git clone git@github.com:<your-user>/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

# 2) Táº¡o nhÃ¡nh tÃ­nh nÄƒng
git checkout -b feat/your-change

# 3) Thá»±c hiá»‡n thay Ä‘á»•i vÃ  commit
git add .
git commit -m "feat: describe your change"

# 4) Push vÃ  má»Ÿ PR
git push origin feat/your-change
```

Náº¿u báº¡n thay Ä‘á»•i hÃ nh vi cá»§a model, hÃ£y kÃ¨m theo:

- Lá»‡nh cÃ³ thá»ƒ tÃ¡i láº­p.
- VÃ­ dá»¥ Ä‘áº§u ra trÆ°á»›c/sau.
- Ghi chÃº vá» giáº£ Ä‘á»‹nh checkpoint hoáº·c dataset.

## â¤ï¸ Support

| Donate | PayPal | Stripe |
|---|---|---|
| [![Donate](https://img.shields.io/badge/Donate-LazyingArt-0EA5E9?style=for-the-badge&logo=ko-fi&logoColor=white)](https://chat.lazying.art/donate) | [![PayPal](https://img.shields.io/badge/PayPal-RongzhouChen-00457C?style=for-the-badge&logo=paypal&logoColor=white)](https://paypal.me/RongzhouChen) | [![Stripe](https://img.shields.io/badge/Stripe-Donate-635BFF?style=for-the-badge&logo=stripe&logoColor=white)](https://buy.stripe.com/aFadR8gIaflgfQV6T4fw400) |

## ğŸ“„ Giáº¥y phÃ©p

KhÃ´ng cÃ³ tá»‡p license trong snapshot repository hiá»‡n táº¡i.

LÆ°u Ã½ giáº£ Ä‘á»‹nh: cho Ä‘áº¿n khi thÃªm tá»‡p `LICENSE`, Ä‘iá»u khoáº£n tÃ¡i sá»­ dá»¥ng/phÃ¢n phá»‘i váº«n chÆ°a Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh.

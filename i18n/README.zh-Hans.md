[English](../README.md) Â· [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](README.ar.md) Â· [EspaÃ±ol](README.es.md) Â· [FranÃ§ais](README.fr.md) Â· [æ—¥æœ¬èª](README.ja.md) Â· [í•œêµ­ì–´](README.ko.md) Â· [Tiáº¿ng Viá»‡t](README.vi.md) Â· [ä¸­æ–‡ (ç®€ä½“)](README.zh-Hans.md) Â· [ä¸­æ–‡ï¼ˆç¹é«”ï¼‰](README.zh-Hant.md) Â· [Deutsch](README.de.md) Â· [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README.ru.md)


# Clip-GPT-Captioning

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow)
![Status](https://img.shields.io/badge/README-Expanded-success)
![Repo Layout](https://img.shields.io/badge/Layout-Root%20Scripts-informational)
![Legacy Scripts](https://img.shields.io/badge/Legacy%20Scripts-Present-orange)
![i18n](https://img.shields.io/badge/i18n-Enabled-brightgreen)
![Maintained Path](https://img.shields.io/badge/Video-v2c.py-2ea44f)

ä¸€ä¸ª Python å·¥å…·åŒ…ï¼šç»“åˆ OpenAI CLIP è§†è§‰åµŒå…¥ä¸ GPT é£æ ¼è¯­è¨€æ¨¡å‹ï¼Œä¸ºå›¾åƒå’Œè§†é¢‘ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ã€‚

## âœ¨ æ¦‚è§ˆ

æœ¬ä»“åº“æä¾›ï¼š

- å›¾åƒæè¿°ä¸è§†é¢‘å­—å¹•ç”Ÿæˆçš„æ¨ç†è„šæœ¬ã€‚
- è®­ç»ƒæµæ°´çº¿ï¼šå­¦ä¹ ä» CLIP è§†è§‰åµŒå…¥åˆ° GPT-2 token åµŒå…¥çš„æ˜ å°„ã€‚
- é¢å‘ Flickr30k é£æ ¼æ•°æ®çš„æ•°æ®é›†ç”Ÿæˆå·¥å…·ã€‚
- å½“æƒé‡ç¼ºå¤±æ—¶ï¼Œæ”¯æŒæ¨¡å‹å°ºå¯¸çš„è‡ªåŠ¨ checkpoint ä¸‹è½½ã€‚
- ä½äº `i18n/` ä¸‹çš„å¤šè¯­è¨€ README ç‰ˆæœ¬ï¼ˆè§ä¸Šæ–¹è¯­è¨€æ ï¼‰ã€‚

å½“å‰å®ç°åŒæ—¶åŒ…å«è¾ƒæ–°çš„è„šæœ¬ä¸å†å²é—ç•™è„šæœ¬ã€‚éƒ¨åˆ†é—ç•™æ–‡ä»¶ä¸ºå‚è€ƒä¿ç•™ï¼Œå¹¶åœ¨ä¸‹æ–‡è¯´æ˜ã€‚

## ğŸš€ ç‰¹æ€§

- é€šè¿‡ `image2caption.py` å®ç°å•å›¾æè¿°ã€‚
- é€šè¿‡ `v2c.py` æˆ– `video2caption.py` å®ç°è§†é¢‘æè¿°ï¼ˆå‡åŒ€æŠ½å¸§ï¼‰ã€‚
- å¯è‡ªå®šä¹‰è¿è¡Œå‚æ•°ï¼š
  - å¸§æ•°ã€‚
  - æ¨¡å‹å°ºå¯¸ã€‚
  - é‡‡æ ·æ¸©åº¦ã€‚
  - Checkpoint åç§°ã€‚
- æ”¯æŒå¤šè¿›ç¨‹/å¤šçº¿ç¨‹æè¿°ï¼ŒåŠ é€Ÿè§†é¢‘æ¨ç†ã€‚
- è¾“å‡ºäº§ç‰©ï¼š
  - SRT å­—å¹•æ–‡ä»¶ï¼ˆ`.srt`ï¼‰ã€‚
  - `v2c.py` ç”Ÿæˆçš„ JSON æ–‡æœ¬ï¼ˆ`.json`ï¼‰ã€‚
- æä¾› CLIP+GPT2 æ˜ å°„å®éªŒçš„è®­ç»ƒä¸è¯„ä¼°å…¥å£ã€‚

### ä¸€è§ˆ

| åŒºåŸŸ | ä¸»è¦è„šæœ¬ | è¯´æ˜ |
|---|---|---|
| å›¾åƒæè¿° | `image2caption.py`, `i2c.py`, `predict.py` | CLI + å¯å¤ç”¨ç±» |
| è§†é¢‘æè¿° | `v2c.py` | æ¨èçš„ç»´æŠ¤è·¯å¾„ |
| é—ç•™è§†é¢‘æµç¨‹ | `video2caption.py`, `video2caption_v1.1.py` | å«æœºå™¨ç›¸å…³å‡è®¾ |
| æ•°æ®é›†æ„å»º | `dataset_generation.py` | ç”Ÿæˆ `data/processed/dataset.pkl` |
| è®­ç»ƒ / è¯„ä¼° | `training.py`, `evaluate.py` | ä½¿ç”¨ CLIP+GPT2 æ˜ å°„ |

## ğŸ§± æ¶æ„ï¼ˆé«˜å±‚ï¼‰

`model/model.py` ä¸­çš„æ ¸å¿ƒæ¨¡å‹åŒ…å«ä¸‰éƒ¨åˆ†ï¼š

1. `ImageEncoder`ï¼šæå– CLIP å›¾åƒåµŒå…¥ã€‚
2. `Mapping`ï¼šå°† CLIP åµŒå…¥æŠ•å½±ä¸º GPT å‰ç¼€åµŒå…¥åºåˆ—ã€‚
3. `TextDecoder`ï¼šGPT-2 è¯­è¨€æ¨¡å‹å¤´ï¼Œè‡ªå›å½’ç”Ÿæˆæè¿° tokenã€‚

è®­ç»ƒï¼ˆ`Net.train_forward`ï¼‰ä½¿ç”¨é¢„è®¡ç®—çš„ CLIP å›¾åƒåµŒå…¥ + åˆ†è¯åçš„æè¿°ã€‚
æ¨ç†ï¼ˆ`Net.forward`ï¼‰ä½¿ç”¨ PIL å›¾åƒå¹¶æŒç»­è§£ç  tokenï¼Œç›´åˆ° EOS æˆ– `max_len`ã€‚

### æ•°æ®æµ

1. å‡†å¤‡æ•°æ®é›†ï¼š`dataset_generation.py` è¯»å– `data/raw/results.csv` ä¸ `data/raw/flickr30k_images/` ä¸­å›¾åƒï¼Œå†™å…¥ `data/processed/dataset.pkl`ã€‚
2. è®­ç»ƒï¼š`training.py` åŠ è½½ pickle å…ƒç»„ `(image_name, image_embedding, caption)` å¹¶è®­ç»ƒ mapper/decoder å±‚ã€‚
3. è¯„ä¼°ï¼š`evaluate.py` åœ¨ç•™å‡ºçš„æµ‹è¯•å›¾åƒä¸Šæ¸²æŸ“ç”Ÿæˆæè¿°ã€‚
4. æä¾›æ¨ç†ï¼š
   - å›¾åƒï¼š`image2caption.py` / `predict.py` / `i2c.py`ã€‚
   - è§†é¢‘ï¼š`v2c.py`ï¼ˆæ¨èï¼‰ã€`video2caption.py`ï¼ˆé—ç•™ï¼‰ã€‚

## ğŸ—‚ï¸ é¡¹ç›®ç»“æ„

```text
VideoCaptionerWithClip/
â”œâ”€â”€ README.md
â”œâ”€â”€ image2caption.py               # Single-image caption CLI
â”œâ”€â”€ predict.py                     # Alternate single-image caption CLI
â”œâ”€â”€ i2c.py                         # Reusable ImageCaptioner class + CLI
â”œâ”€â”€ v2c.py                         # Video -> SRT + JSON (threaded frame captioning)
â”œâ”€â”€ video2caption.py               # Alternate video -> SRT implementation (legacy constraints)
â”œâ”€â”€ video2caption_v1.1.py          # Older variant
â”œâ”€â”€ video2caption_v1.0_not_work.py # Explicitly marked non-working legacy file
â”œâ”€â”€ training.py                    # Model training entrypoint
â”œâ”€â”€ evaluate.py                    # Test-split evaluation and rendered outputs
â”œâ”€â”€ dataset_generation.py          # Builds data/processed/dataset.pkl
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dataset.py                 # Dataset + DataLoader helpers
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py                   # CLIP encoder + mapping + GPT2 decoder
â”‚   â””â”€â”€ trainer.py                 # Training/validation/test utility class
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                  # ConfigS / ConfigL defaults
â”‚   â”œâ”€â”€ downloads.py               # Google Drive checkpoint downloader
â”‚   â””â”€â”€ lr_warmup.py               # LR warmup schedule
â”œâ”€â”€ i18n/                          # Multilingual README variants
â””â”€â”€ .auto-readme-work/             # Auto-README pipeline artifacts
```

## ğŸ“‹ å‰ç½®æ¡ä»¶

- æ¨è Python `3.10+`ã€‚
- CUDA GPU éå¿…éœ€ï¼Œä½†å¼ºçƒˆå»ºè®®ç”¨äºè®­ç»ƒå’Œå¤§æ¨¡å‹æ¨ç†ã€‚
- å½“å‰è„šæœ¬ä¸ç›´æ¥ä¾èµ– `ffmpeg`ï¼ˆä½¿ç”¨ OpenCV æŠ½å¸§ï¼‰ã€‚
- é¦–æ¬¡è¿è¡Œéœ€è¦è”ç½‘ä» Hugging Face / Google Drive ä¸‹è½½æ¨¡å‹ä¸ checkpointã€‚

å½“å‰æ²¡æœ‰ lockfileï¼ˆç¼ºå°‘ `requirements.txt` / `pyproject.toml`ï¼‰ï¼Œå› æ­¤ä¾èµ–ç”±å¯¼å…¥é¡¹æ¨æ–­ã€‚

## ğŸ› ï¸ å®‰è£…

### åŸºäºå½“å‰ä»“åº“ç»“æ„çš„æ ‡å‡†å®‰è£…

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

python -m venv .venv
source .venv/bin/activate

pip install --upgrade pip
pip install torch torchvision torchaudio
pip install transformers pillow matplotlib numpy tqdm opencv-python pandas wandb gdown
```

### åŸ README çš„å®‰è£…ç‰‡æ®µï¼ˆä¿ç•™ï¼‰

ä¹‹å‰çš„ README åœ¨ä»£ç å—ä¸­é€”ç»“æŸã€‚ä»¥ä¸‹å‘½ä»¤æŒ‰åŸå§‹å†…å®¹é€å­—ä¿ç•™ï¼Œä½œä¸ºå†å²çœŸå®è®°å½•ï¼š

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip/src
```

æ³¨æ„ï¼šå½“å‰ä»“åº“å¿«ç…§ä¸­çš„è„šæœ¬ä½äºä»“åº“æ ¹ç›®å½•ï¼Œä¸åœ¨ `src/` ä¸‹ã€‚

## â–¶ï¸ å¿«é€Ÿå¼€å§‹

### å›¾åƒæè¿°ï¼ˆå¿«é€Ÿè¿è¡Œï¼‰

```bash
python image2caption.py -I /path/to/image.jpg -S L -C model.pt
```

### è§†é¢‘æè¿°ï¼ˆæ¨èè·¯å¾„ï¼‰

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

## ğŸ¯ ç”¨æ³•

### 1. å›¾åƒæè¿°ï¼ˆ`image2caption.py`ï¼‰

```bash
python image2caption.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

å‚æ•°ï¼š

- `-I, --img-path`ï¼šè¾“å…¥å›¾åƒè·¯å¾„ã€‚
- `-S, --size`ï¼šæ¨¡å‹å°ºå¯¸ï¼ˆ`S` æˆ– `L`ï¼‰ã€‚
- `-C, --checkpoint-name`ï¼š`weights/{small|large}` ä¸­çš„ checkpoint æ–‡ä»¶åã€‚
- `-R, --res-path`ï¼šæ¸²æŸ“åå›¾åƒè¾“å‡ºç›®å½•ã€‚
- `-T, --temperature`ï¼šé‡‡æ ·æ¸©åº¦ã€‚

### 2. å¤‡ç”¨å›¾åƒ CLIï¼ˆ`predict.py`ï¼‰

```bash
python predict.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

`predict.py` åœ¨åŠŸèƒ½ä¸Šä¸ `image2caption.py` ç±»ä¼¼ï¼›è¾“å‡ºæ–‡æœ¬æ ¼å¼ç•¥æœ‰å·®å¼‚ã€‚

### 3. å›¾åƒæè¿°ç±» APIï¼ˆ`i2c.py`ï¼‰

```bash
python i2c.py -I /path/to/image.jpg -S L -C model.pt -R ./data/result/prediction -T 1.0
```

æˆ–åœ¨ä½ è‡ªå·±çš„è„šæœ¬ä¸­å¯¼å…¥ï¼š

```python
from i2c import ImageCaptioner

captioner = ImageCaptioner(model_size="L", checkpoint_name="model.pt")
captioner.set_image_path("/path/to/image.jpg")
caption = captioner.generate_caption(save_image=True)
print(caption)
```

### 4. è§†é¢‘è½¬å­—å¹• + JSONï¼ˆ`v2c.py`ï¼‰

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

è¾“å‡ºä½äºè¾“å…¥è§†é¢‘æ—è¾¹ï¼š

- `<video_basename>_caption.srt`
- `<video_basename>_caption.json`
- `<video_basename>_captioning_frames/`

### 5. å¤‡ç”¨è§†é¢‘æµæ°´çº¿ï¼ˆ`video2caption.py`ï¼‰

```bash
python video2caption.py -V /path/to/video.mp4 -N 10
```

é‡è¦ï¼šè¯¥è„šæœ¬å½“å‰åŒ…å«æœºå™¨ç›¸å…³çš„ç¡¬ç¼–ç è·¯å¾„ï¼š

- Python é»˜è®¤è·¯å¾„ï¼š`/home/lachlan/miniconda3/envs/caption/bin/python`
- æè¿°è„šæœ¬è·¯å¾„ï¼š`/home/lachlan/Projects/image_captioning/clip-gpt-captioning/src/image2caption.py`

é™¤éä½ æœ‰æ„ç»´æŠ¤è¿™äº›è·¯å¾„ï¼Œå¦åˆ™è¯·ä½¿ç”¨ `v2c.py`ã€‚

### 6. é—ç•™å˜ä½“ï¼ˆ`video2caption_v1.1.py`ï¼‰

è¯¥è„šæœ¬ä»…ä¸ºå†å²å‚è€ƒä¿ç•™ã€‚å®é™…ä½¿ç”¨è¯·ä¼˜å…ˆé€‰æ‹© `v2c.py`ã€‚

### 7. æ•°æ®é›†ç”Ÿæˆ

```bash
python dataset_generation.py
```

æœŸæœ›çš„åŸå§‹è¾“å…¥ï¼š

- `data/raw/results.csv`ï¼ˆä½¿ç”¨ç«–çº¿åˆ†éš”çš„æè¿°è¡¨ï¼‰ã€‚
- `data/raw/flickr30k_images/`ï¼ˆCSV ä¸­å¼•ç”¨çš„å›¾åƒæ–‡ä»¶ï¼‰ã€‚

è¾“å‡ºï¼š

- `data/processed/dataset.pkl`

### 8. è®­ç»ƒ

```bash
python training.py -S L -C model.pt
```

è®­ç»ƒé»˜è®¤ä½¿ç”¨ Weights & Biasesï¼ˆ`wandb`ï¼‰æ—¥å¿—ã€‚

### 9. è¯„ä¼°

```bash
python evaluate.py \
  -I ./data/raw/flickr30k_images \
  -R ./data/result/eval \
  -S L \
  -C model.pt \
  -T 1.0
```

è¯„ä¼°ä¼šå°†é¢„æµ‹æè¿°æ¸²æŸ“åˆ°æµ‹è¯•å›¾åƒä¸Šï¼Œå¹¶ä¿å­˜åˆ°ï¼š

- `<res-path>/<checkpoint_name_without_ext>_<SIZE>/`

## âš™ï¸ é…ç½®

æ¨¡å‹é…ç½®å®šä¹‰äº `utils/config.py`ï¼š

| é…ç½® | CLIP ä¸»å¹² | GPT æ¨¡å‹ | æƒé‡ç›®å½• |
|---|---|---|---|
| `ConfigS` | `openai/clip-vit-base-patch32` | `gpt2` | `weights/small` |
| `ConfigL` | `openai/clip-vit-large-patch14` | `gpt2-medium` | `weights/large` |

é…ç½®ç±»ä¸­çš„å…³é”®é»˜è®¤å€¼ï¼š

| å­—æ®µ | `ConfigS` | `ConfigL` |
|---|---:|---:|
| `epochs` | 150 | 120 |
| `lr` | 3e-3 | 5e-3 |
| `batch_size_exp` | 6 | 5 |
| `ep_len` | 4 | 4 |
| `max_len` | 40 | 40 |

Checkpoint è‡ªåŠ¨ä¸‹è½½ ID ä½äº `utils/downloads.py`ï¼š

| å°ºå¯¸ | Google Drive ID |
|---|---|
| `L` | `1Gh32arzhW06C1ZJyzcJSSfdJDi3RgWoG` |
| `S` | `1pSQruQyg8KJq6VmzhMLFbT_VaHJMdlWF` |

## ğŸ“¦ è¾“å‡ºæ–‡ä»¶

### å›¾åƒæ¨ç†

- åœ¨ `--res-path` ä¿å­˜å åŠ /ç”Ÿæˆæ ‡é¢˜åçš„å›¾åƒã€‚
- æ–‡ä»¶åæ¨¡å¼ï¼š`<input_stem>-R<SIZE>.jpg`ã€‚

### è§†é¢‘æ¨ç†ï¼ˆ`v2c.py`ï¼‰

- SRTï¼š`<video_stem>_caption.srt`
- JSONï¼š`<video_stem>_caption.json`
- å¸§å›¾åƒï¼š`<video_stem>_captioning_frames/`

JSON å…ƒç´ ç¤ºä¾‹ï¼š

```json
{
  "start": "00:00:03,200",
  "end": "00:00:03,700",
  "lang": "en",
  "text": "A dog running through a field."
}
```

## ğŸ§ª ç¤ºä¾‹

### å¿«é€Ÿå›¾åƒæè¿°ç¤ºä¾‹

```bash
python image2caption.py -I ./examples/dog.jpg -S S -C model.pt
```

é¢„æœŸè¡Œä¸ºï¼š

- è‹¥ç¼ºå°‘ `weights/small/model.pt`ï¼Œä¼šè‡ªåŠ¨ä¸‹è½½ã€‚
- é»˜è®¤ä¼šå°†å¸¦æè¿°çš„å›¾åƒå†™å…¥ `./data/result/prediction`ã€‚
- æè¿°æ–‡æœ¬ä¼šè¾“å‡ºåˆ° stdoutã€‚

### å¿«é€Ÿè§†é¢‘æè¿°ç¤ºä¾‹

```bash
python v2c.py -V ./examples/demo.mp4 -N 8
```

é¢„æœŸè¡Œä¸ºï¼š

- å¯¹ 8 ä¸ªå‡åŒ€é‡‡æ ·å¸§ç”Ÿæˆæè¿°ã€‚
- åœ¨è¾“å…¥è§†é¢‘åŒç›®å½•ç”Ÿæˆ `.srt` ä¸ `.json` æ–‡ä»¶ã€‚

### ç«¯åˆ°ç«¯è®­ç»ƒ/è¯„ä¼°æµç¨‹

```bash
python dataset_generation.py
python training.py -S L -C model.pt
python evaluate.py -I ./data/raw/flickr30k_images -R ./data/result/eval -S L -C model.pt -T 1.0
```

## ğŸ§­ å¼€å‘è¯´æ˜

- `v2c.py`ã€`video2caption.py` ä¸ `video2caption_v1.*` ä¹‹é—´å­˜åœ¨é—ç•™é‡å ã€‚
- `video2caption_v1.0_not_work.py` æœ‰æ„ä¿ç•™ä¸ºä¸å¯ç”¨çš„é—ç•™ä»£ç ã€‚
- `training.py` å½“å‰é€šè¿‡ `config = ConfigL() if args.size.upper() else ConfigS()` é€‰æ‹© `ConfigL()`ï¼Œå¯¹éç©º `--size` å®å‚ä¼šå§‹ç»ˆè§£æä¸º `ConfigL`ã€‚
- `model/trainer.py` åœ¨ `test_step` ä¸­ä½¿ç”¨ `self.dataset`ï¼Œä½†åˆå§‹åŒ–å™¨èµ‹å€¼çš„æ˜¯ `self.test_dataset`ï¼›è‹¥ä¸ä¿®æ­£ï¼Œè®­ç»ƒæ—¶é‡‡æ ·å¯èƒ½å‡ºé”™ã€‚
- `video2caption_v1.1.py` å¼•ç”¨äº† `self.config.transform`ï¼Œä½† `ConfigS`/`ConfigL` å¹¶æœªå®šä¹‰ `transform`ã€‚
- å½“å‰ä»“åº“å¿«ç…§å°šæœªå®šä¹‰ CI/æµ‹è¯•å¥—ä»¶ã€‚
- i18n è¯´æ˜ï¼šè¯­è¨€é“¾æ¥ä½äºæœ¬ README é¡¶éƒ¨ï¼›ç¿»è¯‘æ–‡ä»¶å¯æ·»åŠ åœ¨ `i18n/` ä¸‹ã€‚
- å½“å‰çŠ¶æ€è¯´æ˜ï¼šè¯­è¨€æ é“¾æ¥äº† `i18n/README.ru.md`ï¼Œä½†æ­¤æ–‡ä»¶åœ¨å½“å‰å¿«ç…§ä¸­ä¸å­˜åœ¨ã€‚

## ğŸ©º æ•…éšœæ’æŸ¥

- `AssertionError: Image does not exist`
  - ç¡®è®¤ `-I/--img-path` æŒ‡å‘æœ‰æ•ˆæ–‡ä»¶ã€‚
- `Dataset file not found. Downloading...`
  - `MiniFlickrDataset` åœ¨ç¼ºå°‘ `data/processed/dataset.pkl` æ—¶ä¼šè§¦å‘ï¼›è¯·å…ˆè¿è¡Œ `python dataset_generation.py`ã€‚
- `Path to the test image folder does not exist`
  - ç¡®è®¤ `evaluate.py -I` æŒ‡å‘ç°æœ‰ç›®å½•ã€‚
- é¦–æ¬¡è¿è¡Œç¼“æ…¢æˆ–å¤±è´¥
  - åˆæ¬¡è¿è¡Œä¼šä¸‹è½½ Hugging Face æ¨¡å‹ï¼Œä¹Ÿå¯èƒ½ä» Google Drive ä¸‹è½½ checkpointã€‚
- `video2caption.py` è¿”å›ç©ºæè¿°
  - è¯·éªŒè¯ç¡¬ç¼–ç è„šæœ¬è·¯å¾„å’Œ Python å¯æ‰§è¡Œè·¯å¾„ï¼Œæˆ–æ”¹ç”¨ `v2c.py`ã€‚
- è®­ç»ƒæœŸé—´ `wandb` æç¤ºç™»å½•
  - è¿è¡Œ `wandb login`ï¼Œæˆ–æŒ‰éœ€åœ¨ `training.py` ä¸­æ‰‹åŠ¨å…³é—­æ—¥å¿—ã€‚

## ğŸ›£ï¸ è·¯çº¿å›¾

- å¢åŠ ä¾èµ– lockfileï¼ˆ`requirements.txt` æˆ– `pyproject.toml`ï¼‰ä»¥å®ç°å¯å¤ç°å®‰è£…ã€‚
- å°†é‡å¤çš„è§†é¢‘æµæ°´çº¿ç»Ÿä¸€ä¸ºå•ä¸€ç»´æŠ¤å®ç°ã€‚
- ç§»é™¤é—ç•™è„šæœ¬ä¸­çš„æœºå™¨ç¡¬ç¼–ç è·¯å¾„ã€‚
- ä¿®å¤ `training.py` ä¸ `model/trainer.py` ä¸­å·²çŸ¥çš„è®­ç»ƒ/è¯„ä¼°è¾¹ç•Œé—®é¢˜ã€‚
- å¢åŠ è‡ªåŠ¨åŒ–æµ‹è¯•ä¸ CIã€‚
- è¡¥é½ `i18n/` ä¸­è¯­è¨€æ å¼•ç”¨çš„ç¿»è¯‘ README æ–‡ä»¶ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ã€‚å»ºè®®æµç¨‹ï¼š

```bash
# 1) Fork and clone
git clone git@github.com:<your-user>/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

# 2) Create a feature branch
git checkout -b feat/your-change

# 3) Make changes and commit
git add .
git commit -m "feat: describe your change"

# 4) Push and open a PR
git push origin feat/your-change
```

å¦‚æœä½ ä¿®æ”¹äº†æ¨¡å‹è¡Œä¸ºï¼Œè¯·é™„å¸¦ï¼š

- å¯å¤ç°å‘½ä»¤ã€‚
- å˜æ›´å‰/åçš„ç¤ºä¾‹è¾“å‡ºã€‚
- å…³äº checkpoint æˆ–æ•°æ®é›†å‡è®¾çš„è¯´æ˜ã€‚

## ğŸ™Œ æ”¯æŒ

å½“å‰ä»“åº“å¿«ç…§ä¸­æœªå‘ç°æ˜ç¡®çš„æèµ /èµåŠ©é…ç½®ã€‚

è‹¥æœªæ¥æ–°å¢èµåŠ©é“¾æ¥ï¼Œåº”åœ¨æœ¬èŠ‚ä¿ç•™ã€‚

## ğŸ“„ è®¸å¯è¯

å½“å‰ä»“åº“å¿«ç…§ä¸­ä¸å­˜åœ¨è®¸å¯è¯æ–‡ä»¶ã€‚

å‡è®¾è¯´æ˜ï¼šåœ¨æ·»åŠ  `LICENSE` æ–‡ä»¶å‰ï¼Œå¤ç”¨/åˆ†å‘æ¡æ¬¾æœªå®šä¹‰ã€‚

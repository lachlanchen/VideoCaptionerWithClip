[English](../README.md) Â· [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](README.ar.md) Â· [EspaÃ±ol](README.es.md) Â· [FranÃ§ais](README.fr.md) Â· [æ—¥æœ¬èª](README.ja.md) Â· [í•œêµ­ì–´](README.ko.md) Â· [Tiáº¿ng Viá»‡t](README.vi.md) Â· [ä¸­æ–‡ (ç®€ä½“)](README.zh-Hans.md) Â· [ä¸­æ–‡ï¼ˆç¹é«”ï¼‰](README.zh-Hant.md) Â· [Deutsch](README.de.md) Â· [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README.ru.md)


[![LazyingArt banner](https://github.com/lachlanchen/lachlanchen/raw/main/figs/banner.png)](https://github.com/lachlanchen/lachlanchen/blob/main/figs/banner.png)

# Clip-GPT-Captioning

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow)
![Status](https://img.shields.io/badge/README-Expanded-success)
![Repo Layout](https://img.shields.io/badge/Layout-Root%20Scripts-informational)
![Legacy Scripts](https://img.shields.io/badge/Legacy%20Scripts-Present-orange)
![i18n](https://img.shields.io/badge/i18n-Enabled-brightgreen)
![Maintained Path](https://img.shields.io/badge/Video-v2c.py-2ea44f)

OpenAI CLIP ì‹œê° ì„ë² ë”©ê³¼ GPT ìŠ¤íƒ€ì¼ ì–¸ì–´ ëª¨ë¸ì„ ê²°í•©í•´ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ì— ìì—°ì–´ ìº¡ì…˜ì„ ìƒì„±í•˜ëŠ” Python íˆ´í‚·ì…ë‹ˆë‹¤.

## ğŸ§­ Snapshot

| Dimension | Details |
|---|---|
| Task coverage | Image and video captioning |
| Core outputs | SRT subtitles, JSON transcripts, captioned images |
| Primary scripts | `i2c.py`, `v2c.py`, `image2caption.py` |
| Legacy paths | `video2caption.py` and versioned siblings (kept for history) |
| Dataset flow | `data/raw/results.csv` + `data/raw/flickr30k_images/` |

## âœ¨ ê°œìš”

ì´ ì €ì¥ì†ŒëŠ” ë‹¤ìŒì„ ì œê³µí•©ë‹ˆë‹¤.

- ì´ë¯¸ì§€ ìº¡ì…”ë‹ ë° ë¹„ë””ì˜¤ ìë§‰ ìƒì„±ì„ ìœ„í•œ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸.
- CLIP ì‹œê° ì„ë² ë”©ì„ GPT-2 í† í° ì„ë² ë”©ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” í•™ìŠµ íŒŒì´í”„ë¼ì¸.
- Flickr30k ìŠ¤íƒ€ì¼ ë°ì´í„°ìš© ë°ì´í„°ì…‹ ìƒì„± ìœ í‹¸ë¦¬í‹°.
- ê°€ì¤‘ì¹˜ê°€ ì—†ì„ ë•Œ ì§€ì›ë˜ëŠ” ëª¨ë¸ í¬ê¸°ì˜ ì²´í¬í¬ì¸íŠ¸ ìë™ ë‹¤ìš´ë¡œë“œ.
- `i18n/` í•˜ìœ„ì˜ ë‹¤êµ­ì–´ README ë³€í˜•(ìƒë‹¨ ì–¸ì–´ ë°” ì°¸ì¡°).

í˜„ì¬ êµ¬í˜„ì—ëŠ” ìµœì‹  ìŠ¤í¬ë¦½íŠ¸ì™€ ë ˆê±°ì‹œ ìŠ¤í¬ë¦½íŠ¸ê°€ ëª¨ë‘ í¬í•¨ë©ë‹ˆë‹¤. ì¼ë¶€ ë ˆê±°ì‹œ íŒŒì¼ì€ ì°¸ê³ ìš©ìœ¼ë¡œ ìœ ì§€ë˜ë©° ì•„ë˜ì— ë¬¸ì„œí™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸš€ ê¸°ëŠ¥

- `image2caption.py`ë¥¼ í†µí•œ ë‹¨ì¼ ì´ë¯¸ì§€ ìº¡ì…”ë‹.
- `v2c.py` ë˜ëŠ” `video2caption.py`ë¥¼ í†µí•œ ë¹„ë””ì˜¤ ìº¡ì…”ë‹(ê· ì¼í•œ í”„ë ˆì„ ìƒ˜í”Œë§).
- ì‚¬ìš©ì ì§€ì • ê°€ëŠ¥í•œ ëŸ°íƒ€ì„ ì˜µì…˜:
  - í”„ë ˆì„ ìˆ˜.
  - ëª¨ë¸ í¬ê¸°.
  - ìƒ˜í”Œë§ temperature.
  - ì²´í¬í¬ì¸íŠ¸ ì´ë¦„.
- ë©€í‹°í”„ë¡œì„¸ì‹±/ìŠ¤ë ˆë“œ ìº¡ì…”ë‹ìœ¼ë¡œ ë¹„ë””ì˜¤ ì¶”ë¡  ì†ë„ ê°œì„ .
- ì¶œë ¥ ì‚°ì¶œë¬¼:
  - SRT ìë§‰ íŒŒì¼(`.srt`).
  - `v2c.py`ì˜ JSON ì „ì‚¬ë³¸(`.json`).
- CLIP+GPT2 ë§¤í•‘ ì‹¤í—˜ì„ ìœ„í•œ í•™ìŠµ ë° í‰ê°€ ì§„ì…ì .

### í•œëˆˆì— ë³´ê¸°

| ì˜ì—­ | ì£¼ìš” ìŠ¤í¬ë¦½íŠ¸ | ë¹„ê³  |
|---|---|---|
| ì´ë¯¸ì§€ ìº¡ì…”ë‹ | `image2caption.py`, `i2c.py`, `predict.py` | CLI + ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í´ë˜ìŠ¤ |
| ë¹„ë””ì˜¤ ìº¡ì…”ë‹ | `v2c.py` | ê¶Œì¥ ìœ ì§€ê´€ë¦¬ ê²½ë¡œ |
| ë ˆê±°ì‹œ ë¹„ë””ì˜¤ í”Œë¡œìš° | `video2caption.py`, `video2caption_v1.1.py` | ë¨¸ì‹  ì¢…ì† ê°€ì • í¬í•¨ |
| ë°ì´í„°ì…‹ ë¹Œë“œ | `dataset_generation.py` | `data/processed/dataset.pkl` ìƒì„± |
| í•™ìŠµ / í‰ê°€ | `training.py`, `evaluate.py` | CLIP+GPT2 ë§¤í•‘ ì‚¬ìš© |

## ğŸ§± ì•„í‚¤í…ì²˜ (High Level)

`model/model.py`ì˜ í•µì‹¬ ëª¨ë¸ì€ ì„¸ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

1. `ImageEncoder`: CLIP ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ.
2. `Mapping`: CLIP ì„ë² ë”©ì„ GPT prefix ì„ë² ë”© ì‹œí€€ìŠ¤ë¡œ íˆ¬ì˜.
3. `TextDecoder`: GPT-2 ì–¸ì–´ ëª¨ë¸ í—¤ë“œë¡œ ìº¡ì…˜ í† í°ì„ ìê¸°íšŒê·€ ë°©ì‹ìœ¼ë¡œ ìƒì„±.

í•™ìŠµ(`Net.train_forward`)ì€ ì‚¬ì „ ê³„ì‚°ëœ CLIP ì´ë¯¸ì§€ ì„ë² ë”© + í† í°í™”ëœ ìº¡ì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
ì¶”ë¡ (`Net.forward`)ì€ PIL ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ EOS ë˜ëŠ” `max_len`ì— ë„ë‹¬í•  ë•Œê¹Œì§€ í† í°ì„ ë””ì½”ë”©í•©ë‹ˆë‹¤.

### ë°ì´í„° íë¦„

1. ë°ì´í„°ì…‹ ì¤€ë¹„: `dataset_generation.py`ê°€ `data/raw/results.csv`ì™€ `data/raw/flickr30k_images/`ì˜ ì´ë¯¸ì§€ë¥¼ ì½ê³  `data/processed/dataset.pkl`ì„ ì‘ì„±í•©ë‹ˆë‹¤.
2. í•™ìŠµ: `training.py`ëŠ” í”¼í´ íŠœí”Œ `(image_name, image_embedding, caption)`ì„ ë¡œë“œí•´ ë§¤í¼/ë””ì½”ë” ë ˆì´ì–´ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
3. í‰ê°€: `evaluate.py`ê°€ ë³´ë¥˜ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì—ì„œ ìƒì„±ëœ ìº¡ì…˜ì„ ë Œë”ë§í•©ë‹ˆë‹¤.
4. ì¶”ë¡  ì œê³µ:
   - ì´ë¯¸ì§€: `image2caption.py` / `predict.py` / `i2c.py`.
   - ë¹„ë””ì˜¤: `v2c.py`(ê¶Œì¥), `video2caption.py`(ë ˆê±°ì‹œ).

## ğŸ—‚ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```text
VideoCaptionerWithClip/
â”œâ”€â”€ README.md
â”œâ”€â”€ image2caption.py               # Single-image caption CLI
â”œâ”€â”€ predict.py                     # Alternate single-image caption CLI
â”œâ”€â”€ i2c.py                         # Reusable ImageCaptioner class + CLI
â”œâ”€â”€ v2c.py                         # Video -> SRT + JSON (threaded frame captioning)
â”œâ”€â”€ video2caption.py               # Alternate video -> SRT implementation (legacy constraints)
â”œâ”€â”€ video2caption_v1.1.py          # Older variant
â”œâ”€â”€ video2caption_v1.0_not_work.py # Explicitly marked non-working legacy file
â”œâ”€â”€ training.py                    # Model training entrypoint
â”œâ”€â”€ evaluate.py                    # Test-split evaluation and rendered outputs
â”œâ”€â”€ dataset_generation.py          # Builds data/processed/dataset.pkl
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dataset.py                 # Dataset + DataLoader helpers
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py                   # CLIP encoder + mapping + GPT2 decoder
â”‚   â””â”€â”€ trainer.py                 # Training/validation/test utility class
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                  # ConfigS / ConfigL defaults
â”‚   â”œâ”€â”€ downloads.py               # Google Drive checkpoint downloader
â”‚   â””â”€â”€ lr_warmup.py               # LR warmup schedule
â”œâ”€â”€ i18n/                          # Multilingual README variants
â””â”€â”€ .auto-readme-work/             # Auto-README pipeline artifacts
```

## ğŸ“‹ ì‚¬ì „ ìš”êµ¬ì‚¬í•­

- Python `3.10+` ê¶Œì¥.
- CUDA ì‚¬ìš© ê°€ëŠ¥ GPUëŠ” ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ í•™ìŠµê³¼ ëŒ€í˜• ëª¨ë¸ ì¶”ë¡ ì—ëŠ” ê°•í•˜ê²Œ ê¶Œì¥ë©ë‹ˆë‹¤.
- í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” `ffmpeg`ê°€ ì§ì ‘ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©°(OpenCVê°€ í”„ë ˆì„ ì¶”ì¶œì— ì‚¬ìš©ë¨).
- Hugging Face / Google Driveì—ì„œ ëª¨ë¸/ì²´í¬í¬ì¸íŠ¸ë¥¼ ì²˜ìŒ ë‹¤ìš´ë¡œë“œí•  ë•Œ ì¸í„°ë„· ì ‘ì†ì´ í•„ìš”í•©ë‹ˆë‹¤.

í˜„ì¬ `requirements.txt` / `pyproject.toml`ê³¼ ê°™ì€ ë½íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ, ì˜ì¡´ì„±ì€ import ëª©ë¡ìœ¼ë¡œ ì¶”ë¡ ë©ë‹ˆë‹¤.

## ğŸ› ï¸ ì„¤ì¹˜

### Canonical setup from current repository layout

```bash

git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

python -m venv .venv
source .venv/bin/activate

pip install --upgrade pip
pip install torch torchvision torchaudio
pip install transformers pillow matplotlib numpy tqdm opencv-python pandas wandb gdown
```

### Original README installation snippet (preserved)

ì´ì „ READMEëŠ” ì½”ë“œ ë¸”ë¡ ì¤‘ê°„ì—ì„œ ëë‚¬ìŠµë‹ˆë‹¤. ì›ë³¸ ëª…ë ¹ì€ ì†ŒìŠ¤ ê¸°ì¤€ ì—­ì‚¬ì  ë‚´ìš©ìœ¼ë¡œ ì•„ë˜ì— ê·¸ëŒ€ë¡œ ë³´ì¡´ë©ë‹ˆë‹¤.

```bash
git clone git@github.com:lachlanchen/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip/src
```

ì°¸ê³ : í˜„ì¬ ì €ì¥ì†Œ ìŠ¤ëƒ…ìƒ·ì—ì„œëŠ” ìŠ¤í¬ë¦½íŠ¸ê°€ `src/`ê°€ ì•„ë‹Œ ì €ì¥ì†Œ ë£¨íŠ¸ì— ë°°ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## â–¶ï¸ Quick Start

| Goal | Command |
|---|---|
| Caption an image | `python image2caption.py -I /path/to/image.jpg -S L -C model.pt` |
| Caption a video | `python v2c.py -V /path/to/video.mp4 -N 10` |
| Build dataset | `python dataset_generation.py` |

### Image captioning (quick run)

```bash
python image2caption.py -I /path/to/image.jpg -S L -C model.pt
```

### Video captioning (recommended path)

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

## ğŸ¯ ì‚¬ìš©ë²•

### 1. ì´ë¯¸ì§€ ìº¡ì…”ë‹ (`image2caption.py`)

```bash
python image2caption.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

ì¸ì:

- `-I, --img-path`: ì…ë ¥ ì´ë¯¸ì§€ ê²½ë¡œ.
- `-S, --size`: ëª¨ë¸ í¬ê¸° (`S` ë˜ëŠ” `L`).
- `-C, --checkpoint-name`: `weights/{small|large}` ë‚´ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ëª….
- `-R, --res-path`: ë Œë”ë§ëœ ìº¡ì…˜ ì´ë¯¸ì§€ì˜ ì¶œë ¥ ë””ë ‰í„°ë¦¬.
- `-T, --temperature`: ìƒ˜í”Œë§ temperature.

### 2. ëŒ€ì²´ ì´ë¯¸ì§€ CLI (`predict.py`)

```bash
python predict.py \
  -I /path/to/image.jpg \
  -S L \
  -C model.pt \
  -R ./data/result/prediction \
  -T 1.0
```

`predict.py`ëŠ” ê¸°ëŠ¥ì ìœ¼ë¡œ `image2caption.py`ì™€ ìœ ì‚¬í•©ë‹ˆë‹¤. ë‹¤ë§Œ ì¶œë ¥ í…ìŠ¤íŠ¸ í¬ë§·ì´ ì•½ê°„ ë‹¤ë¦…ë‹ˆë‹¤.

### 3. ì´ë¯¸ì§€ ìº¡ì…”ë‹ í´ë˜ìŠ¤ API (`i2c.py`)

```bash
python i2c.py -I /path/to/image.jpg -S L -C model.pt -R ./data/result/prediction -T 1.0
```

ë˜ëŠ” ì§ì ‘ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ import:

```python
from i2c import ImageCaptioner

captioner = ImageCaptioner(model_size="L", checkpoint_name="model.pt")
captioner.set_image_path("/path/to/image.jpg")
caption = captioner.generate_caption(save_image=True)
print(caption)
```

### 4. ë¹„ë””ì˜¤ -> ìë§‰ + JSON (`v2c.py`)

```bash
python v2c.py -V /path/to/video.mp4 -N 10
```

ì…ë ¥ ë¹„ë””ì˜¤ ì˜†ì— ìƒì„±ë˜ëŠ” íŒŒì¼:

- `<video_basename>_caption.srt`
- `<video_basename>_caption.json`
- `<video_basename>_captioning_frames/`

### 5. ëŒ€ì²´ ë¹„ë””ì˜¤ íŒŒì´í”„ë¼ì¸ (`video2caption.py`)

```bash
python video2caption.py -V /path/to/video.mp4 -N 10
```

ì¤‘ìš”: í˜„ì¬ ì´ ìŠ¤í¬ë¦½íŠ¸ì—ëŠ” ë¨¸ì‹  íŠ¹í™” í•˜ë“œì½”ë”© ê²½ë¡œê°€ ì¡´ì¬í•©ë‹ˆë‹¤.

- Python ê²½ë¡œ ê¸°ë³¸ê°’: `/home/lachlan/miniconda3/envs/caption/bin/python`
- ìº¡ì…˜ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ: `/home/lachlan/Projects/image_captioning/clip-gpt-captioning/src/image2caption.py`

ì˜ë„ì ìœ¼ë¡œ ì´ ê²½ë¡œë¥¼ ìœ ì§€í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ `v2c.py`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

### 6. ë ˆê±°ì‹œ ë³€í˜• (`video2caption_v1.1.py`)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ê³¼ê±° ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ë³´ì¡´ë©ë‹ˆë‹¤. ì‹¤ì‚¬ìš©ì—ëŠ” `v2c.py`ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.

### 7. ë°ì´í„°ì…‹ ìƒì„±

```bash
python dataset_generation.py
```

ì˜ˆìƒ ì›ë³¸ ì…ë ¥:

- `data/raw/results.csv` (íŒŒì´í”„ êµ¬ë¶„ ìº¡ì…˜ í…Œì´ë¸”)
- `data/raw/flickr30k_images/` (CSVì—ì„œ ì°¸ì¡°ë˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼)

ì¶œë ¥:

- `data/processed/dataset.pkl`

### 8. í•™ìŠµ

```bash
python training.py -S L -C model.pt
```

í•™ìŠµì€ ê¸°ë³¸ì ìœ¼ë¡œ Weights & Biases(`wandb`) ë¡œê¹…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

### 9. í‰ê°€

```bash
python evaluate.py \
  -I ./data/raw/flickr30k_images \
  -R ./data/result/eval \
  -S L \
  -C model.pt \
  -T 1.0
```

í‰ê°€ëŠ” í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ì˜ˆì¸¡ ìº¡ì…˜ì„ ë Œë”ë§í•˜ê³  ë‹¤ìŒ ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤:

- `<res-path>/<checkpoint_name_without_ext>_<SIZE>/`

## âš™ï¸ ì„¤ì •

ëª¨ë¸ ì„¤ì •ì€ `utils/config.py`ì— ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

| Config | CLIP backbone | GPT model | Weights dir |
|---|---|---|---|
| `ConfigS` | `openai/clip-vit-base-patch32` | `gpt2` | `weights/small` |
| `ConfigL` | `openai/clip-vit-large-patch14` | `gpt2-medium` | `weights/large` |

ì„¤ì • í´ë˜ìŠ¤ì˜ ì£¼ìš” ê¸°ë³¸ê°’:

| Field | `ConfigS` | `ConfigL` |
|---|---:|---:|
| `epochs` | 150 | 120 |
| `lr` | 3e-3 | 5e-3 |
| `batch_size_exp` | 6 | 5 |
| `ep_len` | 4 | 4 |
| `max_len` | 40 | 40 |

ì²´í¬í¬ì¸íŠ¸ ìë™ ë‹¤ìš´ë¡œë“œ IDëŠ” `utils/downloads.py`ì— ìˆìŠµë‹ˆë‹¤.

| Size | Google Drive ID |
|---|---|
| `L` | `1Gh32arzhW06C1ZJyzcJSSfdJDi3RgWoG` |
| `S` | `1pSQruQyg8KJq6VmzhMLFbT_VaHJMdlWF` |

## ğŸ“¦ ì¶œë ¥ íŒŒì¼

### ì´ë¯¸ì§€ ì¶”ë¡ 

- `--res-path`ì— ìº¡ì…˜/ì œëª©ì´ ì˜¤ë²„ë ˆì´ëœ ì €ì¥ ì´ë¯¸ì§€.
- íŒŒì¼ëª… íŒ¨í„´: `<input_stem>-R<SIZE>.jpg`.

### ë¹„ë””ì˜¤ ì¶”ë¡  (`v2c.py`)

- SRT: `<video_stem>_caption.srt`
- JSON: `<video_stem>_caption.json`
- í”„ë ˆì„ ì´ë¯¸ì§€: `<video_stem>_captioning_frames/`

JSON ì˜ˆì‹œ:

```json
{
  "start": "00:00:03,200",
  "end": "00:00:03,700",
  "lang": "en",
  "text": "A dog running through a field."
}
```

## ğŸ§ª ì˜ˆì‹œ

### ë¹ ë¥¸ ì´ë¯¸ì§€ ìº¡ì…˜ ì˜ˆì‹œ

```bash
python image2caption.py -I ./examples/dog.jpg -S S -C model.pt
```

ì˜ˆìƒ ë™ì‘:

- `weights/small/model.pt`ê°€ ì—†ë‹¤ë©´ ìë™ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.
- ê¸°ë³¸ê°’ìœ¼ë¡œ ìº¡ì…˜ ì²˜ë¦¬ëœ ì´ë¯¸ì§€ëŠ” `./data/result/prediction`ì— ì €ì¥ë©ë‹ˆë‹¤.
- ìº¡ì…˜ í…ìŠ¤íŠ¸ëŠ” stdoutìœ¼ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.

### ë¹ ë¥¸ ë¹„ë””ì˜¤ ìº¡ì…˜ ì˜ˆì‹œ

```bash
python v2c.py -V ./examples/demo.mp4 -N 8
```

ì˜ˆìƒ ë™ì‘:

- 8ê°œì˜ ê· ì¼ ìƒ˜í”Œë§ í”„ë ˆì„ì´ ìº¡ì…”ë‹ë©ë‹ˆë‹¤.
- ì…ë ¥ ë¹„ë””ì˜¤ ì˜†ì— `.srt`ì™€ `.json` íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤.

### End-to-end í•™ìŠµ/í‰ê°€ ì‹œí€€ìŠ¤

```bash
python dataset_generation.py
python training.py -S L -C model.pt
python evaluate.py -I ./data/raw/flickr30k_images -R ./data/result/eval -S L -C model.pt -T 1.0
```

## ğŸ§­ Development Notes

- `v2c.py`, `video2caption.py`, `video2caption_v1.*` ê°„ì— ë ˆê±°ì‹œ ì¤‘ë³µì´ ì¡´ì¬í•©ë‹ˆë‹¤.
- `video2caption_v1.0_not_work.py`ëŠ” ì˜ë„ì ìœ¼ë¡œ ë™ì‘í•˜ì§€ ì•ŠëŠ” ë ˆê±°ì‹œ ì½”ë“œë¡œ ìœ ì§€ë©ë‹ˆë‹¤.
- `training.py`ëŠ” í˜„ì¬ `config = ConfigL() if args.size.upper() else ConfigS()`ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, `--size`ê°€ ë¹„ì–´ ìˆì§€ ì•Šì€ í•œ í•­ìƒ `ConfigL`ì´ ì„ íƒë©ë‹ˆë‹¤.
- `model/trainer.py`ëŠ” `test_step`ì—ì„œ `self.dataset`ì„ ì‚¬ìš©í•˜ì§€ë§Œ, ì´ˆê¸°í™”ì—ì„œëŠ” `self.test_dataset`ì„ í• ë‹¹í•´ ìƒ˜í”Œë§ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- `video2caption_v1.1.py`ëŠ” `self.config.transform`ì„ ì°¸ì¡°í•˜ì§€ë§Œ `ConfigS`/`ConfigL`ì—ëŠ” `transform`ì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.
- í˜„ì¬ ì €ì¥ì†Œ ìŠ¤ëƒ…ìƒ·ì—ëŠ” CI/test suiteê°€ ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.
- i18n ì°¸ê³ : ì´ README ìƒë‹¨ì— ì–¸ì–´ ë§í¬ê°€ ìˆìœ¼ë©° ë²ˆì—­ íŒŒì¼ì´ `i18n/` ì•„ë˜ì— ì¶”ê°€ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- í˜„ì¬ ìƒíƒœ ì°¸ê³ : ì–¸ì–´ ë°” ë§í¬ëŠ” `i18n/README.ru.md`ë¥¼ ê°€ë¦¬í‚¤ì§€ë§Œ, í•´ë‹¹ íŒŒì¼ì€ í˜„ì¬ ìŠ¤ëƒ…ìƒ·ì— ì—†ìŠµë‹ˆë‹¤.

## ğŸ©º ë¬¸ì œ í•´ê²°

- `AssertionError: Image does not exist`
  - `-I/--img-path`ê°€ ìœ íš¨í•œ íŒŒì¼ì„ ê°€ë¦¬í‚¤ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
- `Dataset file not found. Downloading...`
  - `MiniFlickrDataset`ì€ `data/processed/dataset.pkl`ì´ ì—†ì„ ë•Œ ì´ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤. ë¨¼ì € `python dataset_generation.py`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.
- `Path to the test image folder does not exist`
  - `evaluate.py -I` ì¸ìˆ˜ê°€ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í´ë”ë¥¼ ê°€ë¦¬í‚¤ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
- ì²« ì‹¤í–‰ì´ ëŠë¦¬ê±°ë‚˜ ì‹¤íŒ¨í•¨
  - ì´ˆê¸° ì‹¤í–‰ ì‹œ Hugging Face ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  Google Driveì—ì„œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‚´ë ¤ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- `video2caption.py`ê°€ ë¹ˆ ìº¡ì…˜ì„ ë°˜í™˜í•¨
  - í•˜ë“œì½”ë”©ëœ Python ì‹¤í–‰ ê²½ë¡œì™€ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ `v2c.py`ë¡œ ì „í™˜í•˜ì„¸ìš”.
- í•™ìŠµ ì¤‘ `wandb`ì—ì„œ ë¡œê·¸ì¸ í”„ë¡¬í”„íŠ¸ê°€ í‘œì‹œë¨
  - `wandb login`ì„ ì‹¤í–‰í•˜ê±°ë‚˜, í•„ìš”í•˜ë©´ `training.py`ì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ë¡œê¹…ì„ ë¹„í™œì„±í™”í•˜ì„¸ìš”.

## ğŸ›£ï¸ ë¡œë“œë§µ

- ì¬í˜„ ê°€ëŠ¥í•œ ì„¤ì¹˜ë¥¼ ìœ„í•´ ì˜ì¡´ì„± ë½íŒŒì¼(`requirements.txt` ë˜ëŠ” `pyproject.toml`) ì¶”ê°€.
- ì¤‘ë³µëœ ë¹„ë””ì˜¤ íŒŒì´í”„ë¼ì¸ì„ í•˜ë‚˜ì˜ ìœ ì§€ê´€ë¦¬ êµ¬í˜„ìœ¼ë¡œ í†µí•©.
- ë ˆê±°ì‹œ ìŠ¤í¬ë¦½íŠ¸ì˜ í•˜ë“œì½”ë”© ë¨¸ì‹  ê²½ë¡œ ì œê±°.
- `training.py`ì™€ `model/trainer.py`ì˜ ì•Œë ¤ì§„ í•™ìŠµ/í‰ê°€ ì—£ì§€ ì¼€ì´ìŠ¤ ë²„ê·¸ ìˆ˜ì •.
- ìë™í™” í…ŒìŠ¤íŠ¸ ë° CI ì¶”ê°€.
- ì–¸ì–´ ë°”ì—ì„œ ì°¸ì¡°ë˜ëŠ” ë²ˆì—­ README íŒŒì¼ë¡œ `i18n/` ì±„ìš°ê¸°.

## ğŸ¤ ê¸°ì—¬

ê¸°ì—¬ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤. ê¶Œì¥ ì›Œí¬í”Œë¡œ:

```bash
# 1) Fork and clone
git clone git@github.com:<your-user>/VideoCaptionerWithClip.git
cd VideoCaptionerWithClip

# 2) Create a feature branch
git checkout -b feat/your-change

# 3) Make changes and commit
git add .
git commit -m "feat: describe your change"

# 4) Push and open a PR
git push origin feat/your-change
```

ëª¨ë¸ ë™ì‘ì„ ë³€ê²½í–ˆë‹¤ë©´ ë‹¤ìŒì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

- ì¬í˜„ ê°€ëŠ¥í•œ ëª…ë ¹ì–´.
- ë³€ê²½ ì „/í›„ ìƒ˜í”Œ ì¶œë ¥.
- ì²´í¬í¬ì¸íŠ¸ ë˜ëŠ” ë°ì´í„°ì…‹ ê°€ì •ì— ëŒ€í•œ ë©”ëª¨.

## â¤ï¸ Support

| Donate | PayPal | Stripe |
|---|---|---|
| [![Donate](https://img.shields.io/badge/Donate-LazyingArt-0EA5E9?style=for-the-badge&logo=ko-fi&logoColor=white)](https://chat.lazying.art/donate) | [![PayPal](https://img.shields.io/badge/PayPal-RongzhouChen-00457C?style=for-the-badge&logo=paypal&logoColor=white)](https://paypal.me/RongzhouChen) | [![Stripe](https://img.shields.io/badge/Stripe-Donate-635BFF?style=for-the-badge&logo=stripe&logoColor=white)](https://buy.stripe.com/aFadR8gIaflgfQV6T4fw400) |

## ğŸ“„ ë¼ì´ì„ ìŠ¤

í˜„ì¬ ì €ì¥ì†Œ ìŠ¤ëƒ…ìƒ·ì—ëŠ” ë¼ì´ì„ ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.

Assumption note: until a `LICENSE` file is added, reuse/distribution terms are undefined.
